{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/manifoldflasso_jmlr\n",
      "0 1\n",
      "1 0.5\n",
      "2 0.25\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "2 0.25\n",
      "3 0.125\n",
      "4 0.0625\n",
      "5 0.03125\n",
      "6 0.015625\n",
      "7 0.0078125\n",
      "8 0.00390625\n",
      "9 0.001953125\n",
      "10 0.0009765625\n",
      "11 0.00048828125\n",
      "12 0.000244140625\n",
      "13 0.0001220703125\n",
      "14 6.103515625e-05\n",
      "15 3.0517578125e-05\n",
      "16 1.52587890625e-05\n",
      "17 7.62939453125e-06\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "2 0.25\n",
      "3 0.125\n",
      "4 0.0625\n",
      "5 0.03125\n",
      "6 0.015625\n",
      "7 0.0078125\n",
      "8 0.00390625\n",
      "9 0.001953125\n",
      "10 0.0009765625\n",
      "11 0.00048828125\n",
      "12 0.000244140625\n",
      "13 0.0001220703125\n",
      "14 6.103515625e-05\n",
      "15 3.0517578125e-05\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "2 0.25\n",
      "3 0.125\n",
      "4 0.0625\n",
      "5 0.03125\n",
      "6 0.015625\n",
      "7 0.0078125\n",
      "8 0.00390625\n",
      "9 0.001953125\n",
      "10 0.0009765625\n",
      "11 0.00048828125\n",
      "12 0.000244140625\n",
      "13 0.0001220703125\n",
      "14 6.103515625e-05\n",
      "15 3.0517578125e-05\n",
      "16 1.52587890625e-05\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "2 0.25\n",
      "3 0.125\n",
      "4 0.0625\n",
      "5 0.03125\n",
      "6 0.015625\n",
      "7 0.0078125\n",
      "8 0.00390625\n",
      "9 0.001953125\n",
      "10 0.0009765625\n",
      "11 0.00048828125\n",
      "12 0.000244140625\n",
      "13 0.0001220703125\n",
      "14 6.103515625e-05\n",
      "15 3.0517578125e-05\n",
      "16 1.52587890625e-05\n",
      "17 7.62939453125e-06\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "we did it\n",
      "0 1\n",
      "1 0.5\n",
      "2 0.25\n",
      "3 0.125\n",
      "4 0.0625\n",
      "5 0.03125\n",
      "6 0.015625\n",
      "7 0.0078125\n",
      "8 0.00390625\n",
      "9 0.001953125\n",
      "10 0.0009765625\n",
      "11 0.00048828125\n",
      "12 0.000244140625\n",
      "13 0.0001220703125\n",
      "14 6.103515625e-05\n",
      "15 3.0517578125e-05\n",
      "16 1.52587890625e-05\n",
      "17 7.62939453125e-06\n",
      "we did it\n",
      "(16,)\n",
      "(15,)\n",
      "(16,)\n",
      "(17,)\n",
      "(17,)\n"
     ]
    }
   ],
   "source": [
    "#Samson Koelle\n",
    "#Meila group\n",
    "#021419\n",
    "\n",
    "\n",
    "#rootdirectory = '/Users/samsonkoelle/Downloads/manigrad-100818/mani-samk-gradients'\n",
    "#f = open(rootdirectory + '/code/source/packagecontrol.py')\n",
    "#source = f.read()\n",
    "#exec(source)\n",
    "#f = open(rootdirectory + '/code/source/sourcecontrol.py')\n",
    "#source = f.read()\n",
    "#exec(source)\n",
    "#f = open(rootdirectory + '/code/source/RigidEthanol.py')\n",
    "#source = f.read()\n",
    "#exec(source)\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import random\n",
    "import sys\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "now = datetime.datetime.now().strftime(\"%B_%d_%Y_%H_%M_%S\")\n",
    "workingdirectory = os.popen('git rev-parse --show-toplevel').read()[:-1]\n",
    "sys.path.append(workingdirectory)\n",
    "os.chdir(workingdirectory)\n",
    "#print(os.getcwd())\n",
    "from codes.experimentclasses.RigidEthanolPCA import RigidEthanolPCA\n",
    "from codes.otherfunctions.multirun import get_coeffs_reps_tangent\n",
    "from codes.otherfunctions.multirun import get_grads_reps_pca2_tangent\n",
    "from codes.otherfunctions.multiplot import plot_reg_path_ax_lambdasearch_tangent\n",
    "from codes.otherfunctions.get_dictionaries import get_atoms_4\n",
    "from codes.flasso.Replicate import Replicate\n",
    "from codes.otherfunctions.get_grads_tangent import get_grads_tangent\n",
    "from codes.otherfunctions.multirun import get_support_recovery_lambda\n",
    "from codes.otherfunctions.multirun import get_lower_interesting_lambda\n",
    "import matplotlib.pyplot as plt\n",
    "from codes.otherfunctions.multirun import get_coeffs_and_lambdas\n",
    "from codes.geometer.RiemannianManifold import RiemannianManifold\n",
    "from collections import Counter\n",
    "\n",
    "#set parameters\n",
    "n = 10000 #number of data points to simulate\n",
    "nsel = 5 #number of points to analyze with lasso\n",
    "itermax = 1000 #maximum iterations per lasso run\n",
    "tol = 1e-10 #convergence criteria for lasso\n",
    "#lambdas = np.asarray([0,.01,.1,1,10,100], dtype = np.float16)#lambda values for lasso\n",
    "#lambdas = np.asarray(np.hstack([np.asarray([0]),np.logspace(-3,1,11)]), dtype = np.float16)\n",
    "#lambdas = np.asarray(np.hstack([np.asarray([0]),np.logspace(-3,0,7), np.logspace(0,2,5),np.logspace(2,3,2)]), dtype = np.float16)\n",
    "lambdas = np.asarray(np.hstack([np.asarray([0]),np.logspace(-2,1,15)]), dtype = np.float16)\n",
    "n_neighbors = 1000 #number of neighbors in megaman\n",
    "n_components = 3 #number of embedding dimensions (diffusion maps)\n",
    "#diffusion_time = 1. #diffusion time controls gaussian kernel radius per gradients paper\n",
    "#diffusion_time =.05 #(yuchia suggestion)\n",
    "diffusion_time =.25 #(yuchia suggestion)\n",
    "dim = 2 #manifold dimension\n",
    "dimnoise = 2\n",
    "cores = 3 #number of cores for parallel processing\n",
    "cor = 0.0 #correlation for noise\n",
    "var = 0.00001 #variance scaler for noise\n",
    "ii = np.asarray([0,0,0,0,1,1,1,2]) # atom adjacencies for dihedral angle computation\n",
    "jj = np.asarray([1,2,3,4,5,6,7,8])\n",
    "\n",
    "#run experiment\n",
    "atoms4 = np.asarray([[6,1,0,4],[4,0,2,8],[7,6,5,1],[3,0,2,4]],dtype = int)\n",
    "\n",
    "m = 3\n",
    "new_MN = True\n",
    "new_grad = True\n",
    "savename = 'rigidethanol_032520'\n",
    "savefolder = 'rigidethanol'\n",
    "loadfolder = 'rigidethanol'\n",
    "loadname = 'rigidethanol_032520'\n",
    "nreps = 5\n",
    "atoms4,p = get_atoms_4(9,ii,jj)\n",
    "folder = workingdirectory + '/Figures/rigidethanol/' + now + 'n' + str(n) + 'nsel' + str(nsel) + 'nreps' + str(nreps)\n",
    "os.mkdir(folder)\n",
    "\n",
    "if new_MN == True:\n",
    "    experiment = RigidEthanolPCA(dim, cor,var,ii,jj, cores, False, atoms4)\n",
    "    #projector  = np.load(workingdirectory + '/untracked_data/chemistry_data/ethanolangles022119_pca50_components.npy')\n",
    "    #experiment.M = experiment.load_data()  # if noise == False then noise parameters are overriden\n",
    "\t#experiment.Mpca = RiemannianManifold(np.load(workingdirectory + '/untracked_data/chemistry_data/ethanolangles022119_pca50.npy'), dim)\n",
    "    experiment.M, experiment.Mpca, projector = experiment.generate_data(noise=False)\n",
    "    experiment.q = m\n",
    "    experiment.m = m\n",
    "    experiment.dimnoise = dimnoise\n",
    "    experiment.projector = projector\n",
    "    experiment.Mpca.geom = experiment.Mpca.compute_geom(diffusion_time, n_neighbors)\n",
    "    experiment.N = experiment.Mpca.get_embedding3(experiment.Mpca.geom, m, diffusion_time, dim)\n",
    "    with open(workingdirectory + '/untracked_data/embeddings/' + savefolder + '/' + savename + '.pkl' ,\n",
    "             'wb') as output:\n",
    "         pickle.dump(experiment, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "lambda_max = 1\n",
    "max_search = 30\n",
    "\n",
    "experiment.p = p# + experiment.d\n",
    "experiment.atoms4 = atoms4\n",
    "experiment.itermax = itermax\n",
    "experiment.tol = tol\n",
    "experiment.dnoise = dim\n",
    "experiment.nreps = nreps\n",
    "experiment.nsel = nsel\n",
    "experiment.folder = folder\n",
    "experiment.m = dim\n",
    "replicates = {}\n",
    "selected_points_save = np.zeros((nreps,nsel))\n",
    "for i in range(nreps):\n",
    "    selected_points = np.random.choice(list(range(n)),nsel,replace = False)\n",
    "    selected_points_save[i] = selected_points\n",
    "    replicates[i] = Replicate()\n",
    "    replicates[i].nsel = nsel\n",
    "    replicates[i].selected_points = selected_points\n",
    "    replicates[i].df_M,replicates[i].dg_M,replicates[i].dg_w ,replicates[i].dg_w_pca ,replicates[i].dgw_norm  = get_grads_tangent(experiment, experiment.Mpca, experiment.M, selected_points, False)\n",
    "    replicates[i].xtrain, replicates[i].groups = experiment.construct_X(replicates[i].dg_M)\n",
    "    replicates[i].ytrain = experiment.construct_Y(replicates[i].df_M)#,list(range(nsel)))\n",
    "    replicates[i].coeff_dict = {}\n",
    "    replicates[i].coeff_dict[0] = experiment.get_betas_spam2(replicates[i].xtrain, replicates[i].ytrain, replicates[i].groups, np.asarray([0]), nsel, experiment.dim, itermax, tol)\n",
    "    replicates[i].combined_norms = {}\n",
    "    replicates[i].combined_norms[0] = np.linalg.norm(np.linalg.norm(replicates[i].coeff_dict[0][:, :, :, :], axis=2), axis=1)[0,:]\n",
    "    replicates[i].higher_lambda,replicates[i].coeff_dict,replicates[i].combined_norms = get_support_recovery_lambda(experiment, replicates[i],  lambda_max, max_search,dim)\n",
    "    replicates[i].lower_lambda,replicates[i].coeff_dict,replicates[i].combined_norms = get_lower_interesting_lambda(experiment, replicates[i],  lambda_max, max_search)\n",
    "    #= experiment.get_betas_spam2(replicates[i].xtrain, replicates[i].ytrain, replicates[i].groups, lambdas, len(selected_points), n_embedding_coordinates, itermax, tol)\n",
    "\n",
    "for i in range(nreps):\n",
    "    replicates[i].coeffs, replicates[i].lambdas_plot = get_coeffs_and_lambdas(replicates[i].coeff_dict, replicates[i].lower_lambda, replicates[i].higher_lambda)\n",
    "\n",
    "#nreps = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def construct_Y(self, df_M):\n",
    "        \"\"\" df_M should have shape n x dim x dim\n",
    "        \"\"\"\n",
    "        n = df_M.shape[0]\n",
    "        dim = df_M.shape[1]\n",
    "\n",
    "        #reorg1 = np.swapaxes(df_M, 0, 1)\n",
    "        yvec = np.reshape(df_M, (n * dim * dim))\n",
    "        return (yvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.asarray([[[1,0],[1,0.1],[0,.5]]])#.5]]])\n",
    "xtrain,groups = experiment.construct_X(Xtest)\n",
    "ytrain = construct_Y(experiment,np.repeat([np.identity(2)], axis = 0, repeats = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 1. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.1, 0.5, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 1. , 1. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0.1, 0.5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1.0000000e+00, -1.4529776e-16, -1.0000000e+01,  1.0000000e+01]),\n",
       " array([], dtype=float64),\n",
       " 4,\n",
       " array([1.41598464, 1.41598464, 0.07062224, 0.07062224]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(xtrain[:,[0,1,3,4]], ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 2.]),\n",
       " array([], dtype=float64),\n",
       " 4,\n",
       " array([1. , 1. , 0.5, 0.5]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.lstsq(xtrain[:,[0,2,3,5]], ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = experiment.get_betas_spam2(xtrain, ytrain, groups, np.asarray([.5]), 1, 2, itermax, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.16914262,  0.33196074,  0.        ]],\n",
       "\n",
       "        [[-0.01125458,  0.04445066,  0.        ]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the above solution corresponds to the solution to the constrained problem with $t = \\|cs\\|_{1,2}$. Can we find a $\\lambda$ such that the solution gives the result as with the constraint $t = 5$?.  All sorts of sources claim that this is possible, but empirically it seems to not, and if you look at the losses/gradients, this does not appear to be a bug. Let's therefore review the reasons one might believe such a $\\lambda$ exists.\n",
    "\n",
    "The reason we may expect such a $\\lambda$ to exist is Lagrange duality.  Consider the minimization\n",
    "\\begin{eqnarray*}\n",
    "\\min f(\\beta)  \\; s.t.\\;  g(\\beta) = (\\|\\beta\\|_{1,2} -5) \\leq 0\n",
    "\\end{eqnarray*}\n",
    "Define the Lagrangian\n",
    "\\begin{eqnarray*}\n",
    "L(\\beta, \\lambda) = f(\\beta)  + \\lambda g(\\beta)\n",
    "\\end{eqnarray*}\n",
    "Immediately, we see that $f(\\beta) \\geq L(\\beta, \\lambda)$ since $g(\\beta) \\leq 0$.  Then\n",
    "\\begin{eqnarray*}\n",
    "\\max_{\\lambda \\geq 0} L(\\beta, \\lambda) = f(\\beta)\n",
    "\\end{eqnarray*}\n",
    "Also define the Lagrange dual function\n",
    "\\begin{eqnarray*}\n",
    "l(\\lambda) = \\min_{\\beta} L(\\beta, \\lambda) \n",
    "\\end{eqnarray*}\n",
    "Then the Lagrangian dual problem is\n",
    "\\begin{eqnarray*}\n",
    "\\max_{\\lambda \\geq 0} \\min_{\\beta} L(\\beta, \\lambda) \n",
    "\\end{eqnarray*}\n",
    "This is equivalent to $\\min f(\\beta)$ as long as Slater's condition is satisfied, i.e. strong duality holds.\n",
    "The $\\lambda$ is\n",
    "\\begin{eqnarray*}\n",
    "\\lambda = \\frac{\\partial_\\lambda \\|y - x\\beta\\|_2^2}{\\partial_\\lambda \\|\\beta \\|_{1,2}} |_{\\beta = \\beta_{ols}}\n",
    "\\end{eqnarray*}\n",
    "where the derivative is directional along the regularization trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 0, 1, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs = experiment.get_betas_spam2(xtrain, ytrain, groups, np.asarray([.00001]), 1, 2, 1000*itermax, .01*tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.72684425,  0.27314582, -0.05462804]],\n",
       "\n",
       "        [[-0.03714508,  0.03714565,  1.9925313 ]]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.99673279])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.compute_penalty2(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method get_l2loss in module codes.flasso.FlassoExperiment:\n",
      "\n",
      "get_l2loss(coeffs, ys, xs) method of codes.experimentclasses.RigidEthanolPCA.RigidEthanolPCA instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(experiment.get_l2loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9058963e-10])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.get_l2loss(cs, ytrain, xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The true champion\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import elementwise_grad\n",
    "from autograd import grad\n",
    "\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "#import numpy as np\n",
    "from scipy.special import expit\n",
    "from pyglmnet import utils\n",
    "\n",
    "\n",
    "class GLM:\n",
    "    \n",
    "    def __init__(self, xs, ys, reg_lambda, group,max_iter, learning_rate, tol,parameter):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.group = np.asarray(group)\n",
    "        #print(self.group.shape)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.parameter = parameter\n",
    "        self.l4loss = {}\n",
    "        self.penalty = {}\n",
    "        \n",
    "    def _prox(self,beta, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "        \n",
    "        #print(thresh, beta)\n",
    "        #print('beginprox', beta[0:2],thresh)\n",
    "        group_ids = np.unique(self.group)\n",
    "        result = np.zeros(beta.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        #print('gids',group_ids)\n",
    "        for i in range(len(group_ids)):\n",
    "            gid = i \n",
    "            #print(self.group)\n",
    "            idxs_to_update = np.where(self.group == gid)[0]\n",
    "            #print('idx',idxs_to_update)\n",
    "            #print('norm', np.linalg.norm(beta[idxs_to_update]))\n",
    "            if np.linalg.norm(beta[idxs_to_update]) > 0.:\n",
    "                #print('in here', len(idxs_to_update))\n",
    "                potentialoutput = beta[idxs_to_update] - (thresh / np.linalg.norm(beta[idxs_to_update]**2)**0.5) * beta[idxs_to_update]\n",
    "                posind = np.where(beta[idxs_to_update] > 0.)[0]\n",
    "                negind = np.where(beta[idxs_to_update] < 0.)[0]\n",
    "                po = beta[idxs_to_update].copy()\n",
    "                #print('potention', potentialoutput[0:2])\n",
    "                po[posind] = np.asarray(np.clip(potentialoutput[posind],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind] = np.asarray(np.clip(potentialoutput[negind],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[idxs_to_update] = po\n",
    "        #print('end', result[0:2])\n",
    "        return result\n",
    "\n",
    "    def _grad_L4loss(self, beta, X, y):\n",
    "        #print(beta.shape,X.shape,y.shape)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "        n_samples = np.float(X.shape[0])\n",
    "        z = np.dot(X, beta)\n",
    "        #grad_beta = 1. / n_samples * np.transpose(np.dot(np.transpose(z - y), X))\n",
    "        grad_beta = np.transpose(np.dot(np.transpose(z - y)**3, X))\n",
    "        #print('gb',grad_beta.shape)\n",
    "        return grad_beta\n",
    "    \n",
    "    def _loss(self,beta, reg_lambda, X, y):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta, X, y)\n",
    "        P = self._L1penalty(beta)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "    \n",
    "    def _logL(self,beta, X, y):\n",
    "        \"\"\"The log likelihood.\"\"\"\n",
    "        #print('beginlogL', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y),y.shape,beta.shape,X.shape,)\n",
    "        l = np.dot(X, beta)\n",
    "        logL = -0.25 * np.sum((y - l)**4)\n",
    "        #print('endlogL',logL)\n",
    "        return logL\n",
    "    \n",
    "    def _L4loss(self,beta,X,y):\n",
    "        #print('beginl2', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y), y.shape)\n",
    "        output = -self._logL(beta, X, y)\n",
    "        #print('outl2',output)\n",
    "        return(output)\n",
    "    \n",
    "    def _L1penalty(self, beta):\n",
    "        \"\"\"The L1 penalty\"\"\"\n",
    "        # Compute the L1 penalty\n",
    "        group = self.group\n",
    "        group_ids = np.unique(self.group)\n",
    "        L1penalty = 0.0\n",
    "        for group_id in group_ids:\n",
    "            L1penalty += np.linalg.norm(beta[self.group == group_id]**2, 2)**(0.5)\n",
    "        return L1penalty\n",
    "    \n",
    "    #def fhatlambda(self,lamb,x,y):\n",
    "    def fhatlambda(self,lamb,betanew,betaold):\n",
    "        xs = self.xs\n",
    "        ys = self.ys\n",
    "        #print(ys.shape,'fhatlam')\n",
    "        #print(self._L2loss(betaold,xs,ys),self._L2loss(betanew,xs,ys),'old','new') \n",
    "        output = self._L4loss(betaold,xs,ys) + np.dot(self._grad_L4loss(betaold,xs,ys).transpose(),(betanew-betaold)) + (1/(2*lamb)) * np.linalg.norm(betanew-betaold)**2\n",
    "        return(output)\n",
    "    \n",
    "    #_btalgorithm(yk,lamb,.5,1000, rl)\n",
    "    def _btalgorithm(self,bet,lam,b,maxx,rl):\n",
    "        \n",
    "        #print('lam',lam)\n",
    "        X = self.xs\n",
    "        y = self.ys\n",
    "        #print('beginbt', np.linalg.norm(y))\n",
    "        #print('beginbt',self._L2loss(bet,X,y))\n",
    "        #print(np.linalg.norm(bet))\n",
    "        grad_beta = self._grad_L4loss(beta = bet, X = X, y = y)\n",
    "        for i in range(maxx):\n",
    "            betn = bet - lam * grad_beta\n",
    "            z = self._prox(betn, lam * rl)\n",
    "            fz = self._L4loss(z,X,y)\n",
    "            #print(fz,'fz')\n",
    "            fhatz = self.fhatlambda(lam,z, bet)\n",
    "            if fz <= fhatz:\n",
    "            #print(fhatz - fz)\n",
    "            #if 0 <= 1:\n",
    "                break\n",
    "            lam = b*lam\n",
    "        return(z,lam)\n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "\n",
    "        group  = self.group\n",
    "        print(group.shape)\n",
    "        lambdas = self.reg_lambda\n",
    "        parameter = self.parameter\n",
    "        X = self.xs\n",
    "        #print(X.shape)\n",
    "        y = self.ys\n",
    "        \n",
    "        np.random.RandomState(0)\n",
    "        group = np.asarray(group, dtype = np.int64)\n",
    "\n",
    "        #print(group.shape[0])\n",
    "        group.dtype = np.int64\n",
    "        #print(group.shape[0])\n",
    "        #print(X.shape[1])\n",
    "        if group.shape[0] != X.shape[1]:\n",
    "            raise ValueError('group should be (n_features,)')\n",
    "\n",
    "        # type check for data matrix\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError('Input data should be of type ndarray (got %s).'\n",
    "                             % type(X))\n",
    "\n",
    "        n_features = np.float(X.shape[1])\n",
    "        n_features = np.int64(n_features)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "            self.ys = y\n",
    "        #print(y.shape)\n",
    "        n_classes = 1\n",
    "        n_classes = np.int64(n_classes)\n",
    "\n",
    "        beta_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        fit_params = list()\n",
    "        \n",
    "        for l, rl in enumerate(lambdas):\n",
    "            fit_params.append({'beta': beta_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            tol = self.tol\n",
    "            alpha = 1.\n",
    "            beta = np.zeros([n_features, n_classes])\n",
    "            beta = fit_params[-1]['beta']\n",
    "            #print('losser',self._L2loss(beta,X,y))\n",
    "            g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L4,PEN = list(), list() , list(), list()\n",
    "            lamb = self.learning_rate\n",
    "            bm1 = beta.copy()\n",
    "            bm2 = beta.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                L.append(self._loss(beta, rl, X, y))\n",
    "                L4.append(self._L4loss(beta,X,y))\n",
    "                PEN.append(self._L1penalty(beta))\n",
    "                w = (t / (t+ 3))\n",
    "                yk = beta + w*(bm1 - bm2)\n",
    "                #print('losser',self._L2loss(yk,X,y))\n",
    "                #print('beforebt',np.linalg.norm(yk),np.linalg.norm(X),np.linalg.norm(y))\n",
    "                beta , lamb = self._btalgorithm(yk,lamb,.5,1000, rl)\n",
    "                #X = self.xs\n",
    "                #y = self.ys\n",
    "                #print('losser2',self._L2loss(beta,X,y))\n",
    "                bm2 = bm1.copy()\n",
    "                bm1 = beta.copy()\n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "                    \n",
    "            #print(beta)\n",
    "            fit_params[-1]['beta'] = beta\n",
    "            self.lossresults[rl] = L\n",
    "            self.l4loss[rl] = L4\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "            #print(L)\n",
    "        # Update the estimated variables\n",
    "        \n",
    "        self.fit_ = fit_params\n",
    "        self.ynull_ = np.mean(y)\n",
    "\n",
    "        # Return\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_betas_sam(xtrain, ytrain, groups, lambdas, n, q, max_iter, tol, learning_rate):\n",
    "\n",
    "        p = len(np.unique(groups))\n",
    "        models = GLM(xs=xtrain, ys=ytrain,\n",
    "                     tol=tol,\n",
    "                     group=groups,\n",
    "                     learning_rate=learning_rate,\n",
    "                     max_iter=max_iter,\n",
    "                     # reg_lambda=np.logspace(np.log(100), np.log(0.01), 5, base=np.exp(1)))\n",
    "                     reg_lambda=lambdas,\n",
    "                     parameter=.5)\n",
    "        models.fit()\n",
    "        nlam = len(lambdas)\n",
    "        organizedbetas = np.zeros((nlam, q, n, p))\n",
    "        for l in range(nlam):\n",
    "            organizedbetas[l, :, :, :] = np.reshape(models.fit_[l]['beta'], (q, n, p))\n",
    "        # return(models, organizedbetas)\n",
    "        return (organizedbetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "converged 1e-05\n"
     ]
    }
   ],
   "source": [
    "cs4 = get_betas_sam(xtrain, ytrain, groups, np.asarray([.00001]), 1, 2, 1000*itermax, .01*tol,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.61016077,  0.3683157 , -0.05684229]],\n",
       "\n",
       "        [[-0.04207556,  0.05078129,  1.93555574]]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(cs4[:,i]**4)**(.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.72684425,  0.27314582, -0.05462804]],\n",
       "\n",
       "        [[-0.03714508,  0.03714565,  1.9925313 ]]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.61194794,  0.36652525, -0.05654459]],\n",
       "\n",
       "        [[-0.03983996,  0.04860065,  1.936     ]]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 9.62699156e-01,  1.57577983e-02, -2.68149790e-04]],\n",
       "\n",
       "        [[-1.01714086e-03,  3.13708838e-03,  1.94508520e+00]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.923548335528736"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([np.sum(cs4[0][:,0,:][:,i]**4)**(.25) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs4[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum([np.sum(cs4[0][:,0,:][:,i]**2)**(.5) for i in range(3)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifold_env_april2",
   "language": "python",
   "name": "manifold_env_april2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
