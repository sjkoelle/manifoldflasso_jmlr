{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/manifoldflasso_jmlr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/queues.py\", line 345, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/queues.py\", line 346, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/queues.py\", line 345, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/connection.py\", line 219, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/synchronize.py\", line 102, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/synchronize.py\", line 102, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/connection.py\", line 410, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/multiprocess/connection.py\", line 382, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import random\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from matplotlib.lines import Line2D\n",
    "from pylab import rcParams\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "from shutil import copyfile\n",
    "rcParams['figure.figsize'] = 25, 10\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "now = datetime.datetime.now().strftime(\"%B_%d_%Y_%H_%M_%S\")\n",
    "workingdirectory = os.popen('git rev-parse --show-toplevel').read()[:-1]\n",
    "sys.path.append(workingdirectory)\n",
    "os.chdir(workingdirectory)\n",
    "from codes.experimentclasses.RigidEthanolPCA import RigidEthanolPCA\n",
    "from codes.otherfunctions.get_dictionaries import get_all_atoms_4\n",
    "from codes.otherfunctions.get_grads import get_grads\n",
    "from codes.otherfunctions.multirun import get_support_recovery_lambda\n",
    "from codes.otherfunctions.multirun import get_lower_interesting_lambda\n",
    "from codes.otherfunctions.multirun import get_coeffs_and_lambdas\n",
    "from codes.otherfunctions.multirun import get_support\n",
    "from codes.otherfunctions.multiplot import plot_support_2d\n",
    "from codes.otherfunctions.multiplot import plot_reg_path_ax_lambdasearch\n",
    "from codes.otherfunctions.multiplot import plot_gs_v_dgnorm\n",
    "from codes.otherfunctions.multiplot import plot_dot_distributions\n",
    "from codes.otherfunctions.multirun import get_cosines\n",
    "from codes.flasso.Replicate import Replicate\n",
    "from codes.otherfunctions.multirun import get_olsnorm_and_supportsbrute\n",
    "from codes.otherfunctions.multiplot import highlight_cell\n",
    "\n",
    "#set parameters\n",
    "n = 10000 #number of data points to simulate\n",
    "nsel = 2 #number of points to analyze with lasso\n",
    "itermax = 1000 #maximum iterations per lasso run\n",
    "tol = 1e-10 #convergence criteria for lasso\n",
    "#lambdas = np.asarray([0,.01,.1,1,10,100], dtype = np.float16)#lambda values for lasso\n",
    "lambdas = np.asarray(np.hstack([np.asarray([0]),np.logspace(-3,1,11)]), dtype = np.float16)\n",
    "n_neighbors = 1000 #number of neighbors in megaman\n",
    "m = 3 #number of embedding dimensions (diffusion maps)\n",
    "#diffusion_time = 1. #diffusion time controls gaussian kernel radius per gradients paper\n",
    "diffusion_time = 0.05 #(yuchia suggestion)\n",
    "dim = 2 #manifold dimension\n",
    "dimnoise = 2\n",
    "natoms = 9\n",
    "cores = 3 #number of cores for parallel processing\n",
    "cor = 0.0 #correlation for noise\n",
    "var = 0.00001 #variance scaler for noise\n",
    "cor = 0.0 #correlation for noise\n",
    "var = 0.00001 #variance scaler for noise\n",
    "ii = np.asarray([0,0,0,0,1,1,1,2]) # atom adjacencies for dihedral angle computation\n",
    "jj = np.asarray([1,2,3,4,5,6,7,8])\n",
    "\n",
    "#run experiment\n",
    "atoms4 = np.asarray([[6,1,0,4],[4,0,2,8],[7,6,5,1],[3,0,2,4]],dtype = int)\n",
    "nreps = 25\n",
    "lambda_max = 1\n",
    "max_search = 30\n",
    "\n",
    "new_MN = True\n",
    "new_grad = True\n",
    "savename = 'rigidethanol_110120_alltorsions'\n",
    "savefolder = 'rigidethanol'\n",
    "loadfolder = 'rigidethanol'\n",
    "loadname = 'rigidethanol_110120_alltorsions'\n",
    "if new_MN == True:\n",
    "    experiment = RigidEthanolPCA(dim, cor, var, ii, jj, cores, False, atoms4)\n",
    "    experiment.M, experiment.Mpca, projector = experiment.generate_data(noise=False)\n",
    "    experiment.q = m\n",
    "    experiment.m = m\n",
    "    experiment.dimnoise = dimnoise\n",
    "    experiment.projector = projector\n",
    "    experiment.Mpca.geom = experiment.Mpca.compute_geom(diffusion_time, n_neighbors)\n",
    "    experiment.N = experiment.Mpca.get_embedding3(experiment.Mpca.geom, m, diffusion_time, dim)\n",
    "    # with open(workingdirectory + '/untracked_data/embeddings/' + savefolder + '/' + savename + '.pkl' ,\n",
    "    #          'wb') as output:\n",
    "    #      pickle.dump(experiment, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from codes.otherfunctions.get_dictionaries import get_atoms_4\n",
    "#atoms4,p = get_atoms_4(natoms,ii,jj)\n",
    "experiment.p = 4\n",
    "experiment.atoms4 = atoms4\n",
    "experiment.itermax = itermax\n",
    "experiment.tol = tol\n",
    "experiment.dnoise = dim\n",
    "experiment.nreps = nreps\n",
    "experiment.nsel = nsel\n",
    "#experiment.folder = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import matplotlib.colors\n",
    "import spams\n",
    "from collections import OrderedDict\n",
    "from pylab import rcParams\n",
    "from codes.flasso.GLMaccelerated import GLM\n",
    "\n",
    "rcParams['figure.figsize'] = 25, 10\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    output = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return (output)\n",
    "\n",
    "\n",
    "class FlassoExperiment:\n",
    "    \"\"\"\n",
    "    FlassoExperiment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        2 + 2\n",
    "\n",
    "    def get_norms(self, differential):\n",
    "        n = differential.shape[0]\n",
    "        # could be p, or q\n",
    "        p = differential.shape[1]\n",
    "        d = differential.shape[2]\n",
    "\n",
    "        differential_normalized = np.zeros(differential.shape)\n",
    "        vectornorms = np.zeros((n, p))\n",
    "        for i in range(n):\n",
    "            for j in range(p):\n",
    "                if np.linalg.norm(differential[i, j, :]) > 0:\n",
    "                    vectornorms[i, j] = np.linalg.norm(differential[i, j, :])\n",
    "\n",
    "        psum = np.sum(vectornorms, axis=0)\n",
    "        return (psum / n)\n",
    "\n",
    "    def _flatten_coefficient(self, coeff):\n",
    "        n = coeff.shape[1]\n",
    "        p = coeff.shape[2]\n",
    "        q = coeff.shape[0]\n",
    "\n",
    "        output = np.zeros((n * p * q))\n",
    "        for k in range(q):\n",
    "            for i in range(n):\n",
    "                output[((k * n * p) + (i * p)):((k * n * p) + (i + 1) * p)] = coeff[k, i, :]\n",
    "        return (output)\n",
    "\n",
    "    def get_l2loss(self, coeffs, ys, xs):\n",
    "\n",
    "        n = coeffs.shape[2]\n",
    "        nlam = coeffs.shape[0]\n",
    "        output = np.zeros(nlam)\n",
    "        for i in range(nlam):\n",
    "            coeffvec = self._flatten_coefficient(coeffs[i])\n",
    "            output[i] = np.sum((ys - np.dot(coeffvec, xs.transpose())) ** 2)\n",
    "        output = output / n\n",
    "        return (output)\n",
    "\n",
    "    def normalize(self, differential):\n",
    "        n = differential.shape[0]\n",
    "        # could be p, or q\n",
    "        p = differential.shape[1]\n",
    "        d = differential.shape[2]\n",
    "\n",
    "        gammas = np.sum(np.sum(differential ** 2, axis=2), axis=0) ** (.5)\n",
    "        normed = np.swapaxes(differential, 1, 2) / gammas\n",
    "        #print(normed.shape)\n",
    "        normed = np.swapaxes(normed, 1, 2)\n",
    "    #\n",
    "    #     differential_normalized = np.zeros(differential.shape)\n",
    "    #     vectornorms = np.zeros((n, p))\n",
    "    #     for i in range(n):\n",
    "    #         for j in range(p):\n",
    "    #             if np.linalg.norm(differential[i, j, :]) > 0:\n",
    "    #                 vectornorms[i, j] = np.linalg.norm(differential[i, j, :])\n",
    "    #     # psum = np.sum(vectornorms, axis = 0)\n",
    "    #     psum = np.sqrt(np.sum(vectornorms ** 2, axis=0))#np.sum(vectornorms ** 2, axis=0)#\n",
    "    #     for j in range(p):\n",
    "    #         if psum[j] > 0:\n",
    "    #             differential_normalized[:, j, :] = (differential[:, j, :] / psum[j])  # *n\n",
    "    #\n",
    "    #     return (differential_normalized)\n",
    "        return(normed)\n",
    "\n",
    "\n",
    "    def get_betas_sam(self, xtrain, ytrain, groups, lambdas, n, q, max_iter, tol, learning_rate):\n",
    "\n",
    "        p = len(np.unique(groups))\n",
    "        models = GLM(xs=xtrain, ys=ytrain,\n",
    "                     tol=tol,\n",
    "                     group=groups,\n",
    "                     learning_rate=learning_rate,\n",
    "                     max_iter=max_iter,\n",
    "                     # reg_lambda=np.logspace(np.log(100), np.log(0.01), 5, base=np.exp(1)))\n",
    "                     reg_lambda=lambdas,\n",
    "                     parameter=.5)\n",
    "        models.fit()\n",
    "        nlam = len(lambdas)\n",
    "        organizedbetas = np.zeros((nlam, q, n, p))\n",
    "        for l in range(nlam):\n",
    "            organizedbetas[l, :, :, :] = np.reshape(models.fit_[l]['beta'], (q, n, p))\n",
    "        # return(models, organizedbetas)\n",
    "        return (organizedbetas)\n",
    "\n",
    "    def construct_X(self, dg_M):\n",
    "        \"\"\" dg_M should have shape n x p x dim\n",
    "        \"\"\"\n",
    "        n = dg_M.shape[0]\n",
    "        dim = dg_M.shape[2]\n",
    "        p = dg_M.shape[1]\n",
    "\n",
    "        xmat = np.zeros((n * dim, n * p))\n",
    "        for i in range(n):\n",
    "            xmat[(i * dim):(i * dim + dim), (i * p):(i * p + p)] = dg_M[i, :, :].transpose()\n",
    "        b = [xmat] * dim\n",
    "        xmatq = scipy.linalg.block_diag(*b)\n",
    "        groups = np.tile(np.tile(np.asarray(np.linspace(start=0, stop=(p - 1), num=p), dtype=int), n), dim)\n",
    "\n",
    "        return (xmatq, list(groups))\n",
    "\n",
    "    def construct_X_js(self, dg_M):\n",
    "        \"\"\" dg_M should have shape n x p x dim\n",
    "        \"\"\"\n",
    "        n = dg_M.shape[0]\n",
    "        dim = dg_M.shape[2]\n",
    "        p = dg_M.shape[1]\n",
    "        q = self.q\n",
    "\n",
    "        xmat = np.zeros((n * dim, n * p))\n",
    "        for i in range(n):\n",
    "            xmat[(i * dim):(i * dim + dim), (i * p):(i * p + p)] = dg_M[i, :, :].transpose()\n",
    "        b = [xmat] * q\n",
    "        xmatq = scipy.linalg.block_diag(*b)\n",
    "        groups = np.zeros(n * p * q)\n",
    "        groups = np.tile(np.tile(np.asarray(np.linspace(start=0, stop=(p - 1), num=p), dtype=int), n), q)\n",
    "\n",
    "        return (xmatq, list(groups))\n",
    "\n",
    "    def construct_X_js_subset(self, dg_M, selind):\n",
    "        dg_M_subset = np.zeros(dg_M.shape)\n",
    "        dg_M_subset[:, selind, :] = dg_M[:, selind, :]\n",
    "        output = self.construct_X_js(dg_M_subset)\n",
    "        return (output)\n",
    "\n",
    "    def construct_Y(self, df_M):\n",
    "        \"\"\" df_M should have shape n x dim x dim\n",
    "        \"\"\"\n",
    "        n = df_M.shape[0]\n",
    "        dim = df_M.shape[1]\n",
    "\n",
    "        #reorg1 = np.swapaxes(df_M, 0, 1)\n",
    "        yvec = np.reshape(np.swapaxes(df_M,0,1), (n * dim * dim))\n",
    "        return (yvec)\n",
    "\n",
    "    def construct_Y_js(self, df_M, dim=None):\n",
    "        \"\"\" df_M should have shape n x dim x q\n",
    "        \"\"\"\n",
    "        n = df_M.shape[0]\n",
    "        q = self.q\n",
    "        if dim == None:\n",
    "            dim = self.dim\n",
    "\n",
    "        reorg1 = np.swapaxes(df_M, 0, 2)\n",
    "        reorg2 = np.swapaxes(reorg1, 2, 1)\n",
    "        # yvec = np.reshape(reorg2, (n*dim*dim))\n",
    "        yvec = np.reshape(reorg2, (n * dim * q))\n",
    "        return (yvec)\n",
    "\n",
    "    def plot_convergence(self, models, name='lossplot.pdf'):\n",
    "        for key in list(models.keys()):\n",
    "            # key = list(models.keys())[0]\n",
    "            xval = np.log(np.asarray(list(range(len(list(models[key].lossresults.values())[0])))) + 1)\n",
    "            y = list(models[key].lossresults.values())[0]\n",
    "            plt.plot(xval, y)\n",
    "        plt.legend(list(models.keys()), loc='upper right')\n",
    "        plt.savefig(name)\n",
    "\n",
    "    def plot_convergence_sam(self, models, name='lossplot.pdf'):\n",
    "        for key in list(models.lossresults.keys()):\n",
    "            # key = list(models.keys())[0]\n",
    "            xval = np.log(np.asarray(list(range(len(models.lossresults[key])))) + 1)\n",
    "            y = models.lossresults[key]\n",
    "            plt.plot(xval, y)\n",
    "        plt.legend(list(models.lossresults.keys()), loc='upper right')\n",
    "        plt.savefig(name)\n",
    "\n",
    "    def plot_bh(self, coeffs, nsample_pts, p, name='beta'):\n",
    "        dim = self.dim\n",
    "        # p = self.p\n",
    "        n = nsample_pts\n",
    "        nlam = coeffs.shape[0]\n",
    "\n",
    "        for l in range(len(lambdas)):\n",
    "            if dim > 1:\n",
    "                fig, axes = plt.subplots(1, q, figsize=(15, 30))\n",
    "                for k in range(q):\n",
    "                    tempplot = axes[k].imshow(coeffs[l, k, :, :])\n",
    "                    plt.colorbar(tempplot, ax=axes[k])\n",
    "            if dim == 1:\n",
    "                k = 0\n",
    "                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                ax.imshow(coeffs[l, k, :, :])\n",
    "            fig.savefig('lambda' + name + 'heatmap.pdf')\n",
    "\n",
    "    def compute_penalty2(self, coeffs):\n",
    "        n = coeffs.shape[2]\n",
    "        nlam = coeffs.shape[0]\n",
    "        q = coeffs.shape[1]\n",
    "        p = coeffs.shape[3]\n",
    "\n",
    "        # p = self.p\n",
    "        pen = np.zeros(nlam)\n",
    "        for l in range(nlam):\n",
    "            norm2 = np.zeros(p)\n",
    "            for j in range(p):\n",
    "                norm2[j] = np.linalg.norm(coeffs[l, :, :, j])\n",
    "            pen[l] = np.sum(norm2)\n",
    "        pen = pen / n\n",
    "        return (pen)\n",
    "\n",
    "    def plot_penalty(self, coeffs, xaxis, xlabel, xlog, ylog, title, filename):\n",
    "        ylabel = r\"$\\frac{1}{n}\\displaystyle \\|\\beta\\|_{1,2}$ \"\n",
    "\n",
    "        if xlog:\n",
    "            filename = filename + 'xlog'\n",
    "        if ylog:\n",
    "            filename = filename + 'ylog'\n",
    "\n",
    "        pens = self.compute_penalty2(coeffs)\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(15, 15))\n",
    "        axes.plot(xaxis, pens, 'go--', linewidth=5, markersize=0, alpha=1)\n",
    "        axes.set_ylim(bottom=1e-4, top=pens.max())\n",
    "        if xlog:\n",
    "            axes.semilogx()\n",
    "        if ylog:\n",
    "            axes.semilogy()\n",
    "        axes.tick_params(labelsize=50)\n",
    "        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=50)\n",
    "        fig.text(0.05, 0.5, ylabel, ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        fig.savefig(filename + 'penalty' + str(n))\n",
    "\n",
    "    def plot_predictions(self, coeffs, xs, ys, xaxis, xlabe, xlog, ylog, title, filename):\n",
    "        2 + 2\n",
    "\n",
    "    def plot_l2loss(self, coeffs, xs, ys, xaxis, xlabel, xlog, ylog, title, filename):\n",
    "\n",
    "        ylabel = r\"$\\displaystyle \\frac{1}{n}\\|y - x\\beta\\|_2^2$\"\n",
    "\n",
    "        if xlog:\n",
    "            filename = filename + 'xlog'\n",
    "        if ylog:\n",
    "            filename = filename + 'ylog'\n",
    "\n",
    "        losses = self.get_l2loss(coeffs, ys, xs)\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(15, 15))\n",
    "        axes.plot(xaxis, losses, 'go--', linewidth=5, markersize=0, alpha=1)\n",
    "        axes.set_ylim(bottom=1e-4, top=losses.max())\n",
    "        if xlog:\n",
    "            axes.semilogx()\n",
    "        if ylog:\n",
    "            axes.semilogy()\n",
    "        axes.tick_params(labelsize=50)\n",
    "        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=50)\n",
    "        fig.text(0.05, 0.5, ylabel, ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        fig.savefig(filename + 'loss' + str(n))\n",
    "\n",
    "    def plot_beta_paths_best(self, coeffs, xaxis, gnames, fnames, xlabel, title, filename, xlog, ylog,\n",
    "                             norm_betas=False):\n",
    "        if xlog:\n",
    "            filename = filename + 'xlog'\n",
    "        if ylog:\n",
    "            filename = filename + 'ylog'\n",
    "        if norm_betas:\n",
    "            print(filename)\n",
    "            filename = filename + 'norm'\n",
    "            print(filename)\n",
    "            ylabel = r\"$\\displaystyle \\|\\hat \\beta_{j}\\|_2$\"\n",
    "        else:\n",
    "            ylabel = r\"$\\displaystyle \\hat \\beta_{ij}$\"\n",
    "        rcParams['axes.titlesize'] = 30\n",
    "        plt.rc('text', usetex=True)\n",
    "\n",
    "        p = coeffs.shape[3]\n",
    "        n = coeffs.shape[2]\n",
    "        q = coeffs.shape[1]\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=p)\n",
    "        cmap = plt.cm.rainbow\n",
    "\n",
    "        if norm_betas:\n",
    "            maxes = np.zeros(q)\n",
    "            for k in range(q):\n",
    "                # for j in range(p):\n",
    "                maxes[k] = np.linalg.norm(coeffs[:, k, :, :], axis=1).max()\n",
    "            normax = maxes.max()\n",
    "        if q > 1:\n",
    "            fig, axes = plt.subplots(1, q, figsize=((15 * q), 15))\n",
    "            for k in range(q):\n",
    "                for j in range(p):\n",
    "                    if norm_betas == False:\n",
    "                        for i in range(n):\n",
    "                            axes[k].plot(xaxis, coeffs[:, k, i, j], 'go--', linewidth=5, markersize=0, alpha=.2,\n",
    "                                         color=cmap(norm(j)), label=gnames[j])\n",
    "                    else:\n",
    "                        axes[k].plot(xaxis, np.linalg.norm(coeffs[:, k, :, j], axis=1), 'go--', linewidth=5,\n",
    "                                     markersize=0, alpha=1, color=cmap(norm(j)), label=gnames[j])\n",
    "                # axes[k].set_title(fnames[k], fontsize=30)\n",
    "                axes[k].tick_params(labelsize=50)\n",
    "                if xlog:\n",
    "                    axes[k].semilogx()\n",
    "                if ylog:\n",
    "                    axes[k].semilogy()\n",
    "                if norm_betas == True:\n",
    "                    axes[k].set_ylim(bottom=1e-4, top=normax)\n",
    "                else:\n",
    "                    if ylog == True:\n",
    "                        axes[k].set_ylim(bottom=1e-4, top=coeffs.max())  # adjust the max leaving min unchanged\n",
    "            handles, labels = axes[0].get_legend_handles_labels()\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, q, figsize=((15 * q), 15))\n",
    "            for j in range(p):\n",
    "                if norm_betas == False:\n",
    "                    for i in range(n):\n",
    "                        axes.plot(xaxis, coeffs[:, 0, i, j], 'go--', linewidth=5, markersize=0, alpha=.2,\n",
    "                                  color=cmap(norm(j)), label=gnames[j])\n",
    "                else:\n",
    "                    axes.plot(xaxis, np.linalg.norm(coeffs[:, 0, :, j], axis=1), 'go--', linewidth=5, markersize=0,\n",
    "                              alpha=1, color=cmap(norm(j)), label=gnames[j])\n",
    "            # axes.set_title(fnames[0], fontsize=30)\n",
    "            axes.tick_params(labelsize=50)\n",
    "            if xlog:\n",
    "                axes.semilogx()\n",
    "            if ylog:\n",
    "                axes.semilogy()\n",
    "            if norm_betas == True:\n",
    "                axes.set_ylim(bottom=1e-4, top=normax)\n",
    "            else:\n",
    "                if ylog == True:\n",
    "                    axes.set_ylim(bottom=1e-4, top=coeffs.max())  # adjust the max leaving min unchanged\n",
    "            handles, labels = axes.get_legend_handles_labels()\n",
    "\n",
    "        by_label = OrderedDict(zip(labels, handles))\n",
    "        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=50)\n",
    "        fig.text(0.05, 0.5, ylabel, ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        fig.subplots_adjust(right=0.75)\n",
    "        leg_ax = fig.add_axes([.96, 0.15, 0.05, 0.7])\n",
    "        leg_ax.axis('off')\n",
    "        # leg = leg_ax.legend(by_label.values(), by_label.keys(), prop={'size': 55})\n",
    "        leg = leg_ax.legend(by_label.values(), by_label.keys(), prop={'size': 25})\n",
    "        leg.set_title('Group', prop={'size': 45})\n",
    "        for l in leg.get_lines():\n",
    "            l.set_alpha(1)\n",
    "        fig.savefig(filename + 'beta_paths_log_n' + str(n))\n",
    "\n",
    "    def plot_beta_paths_best2(self, coeffs, xaxis, gnames, fnames, xlabel, title, filename, xlog, ylog, colors, lines,\n",
    "                              norm_betas=False):\n",
    "        if xlog:\n",
    "            filename = filename + 'xlog'\n",
    "        if ylog:\n",
    "            filename = filename + 'ylog'\n",
    "        if norm_betas:\n",
    "            print(filename)\n",
    "            filename = filename + 'norm'\n",
    "            print(filename)\n",
    "            ylabel = r\"$\\displaystyle \\|\\hat \\beta_{j}\\|_2$\"\n",
    "        else:\n",
    "            ylabel = r\"$\\displaystyle \\hat \\beta_{ij}$\"\n",
    "        rcParams['axes.titlesize'] = 30\n",
    "        plt.rc('text', usetex=True)\n",
    "\n",
    "        p = coeffs.shape[3]\n",
    "        n = coeffs.shape[2]\n",
    "        q = coeffs.shape[1]\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=p)\n",
    "        cmap = plt.cm.rainbow\n",
    "\n",
    "        if norm_betas:\n",
    "            maxes = np.zeros(q)\n",
    "            for k in range(q):\n",
    "                # or j in range(p):\n",
    "                maxes[k] = np.linalg.norm(coeffs[:, k, :, :], axis=1).max()\n",
    "            normax = maxes.max()\n",
    "        if q > 1:\n",
    "            fig, axes = plt.subplots(1, q, figsize=((15 * q), 15))\n",
    "            for k in range(q):\n",
    "                for j in range(p):\n",
    "                    if norm_betas == False:\n",
    "                        for i in range(n):\n",
    "                            axes[k].plot(xaxis, coeffs[:, k, i, j], 'go--', linewidth=5, markersize=0, alpha=.2,\n",
    "                                         color=colors[j], linestyle=lines[j], label=gnames[j])\n",
    "                    else:\n",
    "                        axes[k].plot(xaxis, np.linalg.norm(coeffs[:, k, :, j], axis=1), 'go--', linewidth=5,\n",
    "                                     markersize=0, alpha=1, color=colors[j], linestyle=lines[j], label=gnames[j])\n",
    "                # axes[k].set_title(fnames[k], fontsize=30)\n",
    "                axes[k].tick_params(labelsize=50)\n",
    "                if xlog:\n",
    "                    axes.set_xscale('symlog')\n",
    "                if ylog:\n",
    "                    axes.set_yscale('symlog')\n",
    "                if norm_betas == True:\n",
    "                    axes[k].set_ylim(bottom=1e-4, top=10 * normax)\n",
    "                else:\n",
    "                    if ylog == True:\n",
    "                        axes[k].set_ylim(bottom=1e-4, top=coeffs.max())  # adjust the max leaving min unchanged\n",
    "            handles, labels = axes[0].get_legend_handles_labels()\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, q, figsize=((15 * q), 15))\n",
    "            for j in range(p):\n",
    "                if norm_betas == False:\n",
    "                    for i in range(n):\n",
    "                        axes.plot(xaxis, coeffs[:, 0, i, j], 'go--', linewidth=5, markersize=0, alpha=.2,\n",
    "                                  color=colors[j], linestyle=lines[j], label=gnames[j])\n",
    "                else:\n",
    "                    axes.plot(xaxis, np.linalg.norm(coeffs[:, 0, :, j], axis=1), 'go--', linewidth=5, markersize=0,\n",
    "                              alpha=1, color=colors[j], linestyle=lines[j], label=gnames[j])\n",
    "            # axes.set_title(fnames[0], fontsize=30)\n",
    "            axes.tick_params(labelsize=50)\n",
    "            axes.set_xscale('symlog')\n",
    "            axes.set_yscale('symlog')\n",
    "            if norm_betas == True:\n",
    "                axes.set_ylim(bottom=0, top=10 * normax)\n",
    "            else:\n",
    "                if ylog == True:\n",
    "                    axes.set_ylim(bottom=0, top=coeffs.max())  # adjust the max leaving min unchanged\n",
    "            handles, labels = axes.get_legend_handles_labels()\n",
    "\n",
    "        by_label = OrderedDict(zip(labels, handles))\n",
    "        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=50)\n",
    "        fig.text(0.05, 0.5, ylabel, ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        fig.subplots_adjust(right=0.75)\n",
    "        leg_ax = fig.add_axes([.96, 0.15, 0.05, 0.7])\n",
    "        leg_ax.axis('off')\n",
    "        # leg = leg_ax.legend(by_label.values(), by_label.keys(), prop={'size': 55})\n",
    "        leg = leg_ax.legend(by_label.values(), by_label.keys(), prop={'size': 25})\n",
    "        leg.set_title('Group', prop={'size': 45})\n",
    "        for l in leg.get_lines():\n",
    "            l.set_alpha(1)\n",
    "        fig.savefig(filename + 'beta_paths_log_n2' + str(n))\n",
    "\n",
    "    def get_cosines(self, dg):\n",
    "        n = dg.shape[0]\n",
    "        p = dg.shape[1]\n",
    "        d = dg.shape[2]\n",
    "\n",
    "        coses = np.zeros((n, p, p))\n",
    "        for i in range(n):\n",
    "            for j in range(p):\n",
    "                for k in range(p):\n",
    "                    coses[i, j, k] = cosine_similarity(dg[i, j, :], dg[i, k,\n",
    "                                                                    :])  # sklearn.metrics.pairwise.cosine_similarity(X = np.reshape(dg[:,i,:], (1,d*n)),Y = np.reshape(dg[:,j,:], (1,d*n)))[0][0]\n",
    "        cos_summary = np.abs(coses).sum(axis=0) / n\n",
    "        return (cos_summary)\n",
    "\n",
    "    def plot_norms(self, norms, filename):\n",
    "        plt.imshow(norms)\n",
    "        plt.colorbar()\n",
    "        plt.savefig(filename + 'norms')\n",
    "\n",
    "    def plot_cosines(self, cos_sumary, filename):\n",
    "        plt.imshow(cos_sumary)\n",
    "        plt.colorbar()\n",
    "        plt.savefig(filename + 'cosines')\n",
    "\n",
    "    def plot_pairwise_solutions(self, coeffs, lambdas, gnames, fnames, title, filename, log):\n",
    "\n",
    "        if log:\n",
    "            filename = filename + 'xlog'\n",
    "            filename = filename + 'ylog'\n",
    "        if norm_betas:\n",
    "            print(filename)\n",
    "            filename = filename + 'norm'\n",
    "            print(filename)\n",
    "\n",
    "        rcParams['axes.titlesize'] = 30\n",
    "        plt.rc('text', usetex=True)\n",
    "        p = coeffs.shape[3]\n",
    "        n = coeffs.shape[2]\n",
    "        nlam = coeffs.shape[0]\n",
    "        fig, axes = plt.subplots(p, q, figsize=((15 * q), (15 * p)))\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=nlam)\n",
    "        cmap = plt.cm.rainbow\n",
    "        colors = cmap(norm(range(nlam)))\n",
    "        for k1 in range(q):\n",
    "            for k2 in range(q):\n",
    "                for j1 in range(p):\n",
    "                    for j2 in range(p):\n",
    "                        for l in range(nlam):\n",
    "                            axes[(k1 * q + j1), (k2 * q + j2)].plot(coeffs[l, k1, :, j1], coeffs[l, k2, :, j2],\n",
    "                                                                    alpha=.2, color=cmap(norm(l)), labels=lambdas[l])\n",
    "                            axes[(k1 * q + j1), (k2 * q + j2)].set_xlabel(fnames[k1] + gnames[j1], fontsize=30)\n",
    "                            axes[(k1 * q + j1), (k2 * q + j2)].set_ylabel(fnames[k1] + gnames[j1], fontsize=30)\n",
    "                            axes[(k1 * q + j1), (k2 * q + j2)].tick_params(labelsize=50)\n",
    "                            if log:\n",
    "                                axes[(k1 * q + j1), (k2 * q + j2)].semilogx()\n",
    "                                axes[(k1 * q + j1), (k2 * q + j2)].semilogy()\n",
    "\n",
    "        handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "        by_label = OrderedDict(zip(labels, handles))\n",
    "        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=50)\n",
    "        fig.text(0.06, 0.5, ylabel, ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        fig.subplots_adjust(right=0.75)\n",
    "        leg_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "        leg_ax.axis('off')\n",
    "        leg = leg_ax.legend(by_label.values(), by_label.keys(), prop={'size': 55})\n",
    "        leg.set_title('Group', prop={'size': 45})\n",
    "        for l in leg.get_lines():\n",
    "            l.set_alpha(1)\n",
    "        fig.savefig(filename + 'beta_paths_log_n' + str(n))\n",
    "\n",
    "    def normalize_xaxis(self, coeff, groups):\n",
    "\n",
    "        # groups = np.asarray(may_experiment.groups)\n",
    "        tempsum = 0\n",
    "        i = 0\n",
    "        ngroups = coeff.shape[3]\n",
    "        for j in range(ngroups):\n",
    "            tempsum = tempsum + np.linalg.norm(coeff[i, :, :, j])\n",
    "        beta0norm = tempsum\n",
    "\n",
    "        xaxis = np.zeros(len(lambdas))\n",
    "        for i in range(len(lambdas)):\n",
    "            tempsum = 0\n",
    "            for j in range(ngroups):\n",
    "                tempsum = tempsum + np.linalg.norm(coeff[i, :, :, j])\n",
    "            xaxis[i] = tempsum / beta0norm\n",
    "        return (xaxis)\n",
    "\n",
    "    def get_betas_spam2(self, xs, ys, groups, lambdas, n, q, itermax, tol):\n",
    "\n",
    "        # n = xs.shape[0]\n",
    "        p = len(np.unique(groups))\n",
    "        lambdas = np.asarray(lambdas, dtype=np.float64)\n",
    "        yadd = np.expand_dims(ys, 1)\n",
    "        groups = np.asarray(groups, dtype=np.int32) + 1\n",
    "        W0 = np.zeros((xs.shape[1], yadd.shape[1]), dtype=np.float32)\n",
    "        Xsam = np.asfortranarray(xs, dtype=np.float32)\n",
    "        Ysam = np.asfortranarray(yadd, dtype=np.float32)\n",
    "        coeffs = np.zeros((len(lambdas), q, n, p))\n",
    "        for i in range(len(lambdas)):\n",
    "            # alpha = spams.fistaFlat(Xsam,Dsam2,alpha0sam,ind_groupsam,lambda1 = lambdas[i],mode = mode,itermax = itermax,tol = tol,numThreads = numThreads, regul = \"group-lasso-l2\")\n",
    "            # spams.fistaFlat(Y,X,W0,TRUE,numThreads = 1,verbose = TRUE,lambda1 = 0.05, it0 = 10, max_it = 200,L0 = 0.1, tol = 1e-3, intercept = FALSE,pos = FALSE,compute_gram = TRUE, loss = 'square',regul = 'l1')\n",
    "            output = spams.fistaFlat(Ysam, Xsam, W0, True, groups=groups, numThreads=-1, verbose=True,\n",
    "                                     lambda1=lambdas[i], it0=100, max_it=itermax, L0=0.5, tol=tol, intercept=False,\n",
    "                                     pos=False, compute_gram=True, loss='square', regul='group-lasso-l2', ista=False,\n",
    "                                     subgrad=False, a=0.1, b=1000)\n",
    "            coeffs[i, :, :, :] = np.reshape(output[0], (q, n, p))\n",
    "            # print(output[1])\n",
    "        return (coeffs)\n",
    "\n",
    "    def plot_coefficient_recovery(self, coeffs, coeffspred, lambdas, filename):\n",
    "\n",
    "        p = coeffs.shape[3]\n",
    "        q = coeffs.shape[1]\n",
    "        nlam = coeffs.shape[0]\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=nlam)\n",
    "        cmap = plt.cm.rainbow\n",
    "        colors = cmap(norm(range(p)))\n",
    "        organizedbetas = coeffs\n",
    "        ymin = organizedbetas.min() - 0.01\n",
    "        ymax = organizedbetas.max() + 0.01\n",
    "        cmap = plt.cm.rainbow\n",
    "        minimum = lambdas.min()\n",
    "        # eps = minimum / 10000\n",
    "        # lambdas = lambdas + eps\n",
    "\n",
    "        if q > 1:\n",
    "            fig, axes = plt.subplots(nrows=p, ncols=q, figsize=(15, 15))\n",
    "            for k in range(q):\n",
    "                for j in range(p):\n",
    "                    # print(k,j)\n",
    "                    for i in range(nlam):\n",
    "                        axes[j, k].scatter(coeffspred[k, :, j], coeffs[i, k, :, j], alpha=.1, color=cmap(norm(i)))\n",
    "                    axes[j, k].set_ylabel(r\"$\\displaystyle \\widehat{{ d_{{g_{}}} h_{{}}}}$\".format(j, k), fontsize=35)\n",
    "                    # axes[j,k].set_title('function = ' + str(k + 1) + 'group = ' + str(j+1), fontsize = 15)\n",
    "                    axes[j, k].set_xlabel(r\"$\\displaystyle  d_{{g_{}}}^? h_{{}}$\".format(j, k), fontsize=35)\n",
    "                    axes[j, k].tick_params(labelsize=40)\n",
    "                    # axes[j,k].semilogy()\n",
    "                    # axes[j,k].semilogx()\n",
    "        if q == 1:\n",
    "            fig, axes = plt.subplots(nrows=p, ncols=1, figsize=(15, 15))\n",
    "            for j in range(p):\n",
    "                # print(j)\n",
    "                for i in range(nlam):\n",
    "                    axes[j].scatter(coeffspred[0, :, j], coeffs[i, 0, :, j], alpha=.1, color=cmap(norm(i)))\n",
    "                axes[j].set_ylabel(r\"$\\displaystyle \\widehat{{d_{{g_{}}} h}}$\".format(j), fontsize=35)\n",
    "                # axes[j].set_xlabel(predictionlabels[j], fontsize = 35)\n",
    "                axes[j].set_xlabel(r\"$\\displaystyle d_{{g_{}}}^? h$\".format(j), fontsize=35, labelpad=-1)\n",
    "                axes[j].tick_params(labelsize=40)\n",
    "                # axes[j].semilogy()\n",
    "                # axes[j].semilogx()\n",
    "                # axes[j].set_xlabel('Predicted Beta', fontsize = 15)\n",
    "                # axes[j].set_title('group = ' + str(j+1), fontsize = 15)\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "        sm.set_array([])\n",
    "        fig.subplots_adjust(right=0.8)\n",
    "        cbar_ax = fig.add_axes([.85, 0.15, 0.05, 0.7])\n",
    "        cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "        cbar.ax.tick_params(labelsize=40)\n",
    "        cbar.set_label(r\"$\\displaystyle \\lambda$\", rotation=270, fontsize=50)\n",
    "        # fig.text(0.0, 0.5, 'Estimates', ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        # fig.text(0.5, 0.04, r\"$\\displaystyle d_g^? h$\", ha='center', va='center', fontsize=50)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        plt.subplots_adjust(wspace=.3, hspace=.7, left=0.2, bottom=0.1, right=0.85, top=0.85)\n",
    "        # plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.22, right=0.96, top=0.96)\n",
    "        fig.savefig(filename + 'coefficientrecovery' + str(n))\n",
    "\n",
    "    def plot_coefficient_error_distribution(self, coeffs, coeffspred, lambdas, filename):\n",
    "\n",
    "        p = coeffs.shape[3]\n",
    "        q = coeffs.shape[1]\n",
    "        nlam = coeffs.shape[0]\n",
    "        n_bins = 50\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=nlam)\n",
    "        cmap = plt.cm.rainbow\n",
    "        colors = cmap(norm(range(p)))\n",
    "        organizedbetas = coeffs\n",
    "        ymin = organizedbetas.min() - 0.01\n",
    "        ymax = organizedbetas.max() + 0.01\n",
    "        cmap = plt.cm.rainbow\n",
    "        minimum = lambdas.min()\n",
    "        # eps = minimum / 10000\n",
    "        # lambdas = lambdas + eps\n",
    "        error = coeffs.copy()\n",
    "        for i in range(nlam):\n",
    "            error[i] = np.abs(coeffs[i]) - np.abs(coeffspred)\n",
    "\n",
    "        if q > 1:\n",
    "            fig, axes = plt.subplots(nrows=p, ncols=q, figsize=((15 * p), 15 * q))\n",
    "            for k in range(q):\n",
    "                for j in range(p):\n",
    "                    # print(k,j)\n",
    "                    for i in range(nlam):\n",
    "                        # axes[j,k].scatter(coeffspred[k,:,j], coeffs[i,k,:,j],  alpha = .1, color = cmap(norm(i)))\n",
    "                        n, bins, patches = axes[j, k].hist(error[i, k, :, j], n_bins, normed=1, histtype='step',\n",
    "                                                           cumulative=True, label='Empirical', color=cmap(norm(i)))\n",
    "                        patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "                        axes[j, k].set_xlabel(\n",
    "                            r\"$\\displaystyle \\widehat{{ d_{{g_{}}} h_{{}}}} - d_{{g_{}}}^? h_{{}}$\".format(j, k, j, k),\n",
    "                            fontsize=40)\n",
    "                        axes[j, k].tick_params(labelsize=50)\n",
    "                        # axes[j,k].set_ylabel(r\"$\\displaystyle \\widehat{{ d_{{g_{}}} h_{{}}}}$\".format(j,k), fontsize = 15)\n",
    "                        # axes[j,k].set_title('function = ' + str(k + 1) + 'group = ' + str(j+1), fontsize = 15)\n",
    "                        # axes[j,k].set_xlabel(predictionlabels[j,k], fontsize = 35)\n",
    "                        # axes[j,k].semilogy()\n",
    "                        # axes[j,k].semilogx()\n",
    "        if q == 1:\n",
    "            fig, axes = plt.subplots(nrows=p, ncols=1, figsize=(15, 15))\n",
    "            for j in range(p):\n",
    "                print(j)\n",
    "                for i in range(nlam):\n",
    "                    # print(j)\n",
    "                    n, bins, patches = axes[j].hist(error[i, 0, :, j], n_bins, normed=1, histtype='step',\n",
    "                                                    cumulative=True, label='Empirical', color=cmap(norm(i)))\n",
    "                    patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "                    axes[j].set_xlabel(r\"$\\displaystyle \\widehat{{d_{{g_{}}} h}} - d_{{g_{}}}^? h$\".format(j, j),\n",
    "                                       fontsize=40)\n",
    "                    axes[j].tick_params(labelsize=50)\n",
    "                    # axes[j].scatter(coeffspred[0,:,j], coeffs[i,0,:,j],  alpha = .1, color = cmap(norm(i)))\n",
    "                    # axes[j].set_ylabel(r\"$\\displaystyle \\widehat{{d_{{g_{}}} h}}$\".format(j), fontsize = 25)\n",
    "                    # axes[j].set_xlabel(predictionlabels[j], fontsize = 35)\n",
    "                    # axes[j].semilogy()\n",
    "                    # axes[j].semilogx()\n",
    "                    # axes[j].set_xlabel('Predicted Beta', fontsize = 15)\n",
    "                    # axes[j].set_title('group = ' + str(j+1), fontsize = 15)\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "        sm.set_array([])\n",
    "        fig.subplots_adjust(right=0.85)\n",
    "        cbar_ax = fig.add_axes([.85, 0.15, 0.05, 0.7])\n",
    "        cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "        cbar.ax.tick_params(labelsize=40)\n",
    "        cbar.set_label(r\"$\\displaystyle \\lambda$\", rotation=270, fontsize=60)\n",
    "        # fig.text(0.0, 0.5, 'Estimates', ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        fig.text(0.05, 0.5, r\"ECDF\", ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        # fig.text(0.5, 0.04, r\"$\\displaystyle d_g^? h$\", ha='center', va='center', fontsize=50)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        plt.subplots_adjust(wspace=.3, hspace=.85)\n",
    "        fig.savefig(filename + 'coefficientrecovery.png')\n",
    "\n",
    "    def plot_coefficient_distribution(self, coeffs, lambdas, filename):\n",
    "\n",
    "        p = coeffs.shape[3]\n",
    "        q = coeffs.shape[1]\n",
    "        nlam = coeffs.shape[0]\n",
    "        n_bins = 50\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=nlam)\n",
    "        cmap = plt.cm.rainbow\n",
    "        colors = cmap(norm(range(p)))\n",
    "        organizedbetas = coeffs\n",
    "        ymin = organizedbetas.min() - 0.01\n",
    "        ymax = organizedbetas.max() + 0.01\n",
    "        cmap = plt.cm.rainbow\n",
    "        minimum = lambdas.min()\n",
    "\n",
    "        if q < 1:\n",
    "            fig, axes = plt.subplots(nrows=p, ncols=q, figsize=(15, 15))\n",
    "            for k in range(q):\n",
    "                for j in range(p):\n",
    "                    # print(k,j)\n",
    "                    for i in range(nlam):\n",
    "                        # axes[j,k].scatter(coeffspred[k,:,j], coeffs[i,k,:,j],  alpha = .1, color = cmap(norm(i)))\n",
    "                        n, bins, patches = axes[j, k].hist(coeffs[i, k, :, j], n_bins, normed=1, histtype='step',\n",
    "                                                           cumulative=True, label='Empirical', color=cmap(norm(i)))\n",
    "                        patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "                        axes[j, k].set_xlabel(r\"$\\displaystyle \\widehat{{ d_{{g_{}}} h_{{}}}}$\".format(j, k),\n",
    "                                              fontsize=15)\n",
    "                        # axes[j,k].set_ylabel(r\"$\\displaystyle \\widehat{{ d_{{g_{}}} h_{{}}}}$\".format(j,k), fontsize = 15)\n",
    "                        # axes[j,k].set_title('function = ' + str(k + 1) + 'group = ' + str(j+1), fontsize = 15)\n",
    "                        # axes[j,k].set_xlabel(predictionlabels[j,k], fontsize = 35)\n",
    "                        # axes[j,k].semilogy()\n",
    "                        # axes[j,k].semilogx()\n",
    "        if q == 1:\n",
    "            fig, axes = plt.subplots(nrows=p, ncols=1, figsize=(15, 15))\n",
    "            for j in range(p):\n",
    "                print(j)\n",
    "                for i in range(nlam):\n",
    "                    # print(j)\n",
    "                    n, bins, patches = axes[j].hist(coeffs[i, 0, :, j], n_bins, normed=1, histtype='step',\n",
    "                                                    cumulative=True, label='Empirical', color=cmap(norm(i)))\n",
    "                    patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "                    axes[j].set_xlabel(r\"$\\displaystyle \\widehat{{d_{{g_{}}} h}}$\".format(j), fontsize=25)\n",
    "                    # axes[j].scatter(coeffspred[0,:,j], coeffs[i,0,:,j],  alpha = .1, color = cmap(norm(i)))\n",
    "                    # axes[j].set_ylabel(r\"$\\displaystyle \\widehat{{d_{{g_{}}} h}}$\".format(j), fontsize = 25)\n",
    "                    # axes[j].set_xlabel(predictionlabels[j], fontsize = 35)\n",
    "                    # axes[j].semilogy()\n",
    "                    # axes[j].semilogx()\n",
    "                    # axes[j].set_xlabel('Predicted Beta', fontsize = 15)\n",
    "                    # axes[j].set_title('group = ' + str(j+1), fontsize = 15)\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "        sm.set_array([])\n",
    "        fig.subplots_adjust(right=0.9)\n",
    "        cbar_ax = fig.add_axes([1, 0.15, 0.05, 0.7])\n",
    "        cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "        cbar.set_label(r\"$\\displaystyle \\lambda$\", rotation=270, fontsize=30)\n",
    "        # fig.text(0.0, 0.5, 'Estimates', ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        fig.text(0.0, 0.5, r\"$\\displaystyle \\frac{1}{n} \\sum_{i=1}^n 1_{\\widehat{ d_g h} |_i < x}$\", ha='center',\n",
    "                 va='center', rotation='vertical', fontsize=60)\n",
    "        # fig.text(0.5, 0.04, r\"$\\displaystyle d_g^? h$\", ha='center', va='center', fontsize=50)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        plt.subplots_adjust(wspace=.3, hspace=.3)\n",
    "        fig.savefig(filename + 'coefficientrecovery' + str(n))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_betas_spam2_sparse(self, xs, ys, groups, lambdas, n, q, itermax, tol):\n",
    "\n",
    "        # n = xs.shape[0]\n",
    "        p = len(np.unique(groups))\n",
    "        lambdas = np.asarray(lambdas, dtype=np.float64)\n",
    "        yadd = np.expand_dims(ys, 1)\n",
    "        groups = np.asarray(groups, dtype=np.int32) + 1\n",
    "        W0 = np.zeros((xs.shape[1], yadd.shape[1]), dtype=np.float32)\n",
    "        Xsam = np.asfortranarray(xs, dtype=np.float32)\n",
    "        Ysam = np.asfortranarray(yadd, dtype=np.float32)\n",
    "        coeffs = np.zeros((len(lambdas), q, n, p))\n",
    "        for i in range(len(lambdas)):\n",
    "            # alpha = spams.fistaFlat(Xsam,Dsam2,alpha0sam,ind_groupsam,lambda1 = lambdas[i],mode = mode,itermax = itermax,tol = tol,numThreads = numThreads, regul = \"group-lasso-l2\")\n",
    "            # spams.fistaFlat(Y,X,W0,TRUE,numThreads = 1,verbose = TRUE,lambda1 = 0.05, it0 = 10, max_it = 200,L0 = 0.1, tol = 1e-3, intercept = FALSE,pos = FALSE,compute_gram = TRUE, loss = 'square',regul = 'l1')\n",
    "            output = spams.fistaFlat(Ysam, Xsam, W0, True, groups=groups, numThreads=-1, verbose=True,\n",
    "                                     lambda1=lambdas[i], it0=100, max_it=itermax, L0=0.5, tol=tol, intercept=False,\n",
    "                                     pos=False, compute_gram=True, loss='square', regul='group-lasso-l2', ista=False,\n",
    "                                     subgrad=False, a=0.1, b=1000)\n",
    "            coeffs[i, :, :, :] = np.reshape(output[0], (q, n, p))\n",
    "            # print(output[1])\n",
    "        return (coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i  = 0 \n",
    "replicates = {}\n",
    "selected_points_save = np.zeros((nreps,nsel))\n",
    "selected_points = np.random.choice(list(range(n)),nsel,replace = False)\n",
    "selected_points_save[i] = selected_points\n",
    "replicates[i] = Replicate()\n",
    "replicates[i].nsel = nsel\n",
    "replicates[i].selected_points = selected_points\n",
    "replicates[i].df_M,replicates[i].dg_M,replicates[i].dg_w ,replicates[i].dg_w_pca ,replicates[i].dgw_norm  = get_grads(experiment, experiment.Mpca, experiment.M, experiment.N, selected_points)\n",
    "replicates[i].xtrain, replicates[i].groups = experiment.construct_X_js(replicates[i].dg_M)\n",
    "replicates[i].ytrain = experiment.construct_Y_js(replicates[i].df_M,dimnoise)\n",
    "replicates[i].coeff_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = replicates[i].xtrain\n",
    "ys =  replicates[i].ytrain\n",
    "groups = replicates[i].groups\n",
    "lambdas =  np.asarray([0])\n",
    "n = nsel\n",
    "q = experiment.m\n",
    "#itermax itermax, tol\n",
    "#tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as ssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        p = len(np.unique(groups))\n",
    "        lambdas = np.asarray(lambdas, dtype=np.float64)\n",
    "        yadd = np.expand_dims(ys, 1)\n",
    "        groups = np.asarray(groups, dtype=np.int32) + 1\n",
    "        W0 = np.zeros((xs.shape[1], yadd.shape[1]), dtype=np.float32)\n",
    "        Xsam = np.asfortranarray(xs, dtype=np.float32)\n",
    "        Ysam = np.asfortranarray(yadd, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = ssp.csc_matrix(Xsam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx[0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        p = len(np.unique(groups))\n",
    "        lambdas = np.asarray(lambdas, dtype=np.float64)\n",
    "        yadd = np.expand_dims(ys, 1)\n",
    "        groups = np.asarray(groups, dtype=np.int32) + 1\n",
    "        W0 = np.zeros((xs.shape[1], yadd.shape[1]), dtype=np.float32)\n",
    "        Xsam = np.asfortranarray(xs, dtype=np.float32)\n",
    "        Ysam = np.asfortranarray(yadd, dtype=np.float32)\n",
    "        coeffs = np.zeros((len(lambdas), q, n, p))\n",
    "        for i in range(len(lambdas)):\n",
    "            # alpha = spams.fistaFlat(Xsam,Dsam2,alpha0sam,ind_groupsam,lambda1 = lambdas[i],mode = mode,itermax = itermax,tol = tol,numThreads = numThreads, regul = \"group-lasso-l2\")\n",
    "            # spams.fistaFlat(Y,X,W0,TRUE,numThreads = 1,verbose = TRUE,lambda1 = 0.05, it0 = 10, max_it = 200,L0 = 0.1, tol = 1e-3, intercept = FALSE,pos = FALSE,compute_gram = TRUE, loss = 'square',regul = 'l1')\n",
    "            output = spams.fistaFlat(Ysam, Xsam, W0, True, groups=groups, numThreads=-1, verbose=True,\n",
    "                                     lambda1=lambdas[i], it0=100, max_it=itermax, L0=0.5, tol=tol, intercept=False,\n",
    "                                     pos=False, compute_gram=True, loss='square', regul='group-lasso-l2', ista=False,\n",
    "                                     subgrad=False, a=0.1, b=1000)\n",
    "            coeffs[i, :, :, :] = np.reshape(output[0], (q, n, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ssp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-605fe157ea1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ssp' is not defined"
     ]
    }
   ],
   "source": [
    "ssp.csc_matrix(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        p = len(np.unique(groups))\n",
    "        lambdas = np.asarray(lambdas, dtype=np.float64)\n",
    "        yadd = np.expand_dims(ys, 1)\n",
    "        groups = np.asarray(groups, dtype=np.int32) + 1\n",
    "        W0 = np.zeros((xs.shape[1], yadd.shape[1]), dtype=np.float32)\n",
    "        Xsam = np.asfortranarray(xs, dtype=np.float32)\n",
    "        Ysam = np.asfortranarray(yadd, dtype=np.float32)\n",
    "        coeffs = np.zeros((len(lambdas), q, n, p))\n",
    "        for i in range(len(lambdas)):\n",
    "            # alpha = spams.fistaFlat(Xsam,Dsam2,alpha0sam,ind_groupsam,lambda1 = lambdas[i],mode = mode,itermax = itermax,tol = tol,numThreads = numThreads, regul = \"group-lasso-l2\")\n",
    "            # spams.fistaFlat(Y,X,W0,TRUE,numThreads = 1,verbose = TRUE,lambda1 = 0.05, it0 = 10, max_it = 200,L0 = 0.1, tol = 1e-3, intercept = FALSE,pos = FALSE,compute_gram = TRUE, loss = 'square',regul = 'l1')\n",
    "            output = spams.fistaFlat(Ysam, ssp.csc_matrix(Xsam), W0, True, groups=groups, numThreads=-1, verbose=True,\n",
    "                                     lambda1=lambdas[i], it0=100, max_it=itermax, L0=0.5, tol=tol, intercept=False,\n",
    "                                     pos=False, compute_gram=True, loss='square', regul='group-lasso-l2', ista=False,\n",
    "                                     subgrad=False, a=0.1, b=1000)\n",
    "            coeffs[i, :, :, :] = np.reshape(output[0], (q, n, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs2 = coeffs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fistaFlat in module spams:\n",
      "\n",
      "fistaFlat(Y, X, W0, return_optim_info=False, numThreads=-1, max_it=1000, L0=1.0, fixed_step=False, gamma=1.5, lambda1=1.0, delta=1.0, lambda2=0.0, lambda3=0.0, a=1.0, b=0.0, c=1.0, tol=1e-06, it0=100, max_iter_backtracking=1000, compute_gram=False, lin_admm=False, admm=False, intercept=False, resetflow=False, regul='', loss='', verbose=False, pos=False, clever=False, log=False, ista=False, subgrad=False, logName='', is_inner_weights=False, inner_weights=None, size_group=1, groups=None, sqrt_step=True, transpose=False, linesearch_mode=0)\n",
      "        fistaFlat solves sparse regularized problems.\n",
      "    \n",
      "            X is a design matrix of size m x p\n",
      "            X=[x^1,...,x^n]', where the x_i's are the rows of X\n",
      "            Y=[y^1,...,y^n] is a matrix of size m x n\n",
      "            It implements the algorithms FISTA, ISTA and subgradient descent.\n",
      "            \n",
      "              - if loss='square' and regul is a regularization function for vectors,\n",
      "                the entries of Y are real-valued,  W = [w^1,...,w^n] is a matrix of size p x n\n",
      "                For all column y of Y, it computes a column w of W such that\n",
      "                  w = argmin 0.5||y- X w||_2^2 + lambda1 psi(w)\n",
      "                  \n",
      "              - if loss='square' and regul is a regularization function for matrices\n",
      "                the entries of Y are real-valued,  W is a matrix of size p x n. \n",
      "                It computes the matrix W such that\n",
      "                  W = argmin 0.5||Y- X W||_F^2 + lambda1 psi(W)\n",
      "                  \n",
      "              - loss='square-missing' same as loss='square', but handles missing data\n",
      "                represented by NaN (not a number) in the matrix Y\n",
      "                \n",
      "              - if loss='logistic' and regul is a regularization function for vectors,\n",
      "                the entries of Y are either -1 or +1, W = [w^1,...,w^n] is a matrix of size p x n\n",
      "                For all column y of Y, it computes a column w of W such that\n",
      "                  w = argmin (1/m)sum_{j=1}^m log(1+e^(-y_j x^j' w)) + lambda1 psi(w),\n",
      "                where x^j is the j-th row of X.\n",
      "                \n",
      "              - if loss='logistic' and regul is a regularization function for matrices\n",
      "                the entries of Y are either -1 or +1, W is a matrix of size p x n\n",
      "                  W = argmin sum_{i=1}^n(1/m)sum_{j=1}^m log(1+e^(-y^i_j x^j' w^i)) + lambda1 psi(W)\n",
      "                  \n",
      "              - if loss='multi-logistic' and regul is a regularization function for vectors,\n",
      "                the entries of Y are in {0,1,...,N} where N is the total number of classes\n",
      "                W = [W^1,...,W^n] is a matrix of size p x Nn, each submatrix W^i is of size p x N\n",
      "                for all submatrix WW of W, and column y of Y, it computes\n",
      "                  WW = argmin (1/m)sum_{j=1}^m log(sum_{j=1}^r e^(x^j'(ww^j-ww^{y_j}))) + lambda1 sum_{j=1}^N psi(ww^j),\n",
      "                where ww^j is the j-th column of WW.\n",
      "                \n",
      "              - if loss='multi-logistic' and regul is a regularization function for matrices,\n",
      "                the entries of Y are in {0,1,...,N} where N is the total number of classes\n",
      "                W is a matrix of size p x N, it computes\n",
      "                  W = argmin (1/m)sum_{j=1}^m log(sum_{j=1}^r e^(x^j'(w^j-w^{y_j}))) + lambda1 psi(W)\n",
      "                where ww^j is the j-th column of WW.\n",
      "                \n",
      "              - loss='cur' useful to perform sparse CUR matrix decompositions, \n",
      "                  W = argmin 0.5||Y-X*W*X||_F^2 + lambda1 psi(W)\n",
      "                  \n",
      "                  \n",
      "            The function psi are those used by proximalFlat (see documentation)\n",
      "            \n",
      "            This function can also handle intercepts (last row of W is not regularized),\n",
      "            and/or non-negativity constraints on W, and sparse matrices for X\n",
      "    \n",
      "    Args:\n",
      "        Y: double dense m x n matrix\n",
      "        X: double dense or sparse m x p matrix   \n",
      "        W0: double dense p x n matrix or p x Nn matrix (for multi-logistic loss)\n",
      "          initial guess\n",
      "        return_optim_info: \n",
      "          if true the function will return a tuple of matrices.\n",
      "    \n",
      "    Kwargs:\n",
      "        loss: (choice of loss, see above)\n",
      "        regul: (choice of regularization, see function proximalFlat)\n",
      "        lambda1: (regularization parameter)\n",
      "        lambda2: (optional, regularization parameter, 0 by default)\n",
      "        lambda3: (optional, regularization parameter, 0 by default)\n",
      "        verbose: (optional, verbosity level, false by default)\n",
      "        pos: (optional, adds positivity constraints on the\n",
      "          coefficients, false by default)\n",
      "        transpose: (optional, transpose the matrix in the regularization function)\n",
      "        size_group: (optional, for regularization functions assuming a group\n",
      "          structure)\n",
      "        groups: (int32, optional, for regularization functions assuming a group\n",
      "          structure, see proximalFlat)\n",
      "        numThreads: (optional, number of threads for exploiting\n",
      "          multi-core / multi-cpus. By default, it takes the value -1,\n",
      "          which automatically selects all the available CPUs/cores).\n",
      "        max_it: (optional, maximum number of iterations, 100 by default)\n",
      "        it0: (optional, frequency for computing duality gap, every 10 iterations by default)\n",
      "        tol: (optional, tolerance for stopping criteration, which is a relative duality gap\n",
      "          if it is available, or a relative change of parameters).\n",
      "        gamma: (optional, multiplier for increasing the parameter L in fista, 1.5 by default)\n",
      "        L0: (optional, initial parameter L in fista, 0.1 by default, should be small enough)\n",
      "        fixed_step: (deactive the line search for L in fista and use L0 instead)\n",
      "        linesearch_mode: (line-search scheme when ista=true:\n",
      "        0: default, monotonic backtracking scheme\n",
      "        1: monotonic backtracking scheme, with restart at each iteration\n",
      "        2: Barzilai-Borwein step sizes (similar to SparSA by Wright et al.) \n",
      "        3: non-monotonic backtracking\n",
      "        compute_gram: (optional, pre-compute X^TX, false by default).\n",
      "        intercept: (optional, do not regularize last row of W, false by default).\n",
      "        ista: (optional, use ista instead of fista, false by default).\n",
      "        subgrad: (optional, if not ista, use subradient descent instead of fista, false by default).\n",
      "        a: \n",
      "        b: (optional, if subgrad, the gradient step is a/(t+b)\n",
      "          also similar options as proximalFlat\n",
      "          \n",
      "          the function also implements the ADMM algorithm via an option admm=true. It is not documented\n",
      "          and you need to look at the source code to use it.\n",
      "        delta: undocumented; modify at your own risks!\n",
      "        c: undocumented; modify at your own risks!\n",
      "        max_iter_backtracking: undocumented; modify at your own risks!\n",
      "        lin_admm: undocumented; modify at your own risks!\n",
      "        admm: undocumented; modify at your own risks!\n",
      "        resetflow: undocumented; modify at your own risks!\n",
      "        clever: undocumented; modify at your own risks!\n",
      "        log: undocumented; modify at your own risks!\n",
      "        logName: undocumented; modify at your own risks!\n",
      "        is_inner_weights: undocumented; modify at your own risks!\n",
      "        inner_weights: undocumented; modify at your own risks!\n",
      "        sqrt_step: undocumented; modify at your own risks!\n",
      "    \n",
      "    Returns:\n",
      "        W: double dense p x n matrix or p x Nn matrix (for multi-logistic loss)\n",
      "        optim: optional, double dense 4 x n matrix.\n",
      "          first row: values of the objective functions.\n",
      "          third row: values of the relative duality gap (if available)\n",
      "          fourth row: number of iterations\n",
      "        optim_info: vector of size 4, containing information of the optimization.\n",
      "          W = spams.fistaFlat(Y,X,W0,return_optim_info = False,...)\n",
      "          (W,optim_info) = spams.fistaFlat(Y,X,W0,return_optim_info = True,...)\n",
      "    \n",
      "    Authors:\n",
      "    Julien MAIRAL, 2010 (spams, matlab interface and documentation)\n",
      "    Jean-Paul CHIEZE 2011-2012 (python interface)\n",
      "    \n",
      "    Note:\n",
      "        Valid values for the regularization parameter (regul) are:\n",
      "          \"l0\", \"l1\", \"l2\", \"linf\", \"l2-not-squared\", \"elastic-net\", \"fused-lasso\",\n",
      "          \"group-lasso-l2\", \"group-lasso-linf\", \"sparse-group-lasso-l2\",\n",
      "          \"sparse-group-lasso-linf\", \"l1l2\", \"l1linf\", \"l1l2+l1\", \"l1linf+l1\",\n",
      "          \"tree-l0\", \"tree-l2\", \"tree-linf\", \"graph\", \"graph-ridge\", \"graph-l2\",\n",
      "          \"multi-task-tree\", \"multi-task-graph\", \"l1linf-row-column\", \"trace-norm\",\n",
      "          \"trace-norm-vec\", \"rank\", \"rank-vec\", \"none\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spams.fistaFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replicates[i].coeff_dict[0] = experiment.get_betas_spam2(replicates[i].xtrain, replicates[i].ytrain, replicates[i].groups, np.asarray([0]), nsel, experiment.m, itermax, tol)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spams\n",
    "import numpy as np\n",
    "param = {\"numThreads\" : -1,\"verbose\" : True,\n",
    "\"lambda1\" : 0.05, \"it0\" : 10, \"max_it\" : 200,\n",
    "\"L0\" : 0.1, \"tol\" : 1e-3, \"intercept\" : False,\n",
    "\"pos\" : False}\n",
    "np.random.seed(0)\n",
    "m = 100\n",
    "n = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as ssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FISTA + Regression l1\n"
     ]
    }
   ],
   "source": [
    "#print(\"\"\\nVarious regression experiments)\n",
    "param[\"compute_gram\"] = True\n",
    "print(\"\\nFISTA + Regression l1\")\n",
    "param[\"loss\"] = \"square\"\n",
    "param[\"regul\"] = \"l1\"\n",
    "# param.regul=group-las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method choice in module random:\n",
      "\n",
      "choice(seq) method of random.Random instance\n",
      "    Choose a random element from a non-empty sequence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(random.choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asfortranarray(np.random.normal(size = (m,n)))\n",
    "X = np.asfortranarray(X - np.tile(np.mean(X,0),(X.shape[0],1)),dtype=np.float32)\n",
    "X = spams.normalize(X)\n",
    "X[:50,100:] = np.zeros((50,100))\n",
    "X[50:,:100] = np.zeros((50,100))\n",
    "X = np.asfortranarray(X, dtype = np.float32)\n",
    "Y = np.asfortranarray(np.random.normal(size = (m,1)))\n",
    "Y = np.asfortranarray(Y - np.tile(np.mean(Y,0),(Y.shape[0],1)),dtype=np.float32)\n",
    "Y = spams.normalize(Y)\n",
    "W0 = np.zeros((X.shape[1],Y.shape[1]),dtype=np.float32,order=\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplelist0 = np.random.choice(list(range(20000)), 1000, replace=False)\n",
    "#samplelist1 = np.random.choice(list(range(100)), 100, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = X.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2[~samplelist0] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "21.4 ms  3.17 ms per loop (mean  std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "print(\"\\nFISTA + Regression l2 + sparse feature matrix\")\n",
    "param[\"regul\"] = \"l2\"\n",
    "(W, optim_info) = spams.fistaFlat(Y,ssp.csc_matrix(X),W0,True,**param)\n",
    "print(\"mean loss: %f, mean relative duality_gap: %f, number of iterations: %f\" %(np.mean(\n",
    "optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0, ..., 99, 99, 99]),\n",
       " array([100, 101, 102, ...,  97,  98,  99]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X == 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "\n",
      "FISTA + Regression l2 + sparse feature matrix\n",
      "mean loss: 0.041529, mean relative duality_gap: 0.000175, number of iterations: 40.000000\n",
      "24.6 ms  2.76 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "print(\"\\nFISTA + Regression l2 + sparse feature matrix\")\n",
    "param[\"regul\"] = \"l2\"\n",
    "(W, optim_info) = spams.fistaFlat(Y,X,W0,True,**param)\n",
    "print(\"mean loss: %f, mean relative duality_gap: %f, number of iterations: %f\" %(np.mean(\n",
    "optim_info[0,:]),np.mean(optim_info[2,:]),np.mean(optim_info[3,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x118dda588>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEFCAYAAAAsU2YoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd4XMX1v9/Zvlpp1btky71344IxNhgXMM3UmNBCCwmhJqSQkFC+SUgIkJAQCL0EDIRijDHGBmzA4N67Lcmyel9JK23fnd8fZzEWPwMCi2Jz3+fZx5rde2fmzr2e+5kz58worTUGBgYGBkcWpm+7AgYGBgYGXx6j8zYwMDA4AjE6bwMDA4MjEKPzNjAwMDgCMTpvAwMDgyMQo/M2MDAwOAIxOm8DAwODI5DD6ryVUrOUUruVUsVKqV93V6UMDAwMDD4f9VWDdJRSZmAPMB2oBNYCc7XWO7qvegYGBgYGh8JyGOeOA4q11qUASqnngTOAz+y8m2ZP0fdtzycvauLktDoAPmzIZqSjhYJjOgDYuyKFAaf6AChdbGd3KIlcHQLgZYeZG5KbaGpyAeCLWBkwqB5T/Craahx4Wpx4wnYAHCqKwxzFaQ8DEIspCka0EWySAUfSxBS010/JmzYAtofdnPlTsJxzJQD/nPEIpzqaSSuUum3YlsuQHg38syELgNyYmctPbkCHogA0rDbzpjeTEeEgAAWZraSPjLJ6SaZcj83CFEczBTOlvuaeOZT8uwFXohxvMmtsCRECXisAzuQwAa+F7EuKANj4Vw/lJjunXxUDYOvDQVzxa+s5I4QpLZEXnnECMCmhmaa2BNbYHACMCQWpVzZm/3uYtN3jb+IpcdDYIm3ZErWRbfezI5IEwLS+lSizxixNQ/mOFNIzO0gdI23338VZTMBLe1gOGDapgVATuEa7AXjif24uu/qTgd2e/7TSe1oHy96UthvobuHlYCpz3Q2f8bQYfN/pvXWJOtw8wo2lXVKn1ozeh13WN83hmE3ygYqD0pXx7zqhlLpKKbVOKbXuqfKawyjO4LPoOSP0bVfBwMDgG+ZwzCbnALO01lfE0xcB47XWP/usc2qOO0Fbk2LU7HJT7ksEwK3CFOa0YrGLes34249o+e1jADRWJpLZ00tNaTIAaZkdFFekM+FmUXf+9/aya20mPQo9ALQ0JrAt6Ob4nvKSSOyv2PJWKiNOawPAlOQgsL2VhFMGAbDn3mre0W7Ojo8CkvJDvL6pEF/8lfajfw3H85c3CQfMUrezctAtXkx5oh6X/8XLgEwPJrMo4XX1WRSZfLxtETV73eWaF/+jmNmzCoBVpblMP8uDcokarn3Dh8msSR8p1x5tj/DuqgKGJsn1FMxx8ORzCQeGR5f8uSdv/6IUc/ye9UltJWeiKG9L3zz2PtzC7pAo5xnHVtGww0H+OdJW8x+zMedGG5E9lQA8tCSLay6JsPARubaTT23g7dczmPlTESBtS6rRMUg5fwAAt9/Xym+nN7HzDblvGWkdPO/NYED8vVFk7qAtbKNnZqtca3Mmp0yspKVYRhG7a9Mps1mY6myW+57hJ6EQmndYP+txMfie0y3Ku25315R39oAjTnkfjtmkCig8KF0Q/87gG8bSN+/broKBwXeTWOzbrsHXxuEobwsyYTkN6bTXAhdorbd/1jmlw2bo3FuPI7J8Ja3rxc7rzIpiH5BCxzpRbMW7MhgwSuyg87YVckzUR0dU1NnEmxJYfF+Qk2bL76ZkJ/VLA2SfL0r41X/DnKs1Ff/1fFJPk+YlfxoAN1xtYfnfQ4woEqWddn4fAu/tweQQqd1epmiucxGKiBrtOchD034XS+Pnn1VQjXuUncWvSnrm6Y2snp9Chj0AQHUwASuaYf0k//2lqVhMMfpNbwdAh2IQ0ZjTRXm3bQzyWmUeF86Q463nnUHDb19laW0uAOed2UzxAisf3yJ3UoAVrZmcfa60lWXmdJZd+hEABc4O+lyZim4X+3zly36qWxMZd2lE2sGdSPvSctxXTJK61Nax9G8+8sx+AJLdfgouLyBW1wRAuMzDs6sKOLevWMb+V1zI8eZWWgJS98LsFtLHgmejVM7ntWEya5LSpC2ST8hAhyK897QcP2lqLXevzuWqtHoAXm/I4eIZdTSuOeIEj8E3RHco71D19i51cLa8IUfcg/iVlbfWOqKU+hnwFmAGHv+8jtvg66PA2fFtV8HA4LuJoby7B8+5U/WeD9Mo106OyRYFlnl6OlX/awEge0KEQFn4gI2779lQ/DJUBMWG3G4y0WhWXHFXL0k/+T4v7ypk7oli427dDhWVqYy6Q6w5a26tZPTprQfstE/ZLFwZCzLgGlHOHW+X8vyuQi46Tqw95SsTKQ4kMXW8pLesymLM3ABlr0n99/qTmNi3hs17swHY6LBwwx97UvOXtQDkXNKDWEMzxc+L2s0u9JJy6Sg2/K4cgEETG7h9Yy5/PF8MxdH6Nohp1i9OB6BPfjMWewxnjtjAqzcnkTfMy8J1cj2lFs3lWbUHbPDZc9IOtF2tJwmFZuAwGZU0lCaSO7qDmE/ub83OJHr/uh91D2wD4LZWN7c4fGT0E8+efRtTqQs7ybNL2huykZ/RxrZGqduUE2qINMcItEjZvjYbb7VnUBu39y8JVbJkkglfpYxi9pelkpIQoHC2CBr/di+hdjPpd54DwMuXfsQZ57TQsMyYbDU4NN2ivCs2d015F474/ijv7iZ7QuTbrsIRS60n6duugoHBd5NY9NuuwdfGN6q8G6ZP0Uu2FzJ7sihbS1EqHaubKd+bCsDgn7nZ9k8vg88Su2nDexHavXZ6nS51VHYr5pFDaXlwBQDOnor6DTbe7MgA4LKzWln+fCKjeooNOeSz8DdvCj8MiDrMTG2nojmZ3jni8VBdl0x+TitJ+aL+3t5QwMT8Wnxt4ieekuNjz55MnGZ5sQy+zIrKyWLpnXL+iJwGdtWkU2yTd+BFJ9Xy6pIc+sfEjrzV7GTuyfU8t1hs8nP6VlCyK+NAewyb4yO034etl4wsSl63EI7b2/vP8BIoC1NfJt4ieUPFP93fJn7VZQ0p9M1vIuOsHADCO6u5+cN0/jxIRjT7tqRRNLiZWPyd2LA/icIpAZYsllHD8QOqqN+fdMDNsOFDeLM1k15xn/WeLi+ZPb10NEpbZE6x4N3gJ+3vPwWg/dZ/Ye/nYvHLcu/MWjNteh2mVPEzr383zAeeTCoscu9mRXw0RexkWOXeDv5dT2Il5dQvNkw+BoemW5R36ZquKe/e475fylspVQZ4gSgQ0VqP7XLBRamd0oN/5j6cqhxV9J/h7ZTOG9rWKV3WkNIpHd5Z/bXXycDgSETro9fm3R1mkxO01o3dkI+BgYFB92JMWH7GyaK8x3a1866fNkUnjMug4iU/i4IyaXimu4EBe7ZROaEfADqmcJ8oQ/sHn7Twwx5V+FrEVPBuSyZnj6lg0yr53apijL7SRLRWJu1K3nLSGrQxaLCYDhKPzcK/rh5Likyi7f4glQR7GJtdbAmv+tOZm157IOS7/kOFKy3EzRUyKjgt5GREkoetXkk3WBQzkhpwJsfD7SOKupok+k4S173GrTYaW1z06i9mlbf2FjDc0oZJSRvvDro5tqjmwISkr9pM8kQX++ZL++QPa6NxbwLZY8SNMtoWxew207ZbRnQms8aZFaWjRt65Sb2iNGwTV7zGFhe7TU7OGCuufQvWFTKrTyUvlBUAMDgYZtToWlZslCDYmf8azO6fryElVUw8ZmuMneWZDOktbZd6wUAi20rwrJW6bqrOYo0DLrRKW6f36MCapti5QiY0TUpjVpqB50rbmpKctH9QT0WxjBL6Tm6lba8JHZNrcWWGCXeY8LcaQToGh6Y7zCbBPSu61MHZ+x/3hWUppWYB/0C86x7VWt/1qd+vBq5BLBHtwFUfr/WklPoNcHn8t+u01m99mes4FIe7JKwGliil1iulrjrUAQeHxz9dZYTHfx18vD6JgYHBp4hFu/b5AuIL8T0AnAwMBuYqpQZ/6rDntNbDtNYjgb8C98bPHQz8ABgCzAL+Hc/vsDhcs8lxWusqpVQWsFQptUtr/f7BB2itHwYeBmj90Ul63/MhdgVSuXyqTFoGa2OsaR3L1t2iIAf2aMQ0RMLXr5zyPpHmGInD5R2zd1GUkEcxsI+4wxWXZqAK86l/WZSvUpqc5A6S5o4GoP2FDfyqJIN//64nAGs+qufi42t4cLkEwVx/ZyFtT1Rwx1KJULy5qIbke2/izz/+BwBpsyz85ZlUbpggIeV7V6dT3JTKsZMlCKjxI83/LE5+6xb3Ok9rAsOuTeKO/8gk3y9HV9BeZSV5gDwcBb42Nq3Kpm6/qM1xaY2883wSPawyaRduVTwWSCFthdzXH6TWYbFHWVgn9f3R/xXwxq8rOHGc1OeutXlcqNpJcoRYF0rmvEuDNL4peY+2tuLIV1wxVZTwnicDBJrNNFmkLV+4fhenj2sl1CSCo7EykYlzWlj0mpQ1/rG9OFNDuPvI70MCTSQ3uSk4RYTMhy/nYkYzYa5ce9vqDrxNDl57UUZUvWIBstPstIelPpvezWDo6HosbinfnOmi/k2FIyF8qOfKwKB7iHabF9sXLsSntT54csqFiFvixz2vtQ4C+5RSxfH8Vh5OhQ5LeWutq+L/1gOvxitk8A2zLpT8bVfBwOC7iY516XOwhSD++bQloasL8V2jlCpBlPd1X+bcL8tXVt5KKRdg0lp743/PAO74vHP2L3OyNpLM+Rd20LFOJhKsbghHzZTYRKENtmjaH30XgI4GGxa7GcfEeKCIP8qPyxJ5Yox4YxxzdV/G3vwub/dLACD/rzN56KcbOOuhrQCkHZfEPYmNRHfJ2/dH58Mr83KZaZbzP7q5lJFj4Y7rJIin9F+JWG69h/80SLvesKaKnpFCrLmSf4I9zJA5EaoWyTsvGLDw2+lNtGwUZd0RsbDnwRZGhyW/xr0JbGpNY1ir2ImT0gI0mKyMdIly9/usDMto4h/tYhceV5xErgXOL4i7UjpjFO/IYLJDRhYtj1QxeWAM52WzAbjyN+9gd0U4CR/OjAgqbxgZZ4nqT1xZgf3is1lw2WoAhjij1FcnMTlJpie2tKRjTrGyf7V4+WwzJfDhQrjgfCmr8g0rH9Zn494p1zYg1YPTHCXmF6U8vE8dFmcMImKycY9x0PC6hVkj5Bk12cA2MJ13npX6jDe3kXDqMDrmy72xHdsD05Lyz3tcDAwOny5OWB5sITgctNYPAA8opS4Afgdccrh5fhaHYzbJBl5VSn2cz3Na68XdUiuDL4UzwwhwMjA4FFp3W5DOl12I73ngwa94bpc4nLVNSoERX+acjLx2ZoR93PdiDtNCol4TbGH6jGiioEXUabvHzqvVorTnptcSiypaF8l1TpoK76/OwT5W7Ko1d6/nldQ0du8Vpfv+Nes5LhgmFhU77doXEthjS+HCPmKK+ujZBMYlN5M9TOy0jSscbFqXw4RjxbtjdSiFibs01/eT8ry1NobrDkx5Uh+rtZlIjR8Q+3zRtACYTKSMEht1rELxksnFifHIGB1TZMbClHSIum33pzD7lHrM8VUAPfMrsCVG+W2GBBXt2ZXBEMKsKhW786RBVUS1oiMoo5L8zBgNOxNI3CYh7lmTwXqM3AL/G+uZ9+cWzjtVlHXQYyJwx3wm5Eo7e5oSGHCxjfIXRTmnxcJctyyZC6PyCJw6oILluwow58vGESZzDWf8oI23n4svAdvfT26hiZp35PhVbTnMObuFVS+I8j72Ks36qI1+x8goZefjAXr6mhgQjofTR63UPbwbq0PSNfdV0X/+T6m+6P7PfWYMDA6L7vPzXgv0U0r1QjreHwAXHHyAUqqf1npvPDkb+PjvBcBzSql7gTygH7DmcCv0nQmPN/jq+N9Y/21XwcDgu0k3+Xl/1kJ8Sqk7gHVa6wXAz5RSJwFhwEPcZBI/7kVkcjMCXKO7YUjwjYbHP1R4oT5naAXbNmRRZxLf7cGONrJ6ig067DfxRlUeI2OijLMyvVTXJVOt7Afy2GTXnBMT7wx/2EJbzMp7TlGXN/Sq4v59+VxkExWfkuOjrDgNh1WUcP9fFBBc/skubZUbkug1R7F4nqwNMvNsDw+8nsbVx4ryfndZLmk6zKhTxEZdt8pC/vmpvBzfwOCknlU01STyZkwmDM9yNZB5TIR1iyUEfuJNCbxwf5hz58r1Pf1CEmYNQyMSIj5sdiub30gmqmWk0CvfQ2JuGMdEGWH5P6zA4jbhLY1v29Y3RvM2UeFl9SmkOQIUDhUb9Y71mQwZ30Asvs5T8jNP4Pv5lbTG13lMGWOhbVOYxN5yv2O+GNZCF5teEqW82erg8tty8D4ni2xt2pbLxLNbqZbpB9IKfWzckstxd8qooP2FDZTuTKfXAFlC1pIA1gIHlr4yX7D+njaclgj7oqLMg0ox2ulhZVB85s8cUYHjuCLqnjPcRw0OTXf4eQfWvtylDs5xzNlHXHj8F3qbKKUeV0rVK6W2HfRdmlJqqVJqb/zf1M/LoyuE/Yfrcv79oay+c3j8jvWZ31JNDAy+43TR2+RI5AuVt1LqeCRa6Gmt9dD4d38FmrXWdymlfg2kaq1/9UWFFQ+eqUNBCwlJIg99XhvBsAVPUJT1oH4NVJUl0xwWm/L4WQ1YJw1n6237AWiN2hg/o4Gn437aWRHNtOGVlG2Xd0d+USvPlefzk/g2adHtpZS8aaPfVdLZrb3fz4jjGlA2ecmaUx00fBBlUYssHDV3eAWNJaJE93mSedYR4sHz4u2jY0RqvJgSxNIUqQ+yfXUmo68XZVn3XA1bGzIYXSj260U1uZxaWE3YH99CbXoiG58xk2SXa8/K9/JGeT7HxbcFK3rgVGIfLOfhp2REctGggz2LwDmpgFh1Ez9bLOXdntKGu2eIht1S38LLc1n5tw5cZrFpr7QkcPGICqxZkt9TS7IZEgoxbLjUzzkihaZ3O3D3klGJtX86bz7pYOYpEmG57I1M+rlaScmWUZDZptm3J42+I+JKO83Mh29LpGu/NA9tXgeDr5X7UPxgI3mD2ojKAANlgvd2FHDyhZJX9UI/eac6qZgvcw12hzHhavD/0y3Ke9ULXVPeE84/+pR3POim+VNfnwE8Ff/7KeDML1uwz2vrlB7U77uzi/g+z6f8pj/1Zo7UBzulPz30X1ST2ymdMT2xUzorv/PCU0UPnNopfaiO+2BuT+m8UFXh5Z3L+ybpl+bplC5+sPNKCepTT1j1Qn+ntNFxG3ytHMXK+6vaKrK11h/3WLWI2+AhOdj5/XlP5VcszsDAwOArEIt17XME0qUJS6VUEbDwILNJi9Y65aDfPVrrL7R7L80+Xx/32Hg6HniNl7fKpNyAcJAx5/tZ+qKYOmbdaEdHZCK2eX4VaWcV0PaGBHN0NNsIh824M2Q8vrk0izGDa0g8QZSpf2UlHXVWtlSLDbjI2U7vK1JZe7+ovQ4sjB1czYOlcvzNV5kJb6sgFl/vu3xDMkUTvLSXyQgqaaDC5LKx6RUxVew3OciKRhgxqBaASNBE6b40HGap727tYs61ZvY8KBOmhUNaeHlrISUW+X14yMzsSZU0bRczUcaoMN49itQTRek3vuVFa1DxAdz7Ddm4ozFOukjqv+W/Job/MELHesk/7DMf2FWnw2sn0R3kWY+YgG64ysz+x5t4KSK36bKcGvbtT+OFuKteGhbmxNoP7CpU+WQNudNM7I8HIPX57UC8z61Fx90uw34TtsQoH22XCcmX7AEuCNhIM4sZaPDsdvx7Q3Q0yYgqMStIS7WT9L5Sd+eMIey/v4xIfL3yPr8sIrB4Cy2ln0xGGxgcTLeYTT54pmtmk8kXHXFmk6/qKlinlMrVWtcopXKB+u6slMGXo8NrdIAGBodCR4/etXO+qvK+G2g6aMIyTWv9yy/Kx/fgtTq0YjsvvJfHBWeJetzyooO3HHZumigbCvx3RT4XHi9/WwpSeP0ZJ70R9VaunQxytpLVS2zG9jwLkZYI9gGiLoO7Wwh5TDiy5Zpsg7No/6CeO8vEqnOdsxWt1YElYXfUZjDpxFrsF54BwGNXr2Oao5mMInFF3L8zlfyiVhL6iVrUoRjm9AT2virp+pATM5oddnHfO9HcyvyYmwkByf/Y6+xU/beezMEyUgg1awKtVpRJ6pdYGKG11MaqBlHLY5KbeMWXwbWXxJdVPWkWH537JonxSch/2xT/Oj2IcklnHSltxpwqfzevidDU4GLQbbK07gu/r+X8fw5m4zXiA241xcjK9rK9StwYJ0yowTGlP9EysX55PvITCZpZ0Sx1OalnFZtKsrHH19YZOa6WiFcdWOLVbo3S9y/H0Pov8SV8Yn8BJ4Z9DL9PgobK/7CKYk8K9RZpqylp9axvzOTkH7YDoFKSUA4HtU8ZIfIGh6Y7lLd/2aNdUt7OE644+pS3UmoeMBXIUEpVAn8A7gJeVEpdDuwHzvs6K2nw+TSvMSb9DAwOyRFqz+4K32iQzms5F+iRefVEwyYsdrEDhwMWsme72PaMHGMzRwlHRa0NuzGF8Jb9WPqIGvzowRjDBtQdWMC/riGJREeINr+ozz4DG1m6uxBr/JqmDqwkEjDjHi8261irn9ffyqEf4rLWo7cH1yArb78uavSk2Q0Ey0N0NIjd1ue1EdOK98OiNi/9pZvWF3bSUCneIw/h5G83pFJ6vwT19PphAnXzW9DxoJtdDWkcd0oDno2SzjjZTfmLAfZ0iI17WEYTWVPNrP9ffHEnW4hBfx7KwpvLAGgzKzwmuGKi5F+/xU7ebDt1i0XJ516UR2iTHOstNhGLqgMbSzSugu0N6bjigVyR+Nx0vxxx9dtZm846h5kzlXiuVPgTmTb/dF47ayEAvU0+htySz56/ijLO79/K8m0FTBksk85/LsklBlwYFs+brGwvtbVutpjEdXEU7fQa4cE5tQ8AW+9rYfCZfjBJW9S8E2NtawYT0w2Lm8Gh6Rbl/fZDXVPeJ1199Clvg+8+3mIjwMnA4JAcxcq7K2aTx4FTgfqDbN63AVcCHztn36K1XvRFeSXrMA53BItL4xgtdmiV6OL9e/0HdhXvfWwbe1eI0g2sKKFkXRovfSDnX+r2Ul+eRJ8rxbFl7/0h3CpIfm58ydSaBM6+IwtSxIOi7V9VJB+XTGiPuKkvWltIYixGh5bLXlmcyyRrFVPGxBf4MtlZsLOQU4okXVaTwsABDYzYI0FDvoUVvLq/kJPTJNDlnlvH0/HYUhZHxHPmklUVhIIunInigZHv6KDkbRdvmST8/geL6ii6tjcld4narWp0k2UJMGiQNKMtE+r+toYWs7TNBZdF+O/jFkyJn9ymjvUtLG2V8s77qJS31ornzOlXxXjrIQXxvYinHFNFr45P/MmdCWHeas0kWCejjFyrnxtnB/HtlFFI8w4HtT95nNy4E1HPgc0Qzjrgk/7EnkJ+PKMGZZFRyZ8u7MHTt1az3iR1u/jCDNr+WUtqIG7PTwphK3RS/oh45gz7ZSHN8/bi6iH/mXJPcHJyVYXhbWLw9dJ9mzF85+iKZHsS2brn09yntR4Z/3xhx23w9XH6VUevujAwOCwMP29VRGdvk9uAdq31375MYb8qmqsvc7SQN8PMay+JwiuIBhl9qnieWIb1JrR2L759UqeNJTm4iDDudllCNbx6O5Y+OXiXiN01FlGknFHE8nvFGyXH5iemFRkZ4i2S/bsptP3rbfwtYiM3WWIk5kd4eX1cuU6vZcWiTEb2FiWdcmoBpklTeeIi8aAYFg5QVOjBbJWbu60kmzCKLXFf6Z+eWEv7rhj1VaKsExJC1Le46NtX7MqV+1KwWyNk5ImHhdUVwzHAhblnDgBtr5fiyFXUbRZl/7FiT+olduqdK9KxmaMMusIJQPMb9Qc8Vd6ryGNyXg0Z5/YAILShjJbdFtInSN3emZ/GmMK6A3VPPiWfwOoK9m+Sdh/015H8/dcljA9ImQFMjBtZw327xY/7ssQmrI4IS2slevOcaTUEyiPEwvFtzOwxEs8cQmy/jFJ+8j8ztyb4qPPIfMCoC8OYTz2D0FPPAbIRR1YPL3v2iA9+v76NrCzOZWRml/auNvge0i0274X3ds3mfepNR5zN+3CMpT9TSm2JL1x12AtTWYb1Ptwsvje8V5HXKR3aUPbtVMTA4LvOUay8v2rn/SDQBxgJ1AD3fNaBB4fHb/IWf8XiDAwMDL4CR/HaJl/JbNLV3z7NvhHTtS0hSiyiyLh+AgC3/K6EqQET0y6RCcvFTztJi0lQilXFWGF3cM1JYtYoWZqAL2RlwAiZ4CvfkUJH2IrTIpMS/aa3oywmwrXivuatsJE6IsYz78rQ/+JZ9QTLgryyU8wmi0wt3BKxYYuHr2+KJjHR1Ux1qwz9Bw+oZ15ZAcdGZFJv4PRWLD3S6fhI6pM4qw+3P+DnUqtMmGb27SAWglB7fGedqGJjZTbW+MMx+dw2/v56Kj/KkcCYleW5pMQiTJgj5+9eKOaTfifIJKF3j9yblMliltn1vKLvlDbejl/PjNMb2bZA6rrV7GRmRh1J+WIG2b4+i3pl4/i4a1/STXMoufZt9vkkr+NPbiCwL0LNPlmWwBu0Ua3sTB0gZpDE43N4/nELu+NtMyhsZrD20Se+qqA5QfHOR/nMulTu2ztPyMTjieeICQyTiaUvupl1q0weP31XC+ePq6Ruq7gSVrUmken0kZjUeZEvA4OP6Razyat3dc1sMufXR5zZ5Cu5Cn4cGh9PzgG2fd7xBl8vW83Ob7sKBgbfTY5ib5OurOd9IMISqEMiLKciJhMNlAE/Pqgz/0w6/nixvv/hGIkxyIhKuTPGVLJ7XQaj7hXhHlryIToUXyjqQxdFM0NEG0WdxUIaS4aVV5bIhF+LGQYEP7k5x18cJLijmb9uF5tw74iZKQnNuJLl/Mfqc7h2aCWt5aISNzRmMiTRw/42UZ8bHRZ+mFlD2lRRsyufcZCgotjjyr5oiIfWKgcd7bZ420Cfa3PY8w9R4pl57bQ1OjCZ5drypmp2LEggK1OUdPoEE0RilC+T8pPT/dTVJPGhScq7ZFIVlh6pKLvk3/5BDc+VFNI/KOo31RRi2Lk9CEA2AAAgAElEQVRBdr8i79yaiJMTr5Zr3/ZwkL0kMGuYLCe7c3MmBdmtNDVJANBd5hhuZeOMgOQ9pmctKVNTaFkuStnXZiM5x08kvtBVYl9FoCJK4tT4crMxjfb5qV0kSttsjZE+TmGdcgwAz/9qP4kxzXF9xFfRlhzDnGLGnCnlWy/9MaF//oOG1fFdgbIDBFotRILmz3xeDL7fdIvyfvGOrinv835/9ClvrfXcQ3z92NdQF4OvyLaHDdODgcEh+QYjyL9pvtEIy8iuKoLkcNXISlatEXVsTjCxXzkY2Sh27PvfzaYwIi/BWX0qibUpNq8Q97JBg+uxD0wiPyJK+PhUDzlnuPnvU6Jko42N6BhM88sNO/YyP4ufTMXeJkr+TLuXiA+cyWJTT66PEI2aWBHfA/OXMxswpSRDVJRu35xmcq4bwc7bZBNox8h07tnt4AS/5Hf8kosJ/Oluep8kzWgZ3h/Lq7toKBclHW3twGq2U1ordt+sIgd7H/KQ11ds3PYeVhpqNTlhqW/bXhMJzU3culuCdH6sk5mKl+w+EtTjGmDmmVdzOGeAqOshv5nFrsvexGaBgXeP5q1f7iHYJkp25HkBTEkpeJ8VpXx2yM1pP/SBRTr6shcSMH/oIe3CQQDceU8Dt7RGyJgtdb3naSvXn1hHYK0oac8+B21eB8nxfSpubUnisgUWst7ZDMD512USKa4iJl6R2KaMZOsdFWRnyaijZN4zRElni13aalqbF4s5hivRePEYfI0coZ4kXcEIjz8KGHj36G+7CgYG302O4s67KzbvQuBpZLccDTystf6HUioNeAEoQuze52mtPZ+VD8AHOefookIPkZCJhDTxikicnEvDgiaCAXmPeNvttIZFSY8YXYtjeDrb44tWpSb7yD/HDVYJuqma18ROTxo5JlGXO1UCRZEg22xy/uiInwFjG1myXkLIJ+XVkjY9hVvmSVm3j6nDPn0EseIyOX+eiaF39KHqnq0A7GlKpXdyGys7RI0mxDSnnNFE3fvSZlmTYefrLnYo8aA492I/pp4FeJ8XNbpxZw5jR9YQ8oqy31OSwcipjSxYIYEwqdEoJ93Tl803yxbvJSTQFx/DbpRAmuV/81GY0MHmoMjdWSMqcJ1zDPv+tBOAjoCN3VpsyoPNXjrCVgYfIyMYc4IJ29ShVN4vowabPcrquixGJstSAQlJIUwWza5SGdWk2QPUBJ1UWaVthsV8DP9VFtikrZse30HWc3cSfly8Qv/7Xyfj8JKVJ8rakRbFkmJCR6Rtaja4yD82QLheRkkbNuUy8dw2ds8Xm3tGRgcpAyI077B+3iNj8D2mW2ze//1t12zeF/7xiLN5d8XPOwL8XGs9GJgAXKOUGgz8GnhHa90PeCeeNvgWGGz2fvFBBgbfR6LRrn2OQL70krBKqdeAf8U/Uw/aTWe51nrA5537es5cPWliNWXrUg5s9ZWW0YHFHsM9QhRZ/QpN+hBR5VFfDGJgzRMlHSgV++iSuJ/29P6VoDTNlaI+/QEr7WEb8xyS+R9H1TF1ZYiHlNjXHR97jYwWDwtroYvlLyUzabzYdYNNJiJBE4lFcjN1SGPrl0Jga3zjiE3ZDOjdiHuM+GO3rQ+wozSTIX1kWdOk4XYCJX4qd4tytphjRKImXldiA8+PKuYcU0HtFkm/GU5lmmqjYKDYwEu2pjP0cgvhYlHPFasSWRdK5vwb4t4tw0dTfv0bpPcUv/NAi5kPKsQb5Lj8WuzuCIFWUc6VVSlkp7ezv0FUe5PJSg/lpzYmdZ/+iwS8b+xl8w45320OkZHSQX08vD07XYzXdpfMD6zfn8OkUVXU7hY/8Z6ngm+rF2ff+MJSMQ0WE9XLxObu81sJRCykJMioqKY9kZ7pLQQCorTnxdxcltiEUkfvhJLB4dEtyvuJX3ZNef/or0ec8v5SNu94QM4oYDVfYhNig6+X4/Jrv+0qGBh8N/k+27wPHKhUIvAe8Eet9Std3YRYKXUVcBXAv+745ZgzFq7FYpcGtTg1th52dCi+YUBzBMeoXDo+lHeCa0IWDz5j56xkUbapA0O0lVhISBc1aCuwUvGeg6Izxfrz3rMuiu1mrrhUFHakopGFb+Vw+lli5934kotBw+pZv0XU5qSzW/nw5WRGDZbydFSR0M+Kv0TyTzq5FzXPVGOxSf3SxlmofMdEdZuoz6GDa9m2I4cg8tKeNKsBTApLnjTDwsdtnHaDDd/yUgAWby6kyQKzE0VZVza5GferFBqfk98tthjeZgeeNgm6cVgjZBd6aWsUtZyS4ydhoPxdvMiO2RTDapW69bgolSceUZzbVzxRwj4zpfvTCWtpm0ynD6s1SjQq6ZWhFOb+wIt/s4wqXOeOJrplN5EaUfVvrCxgpKPlQNuuedbB+Gus3PK4lHeqX1NutTIoKouCDTvbz4vz0/DH9YvHpDnH5iEQFKVttURZFE1hbHwhrCyXjx4nBGlcc8QJHoNviG5R3o/e1DXlfcW9R9yD2CXlrZSyAi8Dz2qtX4l/3aVNiLXWDwMPAwT3fqQbFq6Vgp2d2zTSfPRGQnU3xYs6r4Hd46LDXhfMwOCoRMeOXrPcF05YKqUUEpSzU2t970E/LQAuif99CfBa91fPwMDA4DCIRrr2OQLpivKeBFwEbFVKbYp/dwtfYRPi8H/+wgcVucwcX4nJJu+Nlo1R1lVm4zPFw6bXxpg6Mx7+/Wobc/PDlO0TV7300R4c7ghtNWI6yOwNPaaHufdl2R3mpot9jF7Zgn+tmGUSZg2i9F0vC18RZXrqmU2UveWm1Cb5TzlpMqN2LKG9QfLLnu1iy9OKoadIUMwz/wwzMyPG/mo5P9nbhM1uZVt8t/i+rVb65DaT9VMJ7d9+e5C+x3p4Lh409MObbKhevXENGiLlv7AIc6qDp14T97xjbV5+cn8zf0iKr8Fdk8u03Br2eWSScaHNyu3JrVyzX67n1eGJxLwBzCYp/37sTPQ7+OhhONbZzHSXpnin5J2d5mW9zX5g+YDsXm3Ul7lJzZK1zk/GR8VrDjLjK/F6522QNhsik7998WG1RQnuFU+Wfj06CG+PcWF8P88NVhd2DT16iHeo9cpfsH3h01yfJutzp42z8NLrGZw2VMw4zuN7ck00SnC9LCXw4KYCbhzkhDXG7vEGXyNHsfLuSnj8CuCz7EHTurc6Bl+FY53N33YVDAy+mxgTlt3D/3J/qLfa4ScF1Swok0CVi37lJrRqF2UrxEWt1/EdtO2S439Rn8Q4nchZqaLW0kZrbKedSO3tywCwOqLEoopHG2Whqg8i9czrHaamTJTrulgSKVHNzFNlgnDVgjTalYnxvcQ74+NFpD4OEGr2O/jAYWNqUFwSM5J8bGlPpY9Z1GqvUR5MDoVtoOxmH61vhUiMyuXiyld0bQEtL+5hQYVc25hYBz0HNpN42kAA/Et3Yc2xsf9dUfoZ+e0sL82j1SzvxuOsLWT38WKS7Ni0Lodyi5WT+8iyrvZMuGd9HrdcJ+oYu53wOmms7YuTqdV2hqZKR56S5+eFPYW0xXfeuTirlqT+mkijTDiGvCbWl+ZQYJMJysELf0ps56oDAVCx3btZ/vcQWRZx9bOYY/Q9PUzFm1JXZ2IIV0YIFTe8LdleyE5bjFtullFS8N1tmNwWWiReCUdymGjQxLYScUpyqQgDRjTQVm3sYWlwaLpjwtL3j6u71MElXP/Q0TlhafDdZvvi5G+7CgYG302O4oWpDic8/ja+5A7y5WOn6R0VmfRN95AxTFzGajY4yTsudKADaotZGTNcXPdiEXi5uJBLrxelGt68D4CW7fKS3FeVypjz/Xg+Ene1jlY7T4dTuCpZqpR9WR9qHy3F1yFStveNPSi+p4KEBCm7tCmVN5xwOaKs/4mDe0/38+yrYte9+HoHOhwmuKoMAEuWndULUg9snqCSE9j1bIzNxINwwhEmze1g33y53mDYQlPEznGXSnm7no2RneulpFzU6Wa7nWOjHQy5RZS696WtWBIgJofjOnMYwXe38doqCe/fbI3ws0QPWSeKOt63QNFjvATT6Jgm5tN4q+RaP6zPJiUaZcrFopzLXg7T81T42QJRug+cG2PnczDPKuk/vXg2i05/lX5OsXH3mOpn85up9MgRV0JnSpiyPWkkJ0p+Le0OAlEzG+1y/uRYO/1P9rH1dVled9Rvs1j2fx4mnSijnDfeyyek4LzLxQa/4FEz/UwdpKX6PvN5Mfh+0y3K+94ru6a8b3rkC8tSSs0C/gGYgUe11nd96vfjgb8Dw4EfaK1fOui3KLA1nizXWp/etSv4bLqivD8Oj9+glEoC1iullsZ/u+/LbkJs0P0cze5QBgaHRTeFviulzMADwHSgElirlFqgtd5x0GHlwKXALw6RhV9rPbJbKhOnKxOWNcg+lWitvUqpnUD+Vyns/epcpvSo4f2qXGa1x4NJwmb2L3Me8FqYX53Hor1iw/7TOQHOCFTTOv+TzilpgpvMoaJc/Y/Vof0RdleIt8mkm5M48W4vif1Eunqe30NCaozMSWKYbX1hJ/8kg7+NEbXX9E6IswNmVlhF9c8JRSh7XXFO3EPC9w68uK2Q8+IbvO1/28740z34dotN3Or20xrOYbhFlPvbDhfHegM4nOI90u93/Yhs20vDYlGrVZFMemV5GP872cCg7urNrLK42PRnUbdjTSmkZ7VTUSneLR/saeEnU83MOVFGIqdUR6jfl8Sul0V5K6V5bKXcikuHVVC3K5GCEyXAKGtBGCsxdFDSJT43gVd8PPgrsTmX3lfOoAus/OJDmU8ou/AJ+jrMBMNS98C+CHstdpwNMqpIi/jYSwInpEtdG9oSGDWjmbx1UpeA38oZb2qezJPfF//RRp45SOtuyW/O79Npemwbe58Se32T2c0JvVrweWyf/cAYGBwu3SdsxgHFWutSAKXU88AZwIHOW2tdFv/tG5kl/VIbEH8qPB66eQd5g6/GpfHdcwwMDDqjY7EufbpAPnDwf7RKvpyIdcQ3Yl+llDrzy1zDZ9HlCct4ePzLwA1a6zal1IPAnYgd/E5kB/nLDnHegfD4i1LGMc2eQFSBr16KfgAHP3d68bXZCAUtXHajk8bnRGlGmxS2xCgv75KFqJKjmgneeu5qFbWWQjK/c3p5Lb6FY7//7seuUrAXiR32oSWZHBOIMtoq+Vld8JNoCNsZJ0h+qz/CZo8Qrpf3TqPJwqPaxQWbRU1mZ3k5u28FzWVSQE7fNpTLTcVeWQJ20FUuikoayZwob/e0lT7C9SF2eOSepsxbj8UNLhkYMCTcROmmNHrcJ1MDMZXPKckNZJ8m6lZ7gjSuNDEwvqxr89o8WrYqXNmingOtFgqnBGhYLWlncpirjokASSz6XzLVVsVxi2QUUGmxMqNHNf+ZL4tyHav89Bjg4em/SllnD/Siw27+r1r8wu8cUsfD2wo5M+52WFWSTGE4Qu+xkp6/uZA5x1TQWiZt37dvE0++98mze9GoCq5fnU9CumxgPMTnofDqHgTekw2LldNJeUUqPQplhHXpiRbCxeD73EWEDQwOky4q74P7qTgPx6PDu4ueWusqpVRv4F2l1FatdcnhZPiVw+O11nUH/f4IsPBQ5x4cHv9YwYUaqg5ZRihoOL58VRb9z/A2MTA4JLprFoyD+6nPoAooPChdwGd1ZofOvyr+b6lSajliwTiszrsr3iYKeApo1lrfcND3B3aQV0rdCIzXWv/g8/IqGTpTlzSkkmwOHfjObo2Q01MiGn0eG8lFQep2iDrsMcdC5YIweZNFaYaqQyzdXEBqfBJi8jVm9jzazmvxJVenB4O47CH+F0//fEQVlZvd9LlIlHP58+08EnYfiALMODObxx9RnJwkSrepxcWQSxSRUlGbS9/PY2xWPWarPABrqnIoMncw5Pa+UvneA6FqH7tu2QZAwYAWtm/JomfcQ8ORFCEaVlRViPdKVqaXaNjE/iZJF2W24PXaSUoSG3pjswuXQ9qmz18nUHHrCnw+UbompekxqYO/fSjzAb+50oJyOel4Q/y839xRyLn39uPfN8vmCxPDfkb9pT8dz34EQMKZwym/r4S0QlHm0aCJaFiROlN81mONrax9OYkEs3iDzLPZmO2HsRNk1GLJsmNKS+S1Z2TU0WqGuRMqD/i8xzzt3LkwmQERsXFPTWlgXVMGp5wt0vqlV9OYklaPK0Oudd2OPHq6vCS4PnkWDAwOpju8TTpum9sl6e26bd7nlqWUsgB7kMDEKmAtcIHWevshjn0SWPixt0ncpOzTWgeVUhnASuCMT012fmkOJzx+rlKq0w7yh1MRY+LqE/r8dUKntOlTa17/5srOt+3NHYUYGBgcgm6asNRaR5RSPwPeQlwFH9dab1dK3QGs01ovUEodA7wKpAKnKaVu11oPAQYB/4lPZJqAuw6344bDC4//XJ9uAwMDg2+dLppNupSVxLEs+tR3vz/o77WIOeXT530EDOu2isT5Rg3NOWe4yRvQi323b6HnZekA6LZ2/Bs09kQZrusI2Ozy98pnEumVHuCFt8S9be4PfUxuruGtKpmE831QQSSaxrUjJXzcV28hqb/mlgGy3vbOh1PxRyzEGsQsUzDbyjkvhFHxkPHNDwZotzlpbBH3NYspRvNSL3U1YtYYnd5I2sgYW96SCc2JvWpImZ7FDXfuB+AadtP7fCu9p0igTPkKNybFATOL2R4jaVI6SfVS/v5lTmy2KH1yxSxTWpPKmBlNBKvlenMvLWTTPR6qfv8BAAG/jXDEzMDTJAjp1dfz+NW5rRCU+kbrmjltuwIqeTHPzMrrQpxgE7NEbcxJrHgfNXskaCbwf5Vk5wYxi9UDb52NXQ1pTPHHw98njKTp1XK8MXkkJgcVyWY/ZrekTUkO3nnKwTCHBCgVB5KwZCdQv0DMIt5WB7+dUo8OyLXHAjFOuyKRVX8Wk9e5F3mJ1ERY+q4829OnVtNWYiISNH/RY2Ng8NU5imMgjFnCo4AX84wO0MDgUHTRDfCI5As7b6WUA3gfsMePf0lr/QelVC/geSAdWA9cpLX+3NknHYnieWgVy0P5zN0gS4G+tqqAs+cEePdlUbenzGlGVUiDT/p9Jq3z6rngbMm2eXEr8+rzOMstE4yOQUm4KwK8t15c1nwmE3NmhYhVi3tav2lhAvujLJ8vO+fEUDSbTQwslAnL/pmNLN5ZwJCzRH0ufCWVD+oTuWKOKOO6d6H8/YQDO7KvXpPLCSOHc09wHQD+DW00v2Ph4UYZGfx8TDV9xmUw75/iqphVGWWipZqGUlHKKRk+rI4YO/eKe96E32fR9GQTjvh+RKvubqND2WhpkraYcGwNYY/Cv1fUw+zRFYCbnY9LfV+1uLnYnMLCOjinfwVFwRb+6ZUApt+OrKHkaTd2h6j6/OFtfLA6n4nJMkH+QHsqc1QYz3IZFaR4NzL710XUPiq7+mSM01iOGYxuksnftjcraDW7KRghx7tKQpiH9CelTiJ+O9bb8e+LknzJWAA2/m4fI4Z7OOZcGZXEWiNYB+QytV5GSW+/X8CUoZV4a40Xj8HXyFGsvLsSpBMETtRajwBGArOUUhOAvyDh8X0BD3D511dNg8/jnP5GkI6BwSExdo+PH6xUArAC+AnwBpATn4WdCNymtZ75eed7f3aKVg4rT7+QxEmJcXe9gX44aGTz/up8hqaJco5pRfaECI1rRZ2l9gmwdE0BvU2fLGa0Qbm44FTJ663X0jlhdCWt5aJ8fR02bPYImUNFqW77IIM+vZvwt4lnS1Wjmx45LbwTV84ZkRjTLuzgvbg73Aqn4gR/7IBro9kUoyqSQEjJ/G1yLMoem4XLbhUlHXpnE5aiVNY8KQOaBpOVCbl16Jgcv7I+i5EuD/nHix143lvZpEc1qTFRxz3TW/C0JpDkErt13uQw897KZnqq7DBX35RIXcxBoTXu7hczEdOSd3pKB1kTY0Qapa4d1RZeqctlTpa4+qUe68SUlkTtSzKqcOcG2LQlh0k3y/zAyru9DB9ee8Ambs114tsVIBaR/N8vyeOUc1tpXS1tn9RfUbfOSkZ/scevX5VDgorSjAQ4TT2rhf2LzezzSf5Tz/RgKsjg1kflWq8we2nqcJKf2XbIZ8XAoDtcBdtvOr1LHVzivQuOuCVhuxQer5Qyx90E64GliHN5i9b64/2DvmyoqEE3kp7S8W1XwcDgO4mO6S59jkS6NGGptY4CI5VSKYgf48CuFnBw2OlvUkZwRUEWW812zu8riq1yoxubLUpiqqjjKZOq2bhCAj+aTVZOGWij7R1Ri8mBAKmxKLG48i3XTs4cUMn2BWLnzdZBHBMLiQbEjGBq0qQdo/jjUrF539i/hoqSVN60iLy8dlIVkTaNOb6o7eSJVVQucDG6nyxjWluWz/AB1USC8o4rLktnaGYTNqe8s7aWZ/KDwRVsv106z8xMC4+udvDLM2XksGG+G4c7gj1fzi+oDbKvzc2WRTKSGKF8LLQ7+PVcyS+4LUDpmlS2+MRGPm19HdPcDWTNlCAj87ttLGlyMjZPlm2NhhVllWlUN4tHSf6Uviz5lcwlTJ9Vx9mmalKmikH9naccDEzxsLVVYvVP7FPJsVdp/vgvUdJjlBlbgZX9y6SsteuTaTHBedmi3GdOreaDF7IZM0hs2JaBPXDuqkLZ5F6MPb4Oa1EqoT3SmMqRTJ8/DGZPvD5/WZzG+ECUn/eqjz8XGlPjkfmfxuAI4gjtmLvCl1qYSmvdAiwDJgIp8agj+JxQUa31w1rrsVrrsWe5ig6nrgYGBgZfjlisa58jkK54m2QCYa11i1LKiaxn+xekEz8H8Tjp0u7xNkuUffWp3DaoDudxsvNt8dogk/tJv1+6PZ0BRR5c8RDtvSYbi/4egvhmB/0nJlBU7iFruNiEB6f7wZTAK2Vi4z43HCC4voLaElGiWT287F6UyiUOsasG2qwM+XUu6i/VALz/YR5TptQwUIv6tI/tSfVHgQPh+LUVmvf25pOnpbwURxB3bgDHAFHGk8e3UfOOk4wMUaNpY+CK9Q1Em+X8GIrke26g4ep/AjDqoijLnrGSGH9YdlkS+IGphX3Pi3p9TueTZzORFRG1oDV4WhPY+KyMFFwxJ9eME2+NpmInrpQQPfPEzzr7Pz/Be8v99LGK0t6zJInBP3ER3inXetK1BbQv9TNtmLT1uvdzMK2CcXFL35QTanh+aT7DojIimjOhgpgPEubIEsTL/lDPxAnVbF8t9v3ewUo2VOQwxiSjFGdGlGh9Gw27pa6vbnLz046N1Fvl+Ov6VGDLMGEb2wuAN+4NsssGP7R/vJeHgcHXwFGsvLtiNskFnoovRm4CXtRaL1RK7QCeV0r9H7AReOxwKlK6Pf1wTv9e0VTs7JTO/s9PvqWaGBh8t9HRI1NVd4VvdAPiDYVn6AE/TaFlgdik7e4oEb/CUyNqzZEQJn0sNK4ROZg104kOiIpteC9CKGjBmSjeFA53BG+9Hb8/vvnv7Ci759uwmOVm5fdvxZphomWnvJ+S+4W5ZW0WpwTE3pxlCpJgD2N3SP45x2uKFznoM1VUdONGC+nDQry3TOzlax1ww4Aq7t4t0Z039qjh9dICeofl/CED6kg6KR/T4EEAdDyylM2bchh3tqj+mmWajP5+dsaVq9MaZnc0kVOmi3LVgRgVaxLpcZyMApYvzWZMz1rcQ6W+wfIQjj5OPOukvIwL+zLvbx2cd6bMB6z/n+vAglg5d0yn+a7FuI+RdtX+ENXvWSj8gYxIOj6swTnMzV3zxRPkuqJqbOmwZ6XMHbgcIVaGUjilUFS7MyuKtdBFuEps+/vXp+B2B/jAI9cyNbOO1FGacHV8xLI3iWKfm9EFsvDki/W5nKTa2BmW8pKjUabe5GTZvTLPMSir6fMeG4PvKd3hbdJ25YwudXDuRw6/rG+aby3C0u7u7FvpSAh/5rEN70U6pR3uzumi2Z3zyu/f2imd3K9z3lmmYKd0zvGd72/jxs7NstbRuT439qjplB4yoK5TuuORpZ3SNcs65++0dq7PxyHlH7N8aXandLC8c+xTxoV9O6XX/8914O+cO6Z3ztvf+dyODzvX/bqi6k7pj1c1PFDXrM5tu399Sqf01MzO1167N6lT+sX63E7p5E/51Bodt8HXyvfcbGJgYGBwRHKkugF2hcMJj38SmAJ8LHMv1VpvOnQuQoIzzNv/CGPXWey1SdETdTv9Z3ZgSpNJyVibj2fbxJ3tRuVDByO81iqug9MtLVRWpzDqclnTes+dtfTt24RnlTjN9J3kx18lYfMAuiNIcr8w9VvFRvzzhFZei2Uy7VpRj62v7SNxuIMF86W88SkNeLaaKG2Q3yvejdFsMfHLmTKpFiqD86M+PtojppOgFUZNqEVZZMS19SE/hT3fxdVbTB1rHFaK3o9ReLfELm244iM2Wx2cN1jy62FqwTpuGLF9Ykba+56LoqQIA1NkEjLYYeXytx3cHd9gzpoW5N2afM6LT3gO1x00rQhTlNtC5unpPPVENp7rNgNwVf8osVY/kUYZpWz2ZtLwcJAhEyWvD+a5KLKa8LdIUI3PA6GYiYz4COjc8VXYLzyDtrvnA1C52UzfXw0nskYmTBMzgpx/12gqbl4iz4lNk3D6QPx/kgnRU9b9huIpv+an14pZq/TBGKdMraV0mdzn0IY6Uk1pn/e4GBgcPt/nzptPwuPb4zvqrFBKvRn/7eaDt7c3+HbIPN2Y7DUwOBQ6cvR23ocTHv8TDtotoiu0XTlD717k4jWrk0Qtatmu4cq5PsLFMvFm7Z1Kxfx4ePixQZo2mMm9ReRi5L3VRD1B5q+QYM5ia4y8qInz4ru9b9mQTX6qF4tFlOm2xnQ+dGgmy7wYfVxt5AxvZ8tHouSrzHZcsRjH9JJJw8r9KditEXaHRLmbgZnntvLga6LExwWCDB1dz46Ncn6KI0CfSxPxfSh2Y2uWGWVS3JPoaSQAACAASURBVPe+jAxunFrLtrdSGTxZwvct6Q4aV0ZJyhO7sq3Ajg5FsY7uB8DWv9TTGrUdCMdfZk1gQiDIMTfIxGPNM7U85kvnUoco8/2eZKot8v49/8+FrPvlXorNYqAfpH30H9/E8+tko4asiKbMqphhkoFSz8k+YoEoz66SJVrnDq8g7FWs2CttO+PUBqqWW0jJFtfBlcW5TB1fhbWntM3aFxIYf4UGuyjr8mc8uDMChP0y6mhuSqDXuFZeWS35T01pIKVXAHOC3PeW3Rba2+zGTjoGn0l3TFi2zD2hSx1cyrxlR9yE5VcKj9daf7x7/B/ju8ffp5Syf8a5V8V3TV73xK7Kbqq2wcGc/2djJx0Dg0MS6+LnCOTLKu+Pw+OvBZqAWsCGbNxZorW+4/PO31B4hrZZ/x97Zx4fVXm2/++ZPZnJZN8TSIAQdsK+ibIroID7Uquv2rpW22pr7fbW19alVqu11lprrbsoqIiCuICAsooQwhICISRkn0kyk2Qy+8z5/XEPiPxQgyACnovPfMiZOctznpl55rqve4vQ+z/zqL9lAQApxX5ceywEY0X5w2E96fmS/t1WbyXOFiSWDc/brZnMTnKQMkR03M0fpmHVh9kYq6Z0RUkt/hY9O6qEGQ8uamb7ngzGXSXszr+9DXeNBb9fdN6coZ34m3V4XPK7k322kbbVPtpaJHpjJQnc+Nd+hFevB6B9g5eg10BCllgGO8oyyE72oNdFD45dp4+SOVJ040/ez+D1uBCPXSwRFpWvRUjP8bCnSmQOo6LSp//nSSrry3OZerWftuXCjrfXpjNqSOPBNmeTcyRSxD5IJkSfamXf63Lt/NEemrdayD1f5mLRc3HMvaQdxSz36i9rJW5MDp6Vcg41CsYElcotMpb+sz2EGgJs3iRWQ4I+hNUcOhi6qM9J5NmX47moUH6Aq/ak0quolY8qhamfe0kHuswUvCuqAdizPY1lZgtXmMRK2OJJpl2v8MNrYu/dU2EGT26lbbvxyB8WDd97HA/m7bp4UrcWuOQFK09P5n0Ah6THn6OqaqMqCAD/BUZ/GwPU8PXIH+35roegQcPJidOYeX/j9PgD3eNj3eXnAdu/7lwf6hK46Vw3jjvmE58kbFQNQ2K+/+A+y7bmk1Mh0SFjbrNQ80wr6wISbmEBEgsCePfJb06CIURqShdj2+THVWdSWFmTw8BYOnxNVTJGorR91BG7FwPvd6Yz2SL6esPWBHr9JBvTe7sB+N2iJO7K89D3JtG4E15ownFfE0lDZWyJI8xUvmM62EatX7ETd1M8kdhv4BJ/CmODfoxlwpxbDXrGhPU8uED2Py/sJT4nynuNwjZ/3rueN3bns0EvuvINOj9/nx/P1bnCVr06PUGPntywMPlAlwFTfJg33hd2nBcOYVEiuFeZKezZRs4MPUueFc17xoBalr2ax9g80fNTLihg/aN+MuJFs05I9OPYbyE7R8aqGE0Ys0xsjTH1ah3cleHGtU0sImOlmzg1nsbqRABKbk+k8h9BJuYLky+bn8Jyc5ifTRMNPKEyyM35rby7VzTvtGiEjEiUqFsSfYZfF8fSZ3IYkaalx2v49vC9DhXky9PjV8QWdgUoBW78Fsep4StQ2LPtux6CBg0nJdTw1+9zquKEpsdvKzxP7TkjSOMqHUlZwjbfqcxjnFmYZiSqoytgZOAc0VlVVcUwpIjg6h0ABFuivLQnn4lRkQn6zvRiGFhIcIMw5/+uyWWnLsD9/aXs6Fvb8rnsRvCukmgU24+mEFqxlo5SYbKWlAh1OxPpPVM08foVOnr+cQzeF1cAEGpXaNiXSI8Bkna+sTSbMouBeWZZLBNS/WytyqDQLsw+f56ZcEM71Z9ILLM/ZKAhaqHMLHLaz85oRGfRo+8jceKfPurFZgxRMFju35hmoHmTkVWdEnc+r38tW8qyqTXKb+ysgno27xHW/bzFx3jVxrVzZCy73jQz8A8FOJ+UtmQJ+SGe2ZLPPJswW3dHHLb4IOGwWAlvRhK5/cogO5+VT3dqShfRiELaMJkb0xkD2XlvLfMNYgXdlubAmhmmdrsw7x4l7US88Jc9ci+3ZTZjK1D5zzrRwHXA+LAXo14srH535fPS/W4GxgpfDb7QR7Cmi/bqI/q5NWg4Lpp36+yzurXApS5Zdcpp3idNhmUkelTy+/caz1t8X9je9aa2AGrQcCSop6ie3R2cNIu3Bg0aNBx3nMaLd7dlk5jmvQmoV1X13G/SPX5Lj7lqXTAeqxphh0kcW9Pj2sgeF8C9TayWpMEqNask3C1vaAfRoMgJADUfx9PuNx/UeJNnZqKkp7LrAZFFcgrbUfQqrftFttDpVdo9FpITRYaxWEPY8sO49ohTL7m3H2OhnfWvyPVGTXOiGBQUg1gBalTlo3czyNKJQ7VoRCvNO230vFocmlGHi7a1QVJGy/jmv5NG71CQYWeIbGNIt7Dw7TSywyJNFGe1YrREqK+V44f8ZSDb79xO/yvl3lve6yQuKXSwc0/AYyCpT4B3Nkqo4Jwz6zEOzGXz4zIen2rAhMgSO01mZqY0E/CJwzHeHiDkNxzs+mOMixD06rHly3b5unR65LsOOl/bmq2k5Xior5Gx9b9KxhCqlMJRzu0WurrMuALC8sc92h/PMyvZUiaFp9LNPhITfZQ7JPSw1KLjytRmkkfIeTw7QnhcZqra5PwBRUeFWcdFdsdXfWQ0fI9xPGQT5/TuySbpH5x6ssnRaBU/BcoP2da6x58k2GnSZBMNGo4ENdq9x6mIbjFvRVHygOeAe4HbgfMAJ0fZPb605xw1f1A7S0rzueByScRxfuDFYIqye7846YYOb8LXIkw25Ndjig9jLxE2qc9OIbzXQXvsJyT9ttFESncQbv48ztk8bSSOv28GYJkji/RwlFAsyycfPxuMcUxWxMFYG7DSodNz9mBh7h0NZsqcaYzsKeF1a/ZnE1U4eHxuOMQWs5EzwsLk01O6SB0YxFEmi2fWmSqmG24h+A/pnONYp6OyNZkJF8v1PKU+NlRmMyxH2OaTbRn8Zl4noX3y+qNb87gxp4H3a8Tpd8EsB67PogeThvrMChCs9R38yQ13Kvy9WhyGk30RMq1d6GP1zJMyfZiSozTvFCuk2pOADph4k9yLbkB/Xrx9N3OK5N5d9VaCQT29r5JrdaxoxpKtorPJe+HaopBYFGLnx/I+lfzUiuf9akwpcr6OfQZcrfH0HC33EvWrLNyST2ZYxjPtCg+eTR2YpQQ4Ub987tr3az88Go6M48G8myd1L0knc+Xpm6TzKHAnnytIqXSze/yh6fGve2qOabAajozJvsjX76RBw/cQpzPz7k6SzrmAQ1XVzxRFmXS0F1BV9SkkfZ6OH89Q1y9OocECb70sySLj072knJfFqmdk/4935lESkB9LixrFRJR0h0RXFF4SYOP7aYyaIsw1vHYLjk9UbGky+2318Xz2UQMpEWGH0zMcZN1cTPVDVQBk9PEwdGI+C/4pHWWmFzaQMEBPMFZyxdlqI6AouBzCPo2qylkj6wm0xpKCSiwM3evlozJJPOlhd+GthfWxErZ9FvsobniYmlLRda3WIOstejIXi+XgDNiZNK2J/Wvk/L+Z10nFQj1JdtHgf/2nAnyv15GzV34Tmz7RsactlUKbsFkMOhZtzccWkfmZdWkHfffoqDfomHNmPetXZJIbL0kwxoQo5qJENmyWe71gRiPNm4yEyiW137+ikQuGqny0We5lxiwH+z/SUb9AwiJ1iol9e21kZ8i10yZbaFhmYODUAzHlVhIuLSG6V7rDf7jBynBdJ8/GClGdpXZy4eBaanfIXEScXXxWkc1gr7x3m+symXaFB/Z/nqClQcPxhho95Qh1t9GdaJMJwBxFUWYhSY524G/EusfH2PeXdo/X8O1jzpna1GvQcCScqqy6OzjawlSTgF/Eok0WAK+rqjpfUZQngTJVVZ/4quNXZF6impUoqXYvDe2ixY693Yrz1VpSzhD26dncRcgvTNfnMbHbncTUu6XQlGdBKXt2pDPs18J0P7i/g5IcB6lTY80XOn2ULrIx7Gp5x5Y9H0emGmBvrEzq+VMaMRRlEamXEq1tG6Jk3n8em69ZDcDAya3461V2VEgbslHz3Ox828rQewoAqP/rDjJnmGhYJsw4c6ifF9bmctUUSRE3TRrBq79rYFyCRGioUYVoRKHHRaLrlr+oElUV+s8WjT7sCGCZWcKOPwp77TPeRfseI1WNUg5g1OU+DFPOQi2XJKW6ZxvJGh9hybsyH616hblZcu20e+bS9ejrtOwTVv9U0M61Og+9H5sCQHTDeqKtnRimTADgk1u2MajIQdLvLwTg/avXUmRtJzlb9HzbcCsLX08iLda2zKyqTHx5Ck/G5mqavp29vgQmjpIfjobtduzJPj5rkLGdNaKeyi2p9J8pvg1dig19ySA6npbjH96Xw5V4sCV8sSWdBg0HcDw07/pxU7q1wOWuW3HKUfRjyYz5FXC7oiiViAZ+TN3jNXxzpN0z97seggYNJyWiYaVbj1MRJzQ93vfS79UXf7WPC0fVYkiLsdElNvqOa2PdJ5L2HVEUJl8p7G/7y3oG3xhHpEZ00qgnTDQYRdHJZFesS6GSeGZPENF6yZo8Zg6rpaNOYsgjIR2pAwNsXS0hDvsNZvrTRdEZoutGg1FadsXh9cr+3qCReizM/j9hjw2P7mRVRzoXTRZ2q7Ma2bk4np2KxIVffEUXz8+3YouZZuuMQc7z6TjrNjmff20NpkIre94SzbvfbwsIrirjnVXi250z28HHi1Ppkyzj0eui5N4/leAb7wHw0XuZ5Op99L9Czv/Ea1Z+PLqOxs3CrtMKPDj2idURbw2SfpaBDxaKxjxpXD2OHVZyJkq6e91KE9nDuw7ORWq8n3BER59Zwnx3Lo6nWo1jn0nmdr8S5H97OLBfUAxAaEslb3+QTUZErA4DUcb+JgXnc5UANDbZGTDbQ9X7YuU4/XGMGN+EebDEgYdrnBgKM1ADkgoQ3NXCgk/zmZ6sxXlrODKOB/PeP3Jqtxa4HpuWf+21FEU5B5GM9cDTqqo+cNjrZuB5YARSMvtSVVWrY6/9GgmnjgC3qar63lHcxhGh5aSfBtC60WjQcGSoUaVbj69DLEnxH8BMYABwuaIoAw7b7TrAFct9eQTJhSG232XAQOAc4InY+Y4JJ5R5d1w3XXVu0B10IqQU+fn51jTuTpSIht+7E3hseCvBVpnMJ2tyGRiEoUmiIVe4kimyt+OJMeVeI1xUb07CEItt7vOTTJ79m5+skGybVZXB+Q4aGqSYksUYpmB8J4pF5s00bRzb79xOXqEw3xdqc7n1vkI23yGB5Pt1ZmaMqKNljzDt/B/lEtq272BT3yWl+Vz8UG/8b6wCoHJtMl1hA6HYb+KEX1hxvVHN1lhzCAPCWPcYTQe3k8NRcmIZnINvsXHtUx08mCqaeGV9KmNvknOVP+0jGNEzaJbM1d8+zOB8fTsNXbbYuaI06E3MuzpW9yQcpXZRAF+s8UTPEjc/Lkvk7IhEn0y1O6lqTWboALEqVlTkoVOhl06sntawmR1mA/8zVOLAfQ49cRkRFm+WbM8ZPRr4V2M2v7pKNPHmN9tQVYXUwfJDUrfBSl2XjSG9mwHwuk0kZASorRTLoGCwi8qtqWSkarXINRwZx4N5V5dM79YCV1D6wVde6/BclhiTRlXV+w/Z573YPusURTEgzWrSgbsO3ffQ/Y7+jj5Ht5l3rBXaFkVR3oltP6soyj5FUUpjj5KjuXBK0RdDxH7vTjiaw79XKH/6i4Wo/vZhxhe2DadzAQcNGo4Bqtq9RzeQC9Qesn2k3JaD+8Si8NoRf2B3jj1qHE1hqgPp8fZDntO6x2vQoOGkRXfjvBVFuR64/pCnnorlqJy06NbiHUuPn83n6fHfCGpUxZYujPupUjG/zwuqJPZq4R926eiiS0wg5JbwsqvsTrY504i3iyk+abKb9s8CmMyfV1jvd1c+L9wvsofpqQaGRyy8ZRGn2W8v6GLH/ETaIiJTjChupWOPdKMBMI500RCJw18pMsoMcwfe+atpVCTRJDESobUyjuyzhNmqni72LE9gwE9TACje4mbXr0rJHyg/3f0vj1L/jofUniI9dL0P9hFWJg6Te2tbH0KNKgxMEidi/KB41iz4/LfQtbiOSwNZfOgQKeSKBdN4/5L36Z8sc/a6Nw3PO0bOs8j8LA8ncfUkCdXbv8HGyKku3n1WWPmU4XVUdObSO072bd5p47m5UWqXSO3wnFv7En5kL84asXhmz3Tg3holPl3mJqXWwln3TiW8XObWU+/nz9syuXOIEIitm7OYRIit/xbZxKmmkaCG6VgjDtDCcR2YysKsiqXvu/QKF6TVYU+Qe7mnPIvBOiNT0WQTDd8eopHuLd6HJhN+CeqBQzt9Hym35cA+dTHZJBFxXHbn2KPGN02PP4Cj6h7/bIWWTPJtYP8G23c9BA0aTkpEVaVbj27gU6BIUZRCRVFMiANy8WH7LAaujv19EbBCFafiYuAyRVHMsWqsRcDGY723r3VYxtLjZ6mqevNhSTrZHGX3ePelk9XGzXFkDugi6JIJM2dC2K2yfpswtBG9mg6WJS06w03dp1ZyRwg7M+bbUYNhIq2iAbvLDXg7TeRPF/bn+ESly2NCFytzmpDkJ2W8CV22JPXUPtdC9pgAxqGFAESdbXg3teKqF4dkpSsJj07HudceEr1hi8fzntRkqa1Moj1kpj3mKC6K6yRnQAelsY7rmdYufAEjfcYLu31lfR79gkGybJKyHmcLYoqPkDRb7jW4pQ5DfgI6u4T+/fl5PbcU1BPyyW+qNS+KPsWEr1LYrM4Aa7flMu2n4oR0L67l031y7Q69jqk9GkiaKaF5oR0NNGyKp84jzHp4SSN6u4IhVaySincsJNgCdHrkN3edYuPaG2DPv8UhmtevHcUA4S55n1wN8bR1xpGWJPfS1WUmENYz8AJh0m++mcL0wnreihXVKgiGGXNmE2G3vBf/tyeLs/06imIWVltnHEZdlNSUri//wGj4XuN4OCwr+s3slqJdvOvd7oQKzkKIrB54RlXVexVFuQfYpKrqYkVRLMALwDCgDbhMVdWq2LG/Ba4FwsDPVFV99xvd0CH4RunxiqK8qKrqlbHXA4qi/Bf4xbEORsM3Q4dei/jUoOFIOJ61TVRVXQosPey5/z3kbz9w8Zccey8iOx83HEt6/KHd4x8B/Kqq3vVVx6/NvlAt18dx+a+T2PuQsNmUrC5W12Rz7iXC+Pa8rmOLKmxxtLGdpDQvrbFCUSnpXaRd2hPnK5JOvqM+jWyzj/h4Ycrl7mSmzmtDjYi6462Q5x375XzpeR4SJqagP2OiDMjlxPPcJ3Q2Cfv0dJoJR3T0OkPGEnZHWV6ax4H3f6Mpwo2WdupcolM79Aa2mKIUh4WJn53bwP79yeRmC7tMnRRP58ZOdEYZj3VEErrePXjpXmHmM3MbsPaI8sY60dj7hv2M/OdIWu77QOZbp2LrpRL1yvHRIOgs8MAWYe5z/SEaY2rVlLF1uPZYWOOSJBx7JMqYPo2srJJ9B5o6iER1FP9MNPFoXTOqP4RjtZw7EtMGDxTZmnNGPe4KA642sUqcgTgsSoQ+vaS0gCU9StQPS7eLlDdI10lV2MZ2cS9wU2E9+/ck06OvFLLavjOLSqORUGwur76wHfcaLyH/MYe7ajhNcTyYd3nRrG4tcP33LD3l0iyPpQ3aS1r3+JMDU8bWfddD0KDhpMTpXFXwhCbp/G/BD9SLlQ4KLzNRu1B060hEh8kcZnGXML7RwQB7DMIm5w6uZc2WXEbEmiOUV6eTZvbTHhB6N/KHQSoXqBRdKfvveTFAZ8BEZrJo5LmXJhNtaSewR3TVnVszSIn3UdMlTNxp0GNUYVLPBgDi86LUbPw8+qNwUhfGsQNYd7ekcPfObsOWGeSlcmGbHkVlZtRD8VUynsB2B+ZBGTQvjkXO6FVqmxIpyBemvbo+m/NmOyCW3h91B9HZDHy4RNjy9AtcuDYG6WiTju095yioXQE+fDum2Zt0nGt3ojcIW/6gJZMr/yjMOtrQBH4/LUuE6ZY2ZKAC57w+G4CuB55i68ZMtpllrBdkNmJJCqPEfr53bsmg/2AHpjzR0w19c/GtrOa+CtHQb7G5qHImccZ/xgBQf+d7JPX0EzdGrr/9qQDJiV5c7cLU7VY/tqQAKbOF6fs/ayDsAfttMwH49KbPGHGzieYFzi/9vGj4fuN4MO9thed1a4EbvO/tU26V1xoQnwaINjR910PQoOGkxAnkpiccJ5R5t809SzUPFSaneiQWWn/WGXQ8Ij4A534bvW7OYuejoqv2u0xl+YtWJo6TEMN3N+QzY2At0ViYd+JDP6fhf54gtUSiTQz9cglXNrBpqTTBHTXHhaFPLs0vi6zgdsXT76ERvP6z3QBUGlW8ShQnoo3//VKV5iVdpI+TOQk7AlRvTqJwrDBpNaiyaX02Y84RthhxhzGP7knHuxL7bLRGUQwQ6pQf8erKFAZe4Gf1AknPnzivjfcXp2GIzXkqQXLTO8i6SjRv1RfgwydUdMjrw3o2kzjMhL63sNsdj7qIqApDfi0RJhv/5KSol5QOSLn7AmpuXYqjQ/wDg8Y5qPo0mbQ0sTp2N6ZSbzBwTqHMpf2cXBwLPm9Y7PUZCUX01KrC+ieV1BHq1B2Mk726zsitoVSmXiORL8uetZCv+Og1UK7f2WQmtSTC06vk/b0wpZln3BnMi4gV1HOIm22bMhhxsYync7OP1kYbVptWElbDkXE8mHdpzzndWuBKahafnsxbUZRqoBOpiBVWVXWkoigpwKtAAVANXKKqquubDsS5X4tV/qZIufuC73oIGjSclFC7F8N9SqK7DYirgZGqqrYc8tyDQJuqqg8oinIXkKyq6q++6jxVg2ecxkaMhm+CvQ5pPDH+Zxa2/72TQdcJnwiUNeOsiCdHeknQsAKe9cu+F4a9FM8L4lovrN2cECboMdDSLASg+GcZrP5zJ31ShUtEVQW9XsUUJyZb2s0jaH1yE7urxZfQqDeRHgkfzEALKQrTl15O208eASDp0mJW3udm4vmSbRpq9qGGYfNGsYCy4r0YjRG2dsr43HqFczKaDkbSrG1PZ3zi59q+LS3A7t3p9BsovhSfy0j69QOpenAvAPkTfTSsNfOJTzJ546KQFwkwbIb4Mz55P4Mpf+2D2ixyWdXjjWzzJ1JvkIXq+plOnlgmfpQf5jSwuiabubGGJsHlpbRsM5F9vvh2nEvcxCWFMFjlq2lIM7BmWToFNsnM9QaM9L8lEeer4hc6sO+B+P+4YjPPLU3nbJssDTnzrATKWnDtk3wCS0KIxrpEBtwkFmG0pZ221T4am+T6/Sa7+WBFNudcHPMTJcTxwX/EGpzX9PIxr7yb8+d2a80ZXvvWKbfKH0uA8Fykozyx/+cd+3A0aPhqXBj2fmHbnBD+wvaBUMgDODx7Lu3mEd/OwE5C/DCn4QvbweWlX9h2LnF/YduQ9kVD3BswfjsDO4GIRHXdepyK6O6oVeB9RVE+ixVwAchUVbUx9ncTkHmkAw9Nj3+lTQtp06BBw4nDcUyPP+nQXdkkV1XVekVRMoAPgFuBxaqqJh2yj0tV1eSvOo8mm2g4HGtbhSmPsrfS6LLRt7eY39ZiPWFHiBWx7vYmVWXKZZ3oi3oCoPTqy7vXrCdLEelkwKQ2dq5M4X6jbL841Y8uMY5oe6ycblRFF2ek+j1hkzv9dooM4kwtujCKYtDx0YsS5jhhTAPLN+axwSxCyu/GNNG6w8Qup8gYhbYOXF1x9O4rY62uTKH/9HYeXykyyhxdB51+E0nx/tilFfp+8mdaLvoJAE21dlqDFqw6sRqKR7awYWM2KXpxnL9lsvC//9eD/fdvBSAp24uj2k7hhcK17lkYT35EzzSLyCgFfxrD1p9vZeAcsUocn6gYLWFWNorzuF0HV01rYuEHMr7pWY00NCRis8j1Ci42EWloR5coIbe+8i7i+lvxV4hzeVtZJgMHSF32R/flMjygYIkV5R+S4yQS0lHpkLnp30PkodRLpQRF3X/qeNRn58FYGYXdC2SxbAuJrDK0fxOmVPA1ygKaUGLh10tFUnm8+tVjXlXX51zQrTVnbMMbp9wK3i2Hpaqq9bH/HYqivAmMBpoPybLMBrR+Vho0aDipcKqy6u6gO4WprIBOVdXO2N8fAPcAU4HWQxyWKaqq3vlV59KYt4bD0d4uoYlmY5jXwkmcGxG2t121UW1QuSZVGF8kpENvjJI6XVhZpKGd1lI9oZA4BRMzfLxUnUdhUBjhjKv97HopSr9LZPuTl+IpSnfxUqc4KW+/xEvVAmG+FQE7kwbVUVomzLTCZGRoyM+Qs8Xh2Vaq41V3Jtf2kZDQih3pbDJZ6Bm7VkhRKDJ46HWmOPmMA/JQrHEE10lIavX6BBz+eIqyJKxSjSrEJYZwNQrT7/SZ6T/bw6KlYoXM6F1HXVUSxfOEGW9ckMCwsU2Y+gq7bV3eSdJgsSQAbng3jkd6tdFYJSGp20N2Zg6rPRiyahuTzJ5Xo1jMUoo4c0AXUT9sKJUQ1JEDGrBNzKb9Q5lrvTnKM1X5nBkQtpyb1c5rLlFF+waiDM9rxpYrczd/cz5RYKwqVow3bMSFkaFZwsA72y3s8tuZMU2cq959URbW5FISknOnJYizNxwrMZE9IciapfIezWqef8wr75qsi7q15kxoWnjKrfLdYd6ZwJtSwgQD8LKqqssURfkUeE1RlOuAGuCSb2+YGjRo0HD0OJ17TH3t4h0raTj0CM+3Iuxbg4ZvjKJLRcdd9FIyVyS2YE2K6bB2F5tKszHEdGdFpxKfGibSJCFlby/PZqStjcQM0bS72sz0DkZJUCVhK1znZpuaA68JI5xwvptwa4ARsUJcisXItqCEr00rOWx5EAAAIABJREFUrsMy0E7fVmHGZ/x0JGpTM68+Jrrshec6mfOhi7Za2b94oJMRs/tS809JeMqbrvLemyn0aJHojVuf8vDkVX46q+TeDvRYNVpkbB6Xhb/VpfDDsLBPR9RM49sW4hQhieZMKB4SZN87wkbLTUZKwnD/ArFSpvhMDE1vwlws2/++0Y5rUfvBOS1Wuoj4Ib5ImPn+N4JYzCplHnFJTerowjY+A0upjMvjtOB/p4XSBtHIp17ZRUGFSoJF/AfpF2Xyk2Rxb62/x4G9dxR3hZRZsEVhmKmdnX6xiGZf6cG/w0GwXe79F61h/tOjEdPZ4wDYcMdeJlo6COhk6fnUK6WEE+f1BqDsr20EleMX/RE5jWUTLT1egwYNpy2inL6L99Ek6RyeYXk38GPgQPbBb2L1br8Umuat4XBsbpFkkkkldewrS6EjLIwuThcmJcFHj18Uy44eD/ufrKfH9aLTrv6LhzFnNuHYJuwzEtKx0Z/M5EzRVuNTQ+zelk6PWFGw9tY4cgaLJm0ekUfTay0YLaLbBr0GrClBtuwRXTdJF6Lf6BYc5cK0zXEh0qfG0bRMmGiL20q8KUSvS4XZ7ngBmqIWpv9eNOkd99UTURUGXS0Lx+b/CpPMTZNSw/FJQeKL9JS/J2y198BWFpXnMyNDxu5qjaf4sTNYdf1mAIb0bibk19MUS2zRKSo5PdrR6eXrtG1PJsOHN7IuVip46l12qh6rJ3ug3K9rr4XnvWlcGSeWRe5FdiqeDVAfEs1dr6oU2jtISBFLoKneTn6Ri5W7JNInJRpmwq/k2q4Fe1ldk83MyRIlrAaj3P1pFjN8cq+Dcp3srU9hyHDRzwNuPa7meKwJMndrnJnM6F+LpUDe58Y1BrJGBalaJQlW/f7Yn/m/kZDia+pfPOaVd3nmpd1ac6Y2H3tky4nG0TDvyYdmWMbwiKqqDx3PAWnQ8G3CPCLvux6ChhOI77XmrUHDt4kZc4QPqF4DjrCFVL0wtHIlnklxHXjfFPb5UWkeBTozSx8WjXv2M2MJzn+bTzxShOzcPnVEKpNpdQpbjoR8vGLRc0dQWG+710KOrpPAljqCrQpbW3LoZxWdOK3Ag7shDodBvg5VOgOe9dmMu1T08offyiB1gcLlgyTaJHOSjn1LDWx9Ts7dp7+TPvHw6n2ynRaxEEGh707RxIv66OhqM5GQKcx21c48avYr/GiyZEDq4s2EdkHaTNGkO172E91Wxoghwm7LyrJIj/OyXSdM+YJxdXiq9TjqpbRxj8QO3DUWzpgQy5lTEnF0xVOzUV6f8sJEhl/1CRWx8gJVT6l8HGflt78Vq4dIBEKJ+JbvAqC5Oo6BPxrMHKDh/o2YrSF8H4lV0OWO47zLOjFe+2sAPpv+JDcnttPoE+a82JnFtTeEcbwlzDp1JJjtncRNKgDgwvRUgqsa0aXJ2LzeCP46L40BsaB231XLuIRvXCLp/4N6Gssmx5JhCfCTWAPiZxRF+coEHQ0aTgYEW0/fL/PxRsP9x9wj9ztHuJuPUxHdXbzPUFV1ODATuEVRlDOBfwK9gRKgEXj4SAdq6fEaNGj4rqCidOtxKuKo63nHHJWeQ7VuRVEKgHdUVR30VcdqDksNh6PcIbKHR6ejl85Lz36S8v3p9hwmzmnFUNwDAN9He7CMyeO1f8kXLajAFRe46dwkST2v1+Zy5dg61q2WRJsR/Zsw5+pwbBbzPa2fD6JQvkGkgpLrFCIOkU2CdQFMPSzsfVdCA7N6dBDy62h1iBQwXxfPdXEuqtokXM5ChJUWE3NjiSnx8UHcHXH0GSEOQb1NhyHXzqYX5Nr56R0kZPp5dbd0YBoYDDJkePNBB+O0187mxSs+pHdIwiQHFDt4tTqPnJAamxuFMRbXwUJRyUk+zNYQH9bJ8Rdd2sHyl22cOUFkGG+DjsRxNro2i4O0rDyTCb+w0rmkCoBIUGHH3gwGFUlS9FkVnawba0VvEy5nGtWH0r+00W+sSFp1W+wUniMJPvo+ebQtqOL3zWJoPzYngH5QEdF90le26S0PDpeNcr3IIDoV9hlV/idB4hqe70jnxzmN6GN9Xf0dRvTGKJYk4b/v78inWCfzOrJu0TGvqm9nXd6tNee8pldOuRX8a5m3oihWRVESDvwNzAC2x1LiD+B8YPu3M0QNGjRo+GaIonTrcSriWDIsX1AUpQTRw6uBG761UWo4bdFglESUH9xqpOIfHgyxFqKTboJ//DeDXu+Ig3JCbx3u11q46EJhn4rVzPv/tTNlqhRjam6IUr0hkXa9nG9reRZnXJaJcftO2V+n8PjmXObFCRvd+xIUzpJ9OxtMbN2SyrAcYaJBrzyflCzn/mWOm84GEzvM8nW54RIf7S8a6fe34QCEV63HtyRMZ60w7erGJDJsXvoVC7P3dxhRo3BZP3F4vr4rH+vWVEry5XqhF17mojFBOmtiqf7XjWX7H/YyNU7uXaeorAsk0xGjWtcM7aDpUwsjTHJ+53I/M54/h/Y/vQZATXUy4X06Rv66HwB9XHvpXNpE4z5Jn/eHDDTrjeTG0vM/yNezdWMKNQYZv3Otj5VGH0/sk+2yYCKfvS3XvuzPmazZ7+NHqjiWLb/+C6tL7iaCvC95NgPBqI4ZeeI8zZh/L40X/p7UsXJvN+6sZ0dFJqkWubfM/E662kwkDpe5NW1X6X+N6cgflm+A09nUP5YMyx9+KyPSoEGDhuOE0zlU8IT2sNQ0bw2HI32qlCF9eWEil4yrI9IhH5G3duQzI7uRtJ+OBaDhgU2YrSFSrhMe0bVgM9U7UygcJBr5ltIshg1vYuvmz4tL/XBuG++8IdrszIkNVK21k54lempiiZ64e/4GwK7xd5IzoINlW0ST9upgSpKTgE+4TfbwLjr2Gg52f3nOZOA2o4ecscI+FZMOQ988OpbsA8BZm8CiqJ3BAbmXM4bVo7OAu1LuNRTSs99lx6JIurxFH8FqCdLpk9d7DWzF7zbwVpMok3OzGmmoT6Swv2jqBhvoLDo+XSmFrCbcnYF/WRmmItHkw42dGPKS2PisjH/kHDer3k5l3DDRxNtrLWRM0tPysejYng4LnX4Tw56dDEDg2dcJt0UJxxJvbONScK0Qlv9hQzYXznKweMnnpXxb3FZCsTT0PgWt2EuMNK4SM+HTjlTOvybAL16S10eHTJxTWM+KqlwAdpqijPLD6F7C1O3j7Tz7qoQR/qT22JN0FmT/oFtrzsWNL51y2okW561Bg4bTFt975q0oShLwNDAIkZGuBSo4ygbEGvPWcDhe7JDoj5+f56J0YTx+VbTR/gVOPq7Npo8iunPRpHZMQ3qiZAuzLrurgr4T2qjbFEv2CBjpO66Nuz6T8/1lqpuIO8QjnwnD+1FaE6Z4YbrxRUb+sDyV8QG51swrvax+3swLFkmiebTQTXN1An1uktKkrkU1JF9axMb7heX3K3ZSVpFFYqx5wgpjPEP9YSb9LpYe/0AjA3+WzJqHJBLGrTMwbXw9jh0SgZE1Ksi+1VZKw8Lkp+U1EI0obKsVNjv5d8k0/msv1U5h0jn2TgoePJOPbpQ2Zi9bgtylDx5MfzdmmKhebqEsKJr2tD51WLI/J5LmWWNofuQzmh0yV3k93XS5TVTHomecegNn5TdgjJOlrq0+nh6X21nzLzl+2JBGoiE5XzSsYJ+WhXORRKKsdmYy58z6g9fS2U0YRw0k8MEWAIItUUq3ZzNyvDDr9ioTmxwZNMf6bdqiMKtfLQ2VMvaiqyxEXWIdJTy+9JjZ8Cs53WPelzecesy7u3HefwOWqaraD9G/y4G7gOWqqhYBy2PbGjSc1DhQaU/D9wOnc7RJd5oxJAKlQC/1kJ0VRakAJh3SSWelqqrFX3UujXlrOBx+ryymGX08RAIKSqzYkqXARNNa/cESsQ37EylTrFx+q+zvfqsaR30C2b1Ei9WbYHlZHufeIl9Ez/Ja4ovjaFgp23lXpuJ6WzTfrdWZ9El243BLKv0ug4XUcJTJE+V1X71CQomF994UJj0stYX6FjvR2Ke3X7GTPbvTGHa1MNXyF1Vye7oPpqv3nhsl4uzisxXCpIsLnBjMURKmiIbtWdlI/IA4/rRUmO/cQBCLIUx5RI4//6dG9j7ZQsFMiX1WI1HeX5zG2B7CXoNeA41OOya9WBL9zvMR6QxiGt0XgOZn9vJIewqFUZmrHxTVEvbpKN0rhbfGT3Og+qOYpw0GwLuoDOvFI4jWyfk/eFLhjGHCpo0ZBsJtn+cg/qMsj5+U1GHMl7l7bVEKVzxcxA/u/AyAXwSNpCV1kf/nafK+/eYD0kaEeWKl3PvPHu7HG7ftog9iUSUnejHHhUn/cX8ASu9pIDVRXive9e4xr6ov5lzZrTXnyoZj19dPNLrDvAuRyoH/VRRli6IoT8fivbvVgFiDhpMJW6u1j2l3Ycw49V1iUaV7j1MR3WHeI4H1wARVVTcoivI3oAO4tTsNiGO1UK4H+FNO/xGXp2hV3TR8DoNZ2GPiYAV9ViKEhOUpZhOuD1sJdAl7zJwZj/H6X7Jt8oMARKI6BlwSovIN4R9FPzBwzcth/pYbK7uaD7/cIJr1Yw8O5r3bdjH1ImmW4CvvYtXOPKaNF3a5fG0uHp3CeLvouLkXJBDc7cQ0UNiib0MDr2/LJycsYx2U2cJv3Fae+X0vAGoe3MX6rhSm5giXsSSFsd10NutuEo06Xh8mN7f9YPTIBKWT/+gsPHCxWBWrnrcQVcBhEA0+ORzlnDssND0v5SQWdmRw3ZBa4n98HgDVd6wAYFlALIPrrw4S3t/KfauE6U/zRcmK99LjTGGwNaviKTwnhBqS8Ze/m4BRH8EdlOgWuzFIkl3irnN/OZTKe8rJGyl6+tI1eaRGwvTLlkiX+qZEcrPaD+rnHREjQwc1ET9SMmXVLj+h+q6D72+4A2wXl+B8agcAO+rTmDDDSahF3ufNm7MZMbqRcKxlm/3XF9Fx/0J5z1euPOZl9b+53WPex6P87IlGd5h3HVCnquqG2PZCYDixBsQAX9WAWFXVp1RVHamq6kht4dZwovHYg4O/sO0r7/qSPTXk/vKL6RxL15z639fTmXl/7eKtqmoTUKsoygE9eyqwE1gMXB177mrgrW9lhBo0aNDwDRHt5uNYoShKiqIoHyiKsif2/xGrrCqKskxRFLeiKO8c9vyziqLsUxSlNPYo+dprdjNUsAQJFTQBVcA1yML/GtCDWANiVVXbvuo8msNSw+GodIjpP3pKMxtXZJJskMSXp40GzvUbGDtYnIgby3LIj/PQc6JIAaGWMHqbwoZYIao9JgODgwEKskUaaXYk4Ivo6d9fCiLFFZspf9uK2Sjmejiio3CoRLbGTY05+p6rBqClxcag+4ppfEhqifu9Rjb7kplvkP1fHOcl0KxSWxkrVGUKo1NUCn4l59l2dw0RVSEpXkIP3wwnMTnopwEpfHXum3MIzX8RtVNef/LDTC6wOqlsle/7gHwn9qIoxiLR530bGmmosKPTydcnFNbT7/4hkC4yTNfDr+JrM1BbJ+PJy3Xj7TBT8IpUb468u5B7no7wh1skVLHi7630nu6lq0LmIi4XKtckskonhbh+fLYDNRzFXy1JPB/vyuOMIpGYEq8bi2/ROnQW4X2LV+cyQPHQd7bILo2rdAT8BnqMkXC/jr06/F1GKQwGOHZY+UPAxFOTYrLMB1mMTmnB2SYO0LaIib6pMs99dr53zJz4X3ndk01uqDs22URRlAeBNlVVH1AU5S4gWVXVXx1hv6lAPHCDqqrnHvL8s0hxv4XdvWa3PBKqqpYCI4/wktaAWIMGDSctTmD/4bnApNjfzwErgf9v8VZVdbmiKJMOf/6b4NR3J2s4pXEg0aVxUxw2XQhbvDDvnxHgs2AKn5XFnHyTmli/MpOm94Whjbu4g9p3FYqzxJE2xBpm9940rOlyvuKBburWxWObKE48/+ZmCvoGSDhfijWtvs9NcLM4NNdu8zEoECbVEOujONdPaO0WXoklEF2Z2sy03DoyK4QJ127UYU/24QmJMzWnRzv2sQkE3pPElIK+EA7oiMsSB+EPa/1sq80gOVb2/9mL32Y0OnpPEYM9hEp8UpAcr+jxif1hw/uZpK8Rtropmk98FGaOkcJWdVvsqG2tvHanpONfdHkSkU3tWAxy/oaGRIZ/chefjhfn7lajhTuHNLD3CUkK2huyY10bwCT+SsJVUaxxQfp2ynhDDQE66kyklMjy0HO3l4ZYUav1v9rP5LMNtG4W52phxE/f2V18ukgsKK+iI10XwDROrJB1qz04DApXp4lLbE9HIveltR08flKvBhLG2ml/Va6dShC///jF4p/ARgvHI/ruXkVR/pdY3oyqxqp/fQm6m6SjQYMGDacc1G4+Dm0aE3tcf/i5FEX5UFGU7Ud4zP3CNUWLPlqJ+NdAP2AUkMIRWPvh6Bbz/pL0+LM5yu7xGjQcjqJRwpyDLoWe83J55a/CNoeoXZw3tZGoXxjZf9flcUmvBkI+YWxvvpnJRXfE4Xi+GgD7hCQ6qwwYU4SPmCeX8PYnDm6uFjLk3GMlGNRjXFEOwNAiHY/UiV7+P3oXqb26SLhiGAD//JOTK3vXcoFJtNe/tKUyqdHA4GRx6bS2x2P2hRg2S16fuizMh4UeTIPFSij/VxCbOci6WmGrE0xuRvaXxByAsTmp7H4Kyt4Tjftnt+pR7P3p+Lvcy0cfZHLWmQ146+ReeleZGfdgL1bdKTyyb0Ybb9/XzuRs6dC+7KVMRmV3sULK7nPLH7P4dPyDDJknunNkkYLXYSCjQMIoM1TRmwMemUtjXITcISGSKqRP5bIt+cyZ04LhbFFFl6/czjnIsQoQ9USod8rYN5ksfPKhhdtukbXKtUTme+P9cu6pJQ6Wbcsn6hH9vG+Sm6y5dtY8La8Pz2ukcoFK8VWxol1VLfhqj/BB+YbobiSJqqpPAU99zT7Tvuw1RVGaFUXJPiRp8YjRd19x7gOsPaAoyn+BX3zdMceSHg/SPb4k9tAWbg0aNJxUOFHRJhxj9N0hYdcKMI9uNLc5lvT4uzmsHdrXQYs20XA4DkSb9M1uRW+M4m6RBgHrQ4mMM7npdYPozmooxKP/VpkZFTbZ77ZUqp9sxpYsERuKAopOZWe1aNyDi5pZWJPLAfU0Pxhh3HiJXPE367CkR4nKoZiLrLzwVgrjFGGkKWldpF8/kF33VsvYLtOx6xWVrFgCUFxGhF2fppNsFSshtYeHzmYLRotYCdbMEHU7E0mLlZ81xkWxnZnF4hjbdOkVWnUqo2JWhUenZ8YsB+XvCnNOTvSSkBpgeaxsqlFVSYxG6GGT8S0MJ3G+roPCS+TuPnnWxMAeThJL5PzuLREqatPokSzjfdWXwiVmF76YlqzXRUlK81JTJ+y5R46b1KkJBHeLZbH003ym9anjif1y/ToC/F+6WBnWnDCm3oksf1n8AyU5DpKHKfx0uejpDxS2kHDJEKofkpZrik4lZ3wA8+13ALB2xrOMutyHPk8k4YZnavm4PZ0+EXkz+vRpZfMesYhmNx97a7KHe3Qv2uSO/cccbZLKEaLvYkmON6qq+qPYfh8j8ogNaAWuU1X1PUVRVgDpiHFTGjvG81XX7I5scmh6/FDgM+Cnsdd+oijKVcAm4I6vqyqoQcN3DX+z5ub5PuFEsUVVVVs5QvSdqqqbgB8dsj3xS46fcrTXPJb0+MeBFmR+/ghkq6p67RGO19LjNXwpUoaIjlu71opeF6Xn1RKrrJ86i677niLgimnYqVGIgiFF+IZxaCHuN/ZyX4Mw7bv7N/HJllz6xAvbNBii7G+30zvGGFP6BzCVSDPjyP5mIm0B1HCs8cPGfMYntPKOV1K8Zxpd+IMG+l4qY2xf00nKHy+h/Z75AFiyFTzVCgkDZGw//yiJx29Lxr2wEgBFB/XViZiNwqwzCjrYXZ5O/2HiHjJmGHjmo2ym6qSoVlKaF2+HmZyzZC58lQHWlecysqdo0GXVGRiAfUa5dwMwq6Cef9dKA+JbhtQR7lT4cLd8t6YV1+JxWsg4U8b39DupXJTZyN79YuUMGe+gZVccGSUSzKBPt+Ld1nkwri5hdi8i+0SC9VV46WoxkfWg+OTa711I7d5kEmPp9BlDArzzcS5zLxDW7in1sXlPFgvjJOrnTqOP9i4LXRFh/YP6NxMO6Phgv4z90vvyiNbWse0Jid8fMNtD1fsSD19Ss/iYmfcDPbvHvO+q+R6lx6uq2qyqakRV1Sjwb2D0kQ7W0uM1nEyI7G/+rodwysBX4f2uh3DM6G60yamI7vSwbFIUpVZRlGJVVSuIpccf8KzGdtO6x2v4Rgg2CTvtOS2Ac51CtE2Yc+iJ/xA3qTeBNyWWWQ3Cvu0p9B4pDE/f5MTfaWSmT/iHu8bCmeMkC1DRKbgrjYweIQu1oSCFV15IZ+wmF2Ci1Wth1C0mok1yrvEJraQWeumzRTTgvJlhNi2yothEQ55fn8C1/5qPWeR3Ktck0v+WRFyLJCzi4VEtNDzdSVqsNIhpVB8aH3FQFpBok4FVCm9bjPTplLHu3JLCjy5xo5jk/J6NEVLPTWTtP8V1lm3tYuq8NlrXy+t9U120uK1cdV8hAPUPbMZ+VhqT/yPMuX6bnTafhQv+LRnVDb9xkn1tD5Y/KJLpjQ/1JFLayeByCYBw7LDyUDAe1tn4Y14rtnRw1ojevs2fiK4iwLThXvQWWPtZDvnmLqqu+hgAuyGZUFRHk0v2V7cqxEej7HpTokX6jPdyRm8HI/fHMln9eho6Eig1C/MeGqei6CMHF8x992yj02dmwHkS467PSGJxzFPxtfnh3UD0lF2avx7dTdK5FXhJUZRD0+Mf07rHazjZoOi+aP0aClK+sN3qtZzI4ZzU+GNe6xe2t/kTv7C99rOcEzmcbwWncxu0Y0mP17rHa9Cg4aTG6cu7tfR4Dd8xrGeJH+SfTyucEfaSM0zSql/4XR2X+6vYWCXsrzjBTV6hm483SPjapBQH93Um8efREv6nj9fRtMVKzmSRYZ7blMaIj6IMypQa3VfcYmPlozaqYlJF32X1rNkn555xTjPb302nwiyyxugdQcxKFOcicXb2DqZTs9FOfLw44XoUu6h4QqX3JGH5697PIs3oJz0WKtjychWhSCKmWDBA3oB24ncm0LRfwunetOjptcFLebXoMHa9nkGTVCb8ThymTU+52LIokfxMKbKVc00uux/y4nxsIwBlbZno33FSkC+88sOGbHwm6PeQFKqrcWaQsGQ30+4fI+O7Yw9mxchefT4AZ6Y3c0NXhITRIn2Emz28Es3ihza5X3uaD3P/JHrvEglrmzeJkF7udcQkJ/pkC4pJlg79lOnkrlxF+Ssyd5379DzjzOJXv5dQwPl/cJCgU0mVqSEahLcq8pndUyQuc2oU7xYji9+WuZg1qpYf5R6/iKDvPfPWoEGDhlMRYeX05d7dCRUsRrrEH0Av4H+B59G6x2s4Rng6xdFVeE6IpW+lMjaW8m3LDdGyJ569LtFhJ90eR/vb1ZgShUvF/2gmwcUfohiEpQX3B3ihPJ/r/0ccZXue89JzdAdht3zkWmus9PztEFbcWQNAghKmqI+wcp0xitEOoXZhl879NuLiQ+j0cq1oRIcpLsyBr0rQa8CWFuDT3ZIOPyizhYfbU5juF1Y//Q4L1U82k3umOBQNfXPZ/U83ObF+m+3NFpLzfKzYLlZHVIHZs50YJ46Q83+wkbs+TuU2ozgc/QEjnUETK8wmAG7pW0dbbTxer2z7QgY2GOM4zyahiFtcaaRHg7To5PXhaU6iEYXsC2Uun3rORHEgzIRZsftPsFD6moV0u0SX5M7Soxh0eLcI82/dbz2Y4JOc4iV9lp3wfvmq7/ooia6IkTE/kNDB5vcDtLdb8MWKdpXcbKbt7SaaG4XlFw5zYRnbkz1Pybn73tkDPF0seVSsmhkzmnB+KpzyeJSE/W3BFd1ac+6tfvn0CxVUVbXiQAo8MALwAm+idY/XoEHDSY4TmB5/wnG0sslUYK+qqjWxSlqTYs9/af1aDRq+Cn1ukrKszldrCSsKSQOFKAUaIO9CG6Z3JCLio7+mMfnuQYTW7gSg47F3CXQasGYLu+1yGrnxmTMIvrAAgMVkcUdJEvpqSXRp2x5HD2cLZ10ladjN73oxp8vXtr3KREpelC3rZSzVRiMTzS34vGIV9P5tPzb8pobBIyXULjnfSNmiNOJVOd6SEOb+ce2sXCShhr/4h4cHLzKgyxJmvuRhH4NsIaxjJPLF914nQY+etKhYCRstRla9nUruMrk3gz6eoRETOZOFjRIO0LU3yrDRkpLu+sRA3oU2fJtkPHq7QtYu08H0/HOmuOgq89JWL+Vzqx1J2I1BSp8WNnzDb+ys+1Mru98XNlyqWLlwTC2GDLnf4J4u1DB8ul18AhPPbcFXJaF8XS1m1HYv7h1iZQy6IkK40Y2/XK5d7cxiUP9mdu+SuYw2tFJam8Gky8SKqF9qRVfpoCMg11YSE9lxfx1TRgqT37M8mX6XHL/lVAsV/ByXAa/E/ta6x2vQoOGkxum7dHezDRpALMa7ARioqmqzoihurXu8hmNFcj9hl8vW5DI6pQVTnLDRpAnxzF+YyOWXSTGmHS/rqSKO0Smi09rSAvg7DCTFEmPaNisYLRH21Qi7HXaNimI00rpUdOCVjdmYo/JZn1pSR+MuO8kZseYHoyR6on2dsMMutwmDIcqGVomAOGdCPR17DbQ6hcn2v3cAH/1yHymxRhKDfgi7XoqSmirni0Z0hMM60nqLhuxr0bOpJoszx0iEhTHfyoqFSUy7VbjTysdC9LK3U9chbLRkSBOlZVk8ZREr4fnf92bvfbvJKJS56Gi0kDY0yC/XCbv9U4GTuN5GvHuk7Orj+3P41bxO6pcKG17iT8GqwrCIjOdZk4F7iprZtU3uz2YK0femJCKVErmDQYcv8uTpAAAgAElEQVRx9GDqH94m85Pr58Md8r1tMCh0KiqDYkZBhQluf2IUbX9aDMCmfVmM6tVI9T55H5JtPuKsIaIRkZRThoQxnz0G3+trAfA26elssxzU1I2GKNU+sTDOazr2wlS3F1zWrQXur9XzTz/N+xDMBDarqnogv1jrHq/hlMPUkrrveggaTiC+1+nxh+ByPpdM4PP6tQ+gdY/X8A0RapOvToteIfMcC48siJUanR9lor2NspekgcFqUxyzde00tAg7zQHUKBj6SvnQzMEmwuU1RKqFQP3yFR0P/zKNjEdmAzD1zv/S0WLBWWml4PFZ9FrzCYGtEse8cmEiJfkOdu0XJlqc38LqhmyGmiU6JOiE9CsLSesQZv7339fSEadjWqwxRPvqVkwGKxnnxgxRnULnKgfxN58PwKfXrmfmxzfz+LQnAfhxhoNecZ24FgtTHz0kQnu9hfxEYdYfb8ulh9HLE30lIqPp79vIKg4e9Kz9w5vMtBUKv8kSK8SYqLLhnZSDTCzdrGPbqyayM+T+bro9Fddz24iEZG4uqk0jflJPchqFh7W54olUNxHpEOYeaIa3ljQB6UxOaCGuJIU+O4W1n/cDHd6NLazdIfH2t83toOXutwkHZS4ydX4+3pfDrItl7pYuSGb6gDrK1ksBsQ9WmbF/tJ/Z50mm65YNKayywB2xIlxVlam06PVf9ZE5KpyqzsjuoFvMW1EUKzAdeOOQpx8ApiuKsgeYFtvWoOGkRsHjs77rIZwymJzQ8l0P4ZihdvPfqYhua97HA1qct4bD0RRj0nm5bswJYfQSmoz1tvOp/cUyAHo8/UMab36WlCHCo9rKdNzXnsSAqERHDA0E6VQMjO0nmq0xWcWxw0rmUGGLnmo9Keek8tfnRVftVKL89iwHLZ+J4WlNCbCzIoO8FGGquZenEvisDr9TuE0kpOON5myGB0WDtplCBMJ6bBYRfrP7d/JqWT6psRKzk/rV8XplPvPyReO29tYR9UdoK5fxbmxL4+xRdWxZL1bDqDkuFKuZrs3CvHfvTmfQGAfRmK58T3kWFnT8Zoyw03+uzyUrDHNHS2GsFevzKEmWhfZA7LdOF8vuHOdl38cJ9BgsLF5nUXjm/7V33nFWFef/f89te+/23htL70WaooLYsKAgFizRGI0lJuarJtHkl0SNGmN6UWPsXewNAcWCCEjvfWEX2N773nv3lvn98RxwuS54lyIuns/rNa97ynPmzMw5Z+7neeZ5ZlZk06tD2jKFDgaOrMGRLWVrWBXE67aRcZqw9KdnJ3FZfilbt4lW0regjuqyGPpeKmXb+LKVxLh2VjUZ0+lOLMcxrh/tc7cCULo1Dq0Vi7U852mZ5azZncawTBmLiB8OVUttBAPS1nGpbtxN8pwKNnx02Hbon+ZfFlaf8/CuV3uczduMsDTxnUbuk/tPoVO/fn9lcbi3Y799e8L+32rrrv1V8BZ1/CrSezvuvcg+cf8pXS3O/funFPZvu4ZV+7fN07OTjmDpjg1MV0ETJkyY6IEIHMed9+GEx8fTzdXjTbOJiQMhaUQAX62f5aslsKXMbuPymS1sN968T1UMg7x+kgz3PLs1QGNHBJscwjbHB9voNaSeqDtk+RvvU6/TuM1GeZVMBtV/VC3Pb8jhglh5XdMvTSZYLgFAvio3vkZ4fYdM3HTNhfVYB/dmx0O7AFgSiGOHNcBvTxSzxdbPE+g9uA6LMbusPdPJ6ndiKciR+cEjYvxYneyry4QpNcyZn06eFrNLcnwbHrcdZRDhsvYoTr7aS/U8GcBsbHThsAfQxso2C/xxnKxaeMq44f9FNrKjLmHfmpa5F0fg21HP+0vFm+vMXmW8vTuLUYZrYH6feqr3xODuEHNEQCtG/ms4ulECY/wrNhFo7EA5RKu5c3ESD02o48kvJEjnJ3dEE6yUdit8yY/dFsDTIbxvwExN5RwPaZNFw1ExLhY+bSfdIeHyA36VxfL7a4iyiQtolGFqWuYRr+L+QTcWpbFZhPXHRHv3aRAj97x72KaMH+dfElaf88Su148/s4mxAMMIAKWUFShDwuOvRVaPD3sBYhMmTJj4NtFTByPDweGExx+N8pj4niE2W8LbGzfZCQYcZDqFLQ5Lb6X6Qxu9xgtTS18cw8QbgrzwrKwuP2NACbkWsK8zBtImNmIflkf7f4Sq+5oUzzek0hEhH699bSItNk2LMRGWnlXLvBa5drxuJbugkdEGU/3L7ATuTKwhLV+YrX1HHL8eWs62z8UGHAhaiDpvANv/IQHGGa1NZCYqqitlUK7v4Bbq11qYeLN8I7o1mikTyrCliNvjqjdj6J1bz9JSGbD80hXgpMY20mZI/ouesDLOUUd8hpTnB+mtVKx0kdUh+VvtQYYWVLOtSIJ0oj+pIyJGc9EVwtyr5jq4uF8JvnZhw7YozeeBOK67XZi7Z3Ehm25fzRyrBB3l+lOptUKrMQPfmR2w4JN0+hiOdlv/WsHAB4cA0OfcVTw8NxmMoYTo9+pJyPSy6S0JyomLcjN6SC2P7RAtwPLXUgYPa8JvrHikLJpPdmQzxikDqK8F40gIKi7NksHm3bsSKFbSTiMP9NJ0A8fvCEf3gnRg//B4kNXj1yulnlZKfS26EiTCUim1Uim18pV6M0DChAkT3x5MV0G6DI9PI4zV4zvDtHmbCEXiIAkMadhqI/1vM3jg+oUAeNDcnlnJziJho6WWCM6/oA5/jdiNlyzKYOywctrrDPe/RifVbZFkJQhbTh3spmJtFFvbZBrUfpHN+AMW+v5YbOCW9FS23ServeeNbSbYrmkpE1vreRXNLBzrZMkKsflOmtZA7ZcB6uqEqQ797FcEi1bjfV7CHqpWRZA23EP1emH1NlsQW0QAR6SEp6/alsHoQeVEnyI2cN3upmlxCxUlUpa8gQ343RbsUcITK7bHEhXtxe8XbhUV30HidSPYcu9OAFLSW4lM91O1WQKakvNbiZqcx+6nxYYfGdNBdEYH5Vsk/4JLbNR/1syb1XL/68+tpXGVj/gTpO12znWQ1a+JVevl/OhRFbTX2IiIlvJHn5nL4n+LBuQgwLCTqtFiwqa2MJKsc+2Uz5XzceluAl4LVod86q/vzGbmkJJ99v9h+dXEDrGy/UNpS5s1SJ+ZNlq/lPGCosIkBk2S7fhXPjts9f6a/Blh9TnP7Xqzx5kSumM22S88vlOYPEqpJ4DZR7hsJkyYMHFYCHyLcSzfNrrDvGcBH2qtnzH2960er5S6DRintZ55sDxM5m0iFOk3ybJn8+6rp8hh4eYb5Hjx0424In3srhHmHGn10xG0ogwV1wLstDo5JVk4RFFlIiecUUOrkGmCAUXCWAfBFsM7ZWQBRIq9vOOLDVSucO5bMk17/Kyal0S/fAl0iUwP4GtStFQLk25pdVLaEcnknwlT9a0vwd8cxFMvhl9XaoDda+Mp8gsTLrC1MvDu3hQ/uA2A/Nt74Z6zgac2ijfL9SNKqNkWSfqJonUom4W65RpXnOzb4zS718aTM0Dswh2tVoJ+RXmZtMWAM5tQNguVS+T+c9uS6e/1M/YkscFXro8mKbeVhVvF7jwuqxKlwN8hTD71/HgWPGkjyyk28vgEN/V1kfSZIh4izev9RBdodnwh99vjj6TdItc6tea8J8fgfV5mw7AmRLD+nWhG3CRt411Tzi/WpXCxW3jhmGEVuBtsNNVJ2ze6IxhxcTvYpOzWjGQ++YeHk8aIzfvR9dmM8IoGciQmproib3pYfc7Lu98+Ppl3p/D4zivE/9lcPd5ET0PHFxuOdRFMfIvoqfbscGCGx5s4pkgaIey3fr2FWY1p3Hq1MGXv+irsGS68xcIGZ2/M4YLRJTgGiYdFyRtuIpx+ksdLPtofpG1bAJvL8Bf+0al8fnshAOMnV2Hvn87fnhH2eNt59Sx4K37fBEhTh5Xga7FQWiwTS+UNqGf31kRWWMQuW9Dhp8Vi5ayzxM971icZTIys51O3eFhMia8m44GzaDIWAI4aEc3a15x4tOQ/qKCGhqpIel0t3iLFz7dQ8PNs1jwoTD+AYtQ1AUreEnu+1RYkZZCHYId8Ls5hqfjLG2gvlLbaUiheMk6L7A86tZ5ZS7KYkiRaSMrUBIpedNP3zR8D8Mb5rzH1zAq2zBcmPXiaGxw2Vr0i3if9+9biSAjSWCz7ySf4efozsVHfdF82jU8sY0+R+CMkxbfhivOxtkim77ejyYltxhkpWkNsrjy/pl2itVTXRaOAPiPFHv/XTVlcplt50vAoydA2UgOKdTa5/re5VWzYJnmfWXX4IeuX5U0Lq895dfc7xyfzNmGip2L85Kr99m87r/4YlaTn4ab7ev4UzmZ4vAkTJkz0QHyvw+Nh34Dk9Yh9ewMSXZkBzAKSgFXAD7TWHQfMBNNsYuLr2LuSjqNfCmuf0ox6qA8AdY8uZ/2uVFosYnrI0B0sjoigr2FKOHVQKdZIcOSJaUPFuKiZ00xrs6j+Re4YClwtrOkQU8FZvUvRQcXqQgmMOe0XLqpfkln5ggFF2vRE/MUSAv7wwnR+lFOONUJMMNEXDGT+g820W0SzPn1gKTqgiBwig3D+inaWLMqgd5yxOnyrk34n1u8ze5SsjyMxpQ2fR7hSdLKXsp1xlPnk+jPfmUbrvY9Sv0tMCTFJXmJPS6HkVZk/3Bnpw5XQQdkOMetk9W7i0+1ZTHt2gjRixR7KHlpDh9fIP8GDz2NjZ42YOsZOqsKa4sKSKK6DwdomAg0eFn9qtMW9qbx6Xx1n5cqgYU15NMGgos+5YsZpXutj/S6Zj3uPw8YVZ1Vhv0qmIVg0cz65Cc0oI8Bnd30c4y5o4IH54uI5ymvh7EnlLP9UTCE5sc04IgJsrhWTU7rVwxt2J3ffInUvfbp834yGMY/OPWxTxvTcqWH1OW/veb/HmU2+MUhHKZUF3AqM1loPQWKrZgIPIeHxfYAG4LqjWVATJkyY6C6C6LBST0Q4E1NlAUuB4UAz8A7wH+AlIF1r7VdKnQjco7U++2B5mczbRCgqaoQNjvixhdLXW/axx5p2F0OGVPHGdnGvu3JSBS3bNd42cUnTQci8pS8L/iCDfkUOGyN9HvoMFPbszLXxx89TuedBcUW0Tb6CxqtuwDVc2Gvxm0FK2oW119pszLjOz+6XJMAn70dJvPNIkP4WYb59TmmifqOdViO0PjrGS31DJLn9ZWKnPdsScNgCBILChVxOH9ua45l4mrjuVa1xkn1FIpZBA+V+D32GK9GPr03kd+xIoqBXPQ1VwsR7XRnJ2085uGCq1OXVD1Io8Pk4YbKsNLjt8wQKBtdRucMIwrk2luo3q/e1nQ4qmlqd9B0jg4QR4/OZ9UiAYVpcA/ud1cKXc5JpMrSacWnV7KmMp90YYE2LcBMZ2bEvPH/j5nRyEkWryJgI776XzIUXydhB9YIAqZOsVC+QwdO2VgdKQYtHAp4GTW6AoGbzAmHaG6wuzoiv4SdNUvcn0jqoqoglb4DkZ3XCipUyWDql6vDXlZyae35Yfc77e2Yff8xba10G/BXYA1QATYiZpFHrvXFWlAJZXV1vhsebMGHiWOHbCo9XSiUqpeYrpQqN364WY89TSq1WSq1VSm1SSt3U6dwJSqkNSqkdSql/qzAmjwqHeScAbwKXAY3A68AbCNPuY8jkAHMNs8oBYTJvE6HYO+3pSy3J/HxSJQs+FNvoh04/qdrOKR7hB/3za2hriiDzDJHf8Z6dWq+TYYPEfa+p0oUjIkDCSHnFbHkp6OY2yuaIC1reyzdTdf2/960AHxvjoapeXPdcdh/VHS5224W5rrZ5+UN+DRXFwmxjYz0kDvJhiZbzz8xPY6DXz/gLhC02r/eTePUgOhZuAmDtgmQGDqxh/nbx1rjgvGpql8LqWnHxm9C7nPg7zuatWzYCUGjXnO71MmKmTNK1/JVI8lMaicsSm7NrQjb+nZU4Jp0AQGBzIfNejCRCi03eqyxMudrNjlnSVnmjm4k4pT/lj+8CIG2ShcL3HfQaK+zZMSqX+5/wc4Zb2HJBej0tTU76PinrfZbd+jaPtyeSFhQmfpa1ibyzZGyi9GMbjgg//24XDeYyT4CMlGZSJkrbfPF6LIMy6kgYKmW794sUUrWNn14hLp9V77eSMgHaNkl+mwtTKUivJ+VM0TqCTW7atxmTkS1ccNhs+Nzcc8Pqc+bsmXNY91JK/Rmo11r/SSl1F5Cgtb4zRMaB9LlepVQ0sBE4SWtdrpRajpinlwFzgH9rrece7J7hTEx1BlCsta7RWvuQdSwnAPFKqb3eKtnIVLEmTJgw8Z1BQOuw0hHAhcBzxvZzwLRQAa11h9baa+xGYPS/SqkMIFZrvVQLm36+q+tDEQ7zHgc8DYwB3MCzwErgVOBNrfUspdRjwHqt9aMHy8tk3iZCkTZTPB7cC3ezcW0q4/7UC4DH7i7nwrhqygybuE9byEpowWt4bFgsGn/AQnK62KWba13kXZvA0v8I03ZZ/Iy4M5nADvEoefetRM7sL9vObBvbP42l92hhzu5qKyuL02k1QsDPHl6Cv01hj5PXddvKZAaMrcUvtyIi1459VD8aXpZ1GhMu6c3yhxrplWUsbtBhJb6XZ9+yY7YUF4Em777Q/drKaJ5ULh76sdjQ//u/IP07AvSJEWasFGxrjufFCLFRpyknF7gVycYCBzZrkIKLLSx8QTw0TpneSKDJiyVyr81bM+fDNE5KFx/3YEBhtQdxxgkzX7sxg6F9qyjcIQFPIy91s/41J3GRwvTzL4tg+4t+MntJeWxRmrriyH3PrLoxirQEaQytFTGJHl4pFavpQK+fCDSjp8hYhK/ajy3Rir9eWP6y1ZlM/KGXjq1ij3cMSOKNFyPpHZB7p8S2k9JX6p30weeHzbzPypkSVp/zUcm8w2XejVrreGNbAQ1790PkcoAPgD7AL7XWjyilRgN/0lqfYcicAtyptT7/YPcMZzGGZUqpN4DVgB9YAzxuFGCWUup+49hT4VfVhIljA2e2GdrwfUK4niRKqRuAGzodelxr/XiIzMdAeheX/7/OO1prrfb6ToZAa10CDFNKZQLvGH3rISGsN1lrfTdwd8jhImDsod7YhAmA4qeE/b4byOb/bgzQ/tYyAE63xJLxu5NJeP1TAJTDwr2LU5ngFTvs2TMa2PBmBHv2yLhQWlIrKiaa/vnbAfhvVTrNDzYxOFe01DGxdQR9FtqLgyiLZiNR9I0WZhk/PJWmpyycN0qYefXWKGKTPbyyTqaE7R/045pxErPv3APASe0VrJ3TwPjRwib/+O82opwRtJcJkz35gno6SmHdQrFxb3VEcFZ8NQvrxVe6d8DDTZFtoMVzJs+nGTe4nDlbxbPmwvGlVC6K4umpokX4q5qYtziLITmyun3cacnsntVOorEkXKDJy4LPMvAbY1zjcys4/+ImCt8W+35cnIfUGanoNmHu4zJr2PJJAruswvzzl9Qz8lcZvPo3Of/KK0GujWmltTYCv9+KI8KP1VimLKG3h5zTstn2TylbcUcUEzLKOM9paDHYyRnYyBbDz3vQOS34Kjv42WZ5Tn/Jr6LhU4XFmJhK2eoZH+XZN0nYP+cl498ksn/4hncnHIQ7/YfRUT/+DTJnHOicUqpq72R9hhmk+hvyKldKbQROARYjpue9CMsM3d3FGEyY6NFQFtNyFy78fuuxLsJh41v0834PuMbYvgZ4N1RAKZWtlEzqYjiCnAxsM2ZnbVZKjTdMLld3df3X8jMnpjJxLJF8krDFuqVBUs6MpPA1Ob7DF83YjCp8HsP3eLIV7fbxiuGNctU1XoiOxLt8FwCPrM7m5gGlzN8gBGban/Npe2kRANXbo7Hag7xqTCR1+4wW9rwbwOsTxbPe62TclW5KZ4tN2BHh5/P6VKafLOTHW6Wxx2nsOcJkC9+14/HbiDEW0+39q3wqHtlOsxHd6XL6SO7dhj1V8t/0UTyDJjdw72Jh3j+2tuCM9O37I2lriUBrRUqO+Jk/szubS2NqSDtLmHHrymY2bUsjP0WmiPV1WMm9vS8P/En8wH85roJtXyQw5Dq5n0pP5YmHGrlquGgS81bl0G6BK++U8YNF99cztG8Vfy6VtvxZTD0x6aKhRE7O52+P+bmxQNx6HyhO5xfJtRSVSNsNHlZN0A8YXkIfFmbzprWRl2+U86v+46Zvn1ribz5Z2vbhxbQ2RGC3C7OOy/Uyb00O4xOl7HFZHmqKoihvFs+fMdMa2fy+tPP48rcO2+Z9atbpYfU5C8s+OVybdxLwGpAL7AYu1VrXG/bsm7TW1yulzgT+hkSqK+DhvaYZQ+5ZwAXMBX6mv6FzDndK2K7C4x8DJiJ+3wA/1FqvDb+6JkwcfVRvj95v//YZLceoJN99RE7O32//geKuzLs9C98WW9Ra1yFr/IYeX4n0nWit5wPDDnD9SuCgrtah+MbOu1N4/CCttVsp9RoSHg8yWnrIBncTJkyYOJroqaHv4SDcoXcb4FJK+YBIZC1LEyYOG2/OlkG9mS9N5svL5jHuZxJWvfwxsDsDJIwx7K5Bjbuog2hjOfA9r7XR1BYgIVZc2H4yrJQ96+K58B4xTTz6m90M9GaQHy1MOybRwy0JpRS/LaaDInccfaNEaRx7bhOW7BzSR4ovn785SGStZvNCMQUMOb+FB+cnc8ZSUf2DQHpkO3mTxb1t6x/30O+KKBpeksJpDTvWJdFnqLjDxbq8rPo4hbsGSbj89i0p9M1qo75MzAM722MZmV2FxS4dzXCPn6g8L4E6MeNEpEBebeO+yZ9a2x08/0AddxrueCvfS2H0tCb8e0T+v887GOvt+GpObRv85OObWDXxHwC4VQRxF/fj3i27AFCuCHSLZsE/xQx0y2j5vO8eKgFQS1Zk0i9BTDZbNqSyxxJBldFzpADnBuPxLNkNwEfOLEZfPpjPjLnUs11R1LudJEfJYOiSNalMSKmiqVHcHJs2O1mhY7jyanEP3PWmnaG37K8tHQ6O5877kMLjtdYfGacfMFaP/4dSKqKr683weBMmTBwraK3DSj0RhxMe/wlQCTgQF5udWuuDeveYA5YmQrGrWuIYUiLdtHvtDH9Lphr1vfgcy1+P2Sf3nktx39kN+/ZXvBNPXlIjWb8ZDcDHvyhi8nV+VhnRBgrNOruTc+PFY8ti1TQ2uEjPE3e7e/ekEoWw+hEdVs7sX0JLlQw4+nxWEjLaiZkkNt9VjwcotEUw8xoZ1JvztAOnDpLnlECV7d5YzrkZ6t4Tpho3EIKeABan5O8pCdBaG0HSYGG2lWtclDdFU2MRV8GzTi7DMTiDzU8Iky8YUU/AA/fslAHF+4ZWsXlFCqnxwk6zpkfys1fgt9Fy/7LaWPoU1BJ/sUynqxISuP/eUjICws1m5JXhylM8tkhcH3+YV8r7RdlEGlrM9N8lQHMrn/1d7j96UDnbt6SwwS7tUdDhJ8kmdR/001jefDhArl/2ndYANkuQZp9oTAEUvVMb9q1wFKjvwN4rjuUvCLersdjJU26GGxOGrfv1dvoMraNiu2hEvS6171tRaGDh4YWsA4zNnBhWn7O8/PADgr5tHGp4/Ela6wot8ALPYPp8mzBh4juGoA6GlXoiDic8/g3DIV0B/wA8Wuu7DpaXybxNhGKvi9p7W3K47PI2Fj0vDO3U38RDRwdb/y12434zLWivD9tZkwGo/v0cKipjiYqQYJGMAc1sXpNK/wHighZ9ajrzH7cwMlOYtw4q1lamcOIAsefa4zQbl4l9fMiJ1exZHUdcvNhll9akEhHUTPmDMN+6J9bjjPfz/DYJopngb2cHkfRBpkxNjGvH7giwu/KraOjU6HYSs4QpRw1yYu2VwaK/S/5jTq4k0KqxGBHn9jQX/joP2xeLjX3YfQX4lq6neb3YsFftTmfi6ZVs+ETOpyW0sqs+ntHjxYa+fGkGY8ZW8OlKcZPsH9HMFm8sva1y/5hoLwGfhcVeCX6ZklFB3DAL7p3Sdu5GO0njrVQtlM8zY0YcjR/X0lwrzDtjrGdfaH/sMBvuwg4iB4nNun2zm/qySJa2S9nOH1SCNRqWL5FpXU+93cX/HvFxUbJoJVEpPtatTadViVYy6ZxqPMV+oieJ/JtPWJmcLc8oe9mnh82GR2WcHFafs7piUY9j3ocTHj9XKZWC+CuuBW46cC4mTJgw8e2jp9qzw8HhhMdPPvLFMfF9Q/Q1Eswxc9duvKvq+dgldtkJO0v55LVYTp8mzFy3KVrWetArZIX2sooE8nvVs7NIwrB7nz2ALRva6G94ZPz3WRsZFojtJex19aI0xhRU4vdaiIgPsHV5MkHju166JAOfsjBphDDVM8pL+WhTDo3PStiCIzpI6bZ4fjTOGHAPQr9gHY+sFqY7sNHF2TMaqH1b7jXgQg+z3kvitFrZj7Io3IuKsBrTYliirQQ9fnFbAWoWB0k9NxG9WPY7Fq5h2bwU+mWI1lHgamHBJ+mceqIEDdVudeFUAayJYjMf0b+SbSuTmTxKyqf90LjOyZdKvDYmtGmesEbQy3DcWV2SRnqZhyHXyoGoYJBAZRPb6yS83zermcyTFAmXGFP0BzXbPhavnZGZlbj6RvC/2dLu4zo8bLI7OSNONJ6o8wfy2p9b2eGSyo1fv5tMXwbOWKMtrj6VId4vWL5VmLZ93GC2fFjC0MliwZ12bjVL3xON6Egsf3w8e5uYs/SY+F4hIj5wrItg4lvEkVho4buKcCMsfw78GDGRPKG1/qdSKhF4FcgHdiHhoA0HzMSEiS7w3K/FP/jKqXV01CkStDCwjp0tpFscNK8TDw1vq415jVlce7ewshW/rWTED0cw548y7enIqlomRrpprRE77UBvgHEjymkvF3ZZkF5PzBAbYKN1k4/MtGZSbxwAwMJ7aph0TjWBRmGHOgiz7a1EFIsWcNYFtezZEkVeh/g6dzQo3E0Obh4kTDfywhHMv0cz+WI539K6TGoAAB5eSURBVLC0g+mDS1izVpj25+/ZOTffw4m/FO+Zokfb2N0Ww2m/FD/vxtU1WD5sYei1YjPfPcvJhggbE6aKjdrzaiujUysJiomdLQ2J9HK2EGgUm3X1nliG/zSKQJF40tgG5rF8i4cfDpTw+H9sy2J4wMIFOcLc3c0OdtXE894z8vlnB72UqzSm3ilGeN+GWpo2AJuKcCVIm0RbhWlvXZrMkHObuShOxhJKa+I4J6WS0nIpe1pxGcOtFrLc8hwKF8SRHPTTWCk28hd/Xcz0OAdnPDkGgNZ/vUt2jpXWz0TLsEXCoPyDzunULQSPY7NJOAsQD0E67rHIOpbnK6X6AHcBn2it+yJugwcdrDRh4ruA1k2+Y12EHoO9HXdPRkAHw0o9EeEw74HAMq11O4BS6nPgImTliEmGzHPAAuDOLq43YeKAmDFA2KGKS8bT7Of6XsIOm0sdZGY2EZkpH1b8oHjGP9vCU/fKdb20JrCxkGkd4l/82EuR/OgEiThsLnbQJ6GR9jp5vZMuSKf2nSqsBZnEFYDt4htpvf3XfHmPMLxkuxdPsR+/Md2sKzVAunJy5hniIbFzbjQThpfhrpbzkdlBXL2CKId4xpT/eyun/yCSD14Upnz+LYpZj8G5faUuHYXprCtMI+kfwi6z8trYUhxHx5oiAJwOF7W1UcSsl/Is7Mjh/Kg6LH2GAxAMbMDnsbJnveR/yrgydq5MxJYodW/vsBMsqcZXafiRLynl4sQAkTNGAXDS7ypJsrZza5kw/0f7NBFd7yMaH4OmuXn3nSQSA34WPNTCsPxqbBFB5lZmQyVM719C0ZYkBpwmWsVzX2QyPNPKLoMcV1ntJDS4aDSmt931rpe+l1vJ+FI0ouYaJwvbohiWI2rDjBo/aX84m+rfz5HnOtiGPTFI2x6j7fvZ+WCtRN1eHcb78004ns0m4fh5bwROUUolKaUigXOBHCDNmMoQJFgn7SiV0YSJsNFc7NhvP+mC/SdXsl1847dZnO80Bk1z77c/LMRcMd1YeagnI6h1WKknIpzw+C3AQ8BHwDzELTAQIqM5wAReZni8CRMmjhW+rdXjjwW6PZ+3UuqPQCnwc2BSp5UjFmit+x/sWjNIx0QoMn4nroL/+uV2XBquv1nU553/q6fe7WToaGGDa1em87nLxk2ZEsDR0W5jRVUquUrYY/+JDVStcRKTKKHVnhY7/2tJ4pZkwxRRnoECUgNi8461+hj6ewm6uetPldx3Si22fBmUa19ahcUJdTtkkM3jtrPDE8PJg8UMYouF1pKvLI4Wq8Zi1cSdJq52uCLY+Gg7Q34iA4B3Punj3qGVfL5CXO9S6GDkVT7mviSufJNHl9JY7KTBmKwpId5NfC8PrWViikgcawOnnZL3xYTU7I5g6PUO6ueKe96ukgRWOpxcM1KYcmuZjYDPsi9oaNCQaiKybcyZL1rIGYNL+XRjNuecL9e7d3iJHBpD0zIxbURlB6nfFsHjrRJ4c6G3g4xUGQwtr4qj//Aatq0T00ah1ckA3c6AieKrULHShdtjJ6u3TPpVvD2RjPRmUu84EYDAmvV8/GIUBU5xPcwd14pjVD7PPyzP5aJ+JSzbJAPFUytfOezAmd7Jo8Lqc3bWru5xQTphraSjlEo1fnMRe/fLhLFyhAkTJkwcSwR1IKzUExEW81ZKfQEkAT7gdq31JwdaOeJg+ZjM20QoolMlCMcer9ixNIHoSNl3e+wMuDWJj/4mTHryhXXsmONkKTLodvXMNrBZsI6Que13/24Z8entfLJDQjvOHlrCrk0JDJgpr1zha7AhEENCQD7UU8+uZv2HMgC4x+Jk2vV+1j4u3hVt2oaNIG5j4qqTz62h8ks7S1qEmV88tZa6pQGi02SA0BqtWLg4k8GJMiBZ1xjF0DtT8Xy6BQBbsoPVs+NJMlZn7/PcDJp//yzWCCmbc3AclrQkNvxL2GpuQQNrCtOY55LzP3U1EZ/ppnxHHAC9z+lg7TvRjLxWzgcqm5g1P52+PilPQUYDqVfmUPGcaAopJ2o2vB/LyLuk/C/9tY0L+5fQUCaagddjIzGtbd8zWbwng4l9y3hwjzD16e4AY38p9277sJC63VHk3iRaxJo/NzD0vCbKvxAtIf0EN5WrXOT9Stww9/x1C7WNUVQhYxGJ2sdzriAPXy9aBlYre56uYZGxylGqP8gZN0m9ou45fOadlzQsrD5nd936Hse8w42wPKWLY12uHGHChAkT3xUcz+Hx5hqWJo4p9k5MFfAqSoviSTAmh2pvc5Dep5nWamFsNkcQR7SfmEuFaStHBM/dU8mpDrG1zvUlcJpuoSMgbDmoFQ5rgP4XCZsOVLVRu8HBkiax1Z4/ugRHb2GT2uNj1ntJnJks7m0JIzVVS20k9ZGyKBtYIq0s/9iYyKp3Fa0NEeT8Qlatap21AteIRApfFZt077Pd+Ku9LFwm7PTMG4Kgg/hLRTG1D8ih9NkKahslSKfvkFq2b0ihHmGvp55dzZz56UyZIMz51aXZzDypjMULhAmPG1GOPdWGfUguAM8/7GN6nxIiMqXubUUQVQCNm2R/WVUqw6IbyJokbWHNT8e/tYwXFkiIev8OH8MGVTJ3u4wBTD+rkpplVqobxCY//HorSx4XYjogt5akHw+j5O9bAShuiGPUkApc/YXFf/h2ImeeUYm/3m88J5i1ModrrpbnvOJJGHODwrdd7O2PLM3knGArbT6pu09bGD5ankPSB4c/TWt24pCw+pzS+o3HJ/M2YcKEiZ6I45l5H054/D3GsRpD7Dda6zlHpZQmjls0V0gY9cKGFKYOKyHy3MEAPPGXZi5payUuX+y4QQ9U74gmulG8Htb8q40p6U3YXWLDntoYIOvKZNY+bCxNZnNy+c8d7HpMWFzmaIjN8DAlo4Sok9KwzvglTbf9BYDoUzLwKEgYLsx5zofpTL2yHdu0HwCwYsbbxLu8jJsir/r2j+NZZoniRw0SuFJTFE2as57SDvGSuOfDAC9d6sK/XMjcM8/Y6NPhZ+KfJGjmw1+XckKOj5EPny11+3IhsTua2OCXtvjvggyuKSijfosEAc08tZwFn2SQZRNNoLk8gh1rE8ldJnW7+q4CVt7nZUiseNasK8ogv7qF2GSRr7YprLYgX7wnNu/Jr0/m7Uc/ZoJVPD4WRsQwwgYr7eLxcXZhAJtDkZogiz20LvJQ6BBWnlIdhe35tST3lbbK6u9gyQsZZO+SvAZFNqKcVlxTDMczj4fpNcU8/oJoIZfkl1M/TxF/omgdFzkaSB/WTsN20bCam500FEs7JB3kvQkXPdWHOxyEswBx5/D4DmCeUmq2cfofWuu/HsXymTBxRBF1khlL9n1CT11oIRyEsxjDJcAUrfV1xv7vAC+yEHFrdzpv0+ZtIhRtrcIuM4c0E3nHNbTc9xwAX27KIkV5sVvk43MHbAwZV01jkchfXNvORyOsPLpZvEtuO6cO7243m9eIXTrB5SF/qua37321mO0dycKc44db8O7poHCj+GUXKRfnnViKcghTbtpmpaY6mtz+Yk+3RUNjUQQpp4gNuXphEIfLT3mZ2Mx7DayjqiiWinZhk7lxzaSPdNO8UzxxvyxL511HO3/Pk/zaGxzYIgIkjBY7b8NKH8nXD6HlVZmCds/2RHxBCwPGSrj/zpWJNPodjJlkREBawD4sl7aPdwGwdVMqsRFektOFKfs8VrZVJpEfJ1pKcq82ImeMQqWJjXvbrYvoe00kwWbxMKn6qIOGpki2ayl/f0srvU8UraJxu52YzA6e3CzM+5ap9cx7M4Hzbpfn4F1axPMrsvnRBWLPf/K9RCbRQpVXvElOPKuGZz/PYFSHaEQ56U1srkiif4q0xcKGFC77VTTzHpSyji+ooLJElkQbVfLuYduhU+L6h9Xn1DRt63E278MJjwf4qbEA8dPGWpcmTHynET88rNAGE0jH3dPxvV6AGEApdR3wE6AN2IQw7weBWiQs/j4gQ2v9oy6uvQG4AeD+zIEnXJ54JKZYN3E8obUlgqyhzTywQZjh/Te7cC8qwpEjHgxlHyv8fgsFd+QD0D57A9vWpNCmxeo3YkgFMTeezuu3bQdg+iVNNCxq3zcr3qytOYzyeRg6VdjdBx+kMPWKVjybhF16G608VJHC3UNlIqpAO0Sdksmel8TvOmOsh5IlUdjtYl/PvS6N1o+KiPnRqQB43vqCP6/K5BdDxTvEdWpv1v+jkbqAsFOfUkw6vZLZn4lNvA/trLZGcppT2Oozvjh+M6GK6tVi991dH0dOnNiQF7cnck5uGa4M+U4jxvZi/b+a6DdGfMqDHWCLt1C8WNhqbKyH2AwPWzaIBpKZ2EzaZCvKIW31+UtRJFu9DL1Tzv/r78LWzw7K/cp8kaRavCy3SttfNb4Ua6ydZ+aLuanQ4uO26EYCASGqO+rjyXa1UeMW+cQIDwmJ7SyuFvl6K1w5qoTKjaIBpQ1qpWR9HM4I/776xQU0JXbJ7xxnPbm39QYg8ub/HDYbTozpG1bPXN9SeFwyb7TWT2mtT9Banwo0ANu11lVa64DWOgg8wQEWINZaP661Hq21Hm123CZC0doSsd/+/Te79tsv+3j/b6p99ob99kcMqdhvf/olTfvtz9qas9/+Bx+kHFI5jwUWG+tC7kXE2F777Qc79pePjfXst5+Z2Lzf/ucvRe23v7fj3osyX+R++1eN7/lzER3PzDtcb5NUrXV1p/D48UqpjE6zCk5HzCsmTJgw8Z3B8bwM2uGEx78AjEDMJruAGzt15l3CHLA0EYrYTAne8LVZiB7hItAg7LFho5WksYqyBV/xi4xRbv6yTEwrNyZXsbA8gzP6CDu0x2mUDRZ/KaaJN5wd/NrhJuNEyf+jj9LZ4VD06ZBX8PTxpexeJRM3aQ2fqxjOMcwY8entvFOczSUjZKKndcvTGNS/mi8Kxd3tSVs9T6Z3UFwizDjS5qP/+R5eni0DoPVWOE810ef/pCzb/l5Jvde5rx6tysqQ5DrmGgFDV51azq5F0eSOEq2ho07hSNLYkkUraVwTIPn6IQQ2FgLw9tuJjIpoxOeXAdTMgiaiRsfx6OsydcAV6eXEnxqHbhFXQUt6Ai2fVrB0u5RneFoNCYP82PuKacO/u4Zff57EgxPFFKMcVpa+l8gp/xkEQOUDX5B+z1fB1Of+7BP+Y8xlnpzdit9rIWGq5B2sbaJjZzMWY/D3PyuzuD6rnI075V5WNKmR7ZQYg7vtFisn5ldgtcvAdHuDgy3V4iR4XtXhh8dHR/YKq89pbS/ucWaTwwmP/8GRL44JEyZMHDn01Olew4EZHm/imKK5WRhpRkETVcWxNHmE0fUbUMvbO7OZ3luY9fatyViVpjEoHhAj+1ZRsTuWL5SwzYvSKijck8SoSeIO+PriLMZYm/EHZFjH67cx7KI2PIUy7WlTmRO3WwYIU/ObsceDu1KY7O5dCQy5oBX8wgZLFzjIGNXGf5YL8759ppv2lXW8ZoST+xTc9PtU6p8SVz+rPcjWbSksdkr+022NFLfGckJ/UUztcZrIC0/g1bvF9W9yRgVtTRH0WfwvAOou/jG1pdGk5MkgYtToRBoXNtHeLPmljfaxbn7ivnD6WO0n0uon0iFBNn0fGIF/2Ro2vSHypdpFiV1x073Cjj+9q4RY5SM1XlwFc37Wm5X3VdGnQFwTi4uSWGd3kumT+p88soy3N0hdr3ogm3/+vpQcYzW5M/uXEHv9qegKqVvdq8W0tziIThCNp7Uhgu2N8ZxQIIPBT5ZlcvtMN5tekOtjozy0uR00+QyX0dgWUvtJuY5EeLzLlRdWn+N27z4+mbcJEyZM9ET01MHIcGAybxPHFHsnpvI2W3EmBggYDhMbN6cz9pIWPn1D7NIJ+GjRNuKVUL6MlGbSLknh9f8JYbrkRk1gd/W+FdVbdlvxeWxU1YmL2qDJDXz8aQYjEox1LluczLXIudN9bobeEk3VLGGHUUkd7NyWtC88PPt/V/H2ZfNwBeX1zXO0oZSmsENY/5mTK6hdY6eiXvZHzzqX2Zd/wjmXC3Mune1nZWsiwxxi035Hx3LryRXgl/yaCq0knuSg9EOpe+4PEvBtq2D9J2L7HTG1iWCrD0u0MO3m9X5aG5wk9xKG+uW6TPKcreRNFK3i4QXpjPEEOPk+GR8IrN1K+2Y3VmOKWU+9ldYG577xgEXzUtnktDI5IPXtPa6RoEdjzxQ2rD0B/vyF2KyvtDbR7rXvY8oprnY+C8ZxUkDKssQaxXWXtuDdIm6Yi1ZlkYCPwWNEy3hnTQ65Ph99M2R8QVk0cf0D2PvLpFtlrzbgcUs9h+16/7DZcIQzJ6w+x+spMZm3CRMmTHxXEAwev+HxZudtwoSJ4xbHtaofrhP7kUrADUdS7mjJHuv796SyHuv796SyHuv797Symukg7fit3xBWHkm5oyV7rO/fk8p6rO/fk8p6rO/f08pqpgMnc5YeEyZMmOiBMDtvEyZMmOiBOBad9+NHWO5oyR7r+3dH9vt+/+7Ift/v3x3ZY31/EwfBt+rnbcKECRMmjgxMs4kJEyZM9ECYnbcJEyZM9EAc1SAdpdQA4EIgyzhUBryntd4SIucAZgLlWuuPlVJXACcBW4DHtda+o1lOEyZMmOhpOGo2b6XUncDlwCxg75Ic2UgnPUtr/adOsi8hfySRQCMQDbwFnG6U8ZojWK5UrXX1kcrvaEEplaS1rgtTtkfUCbpXLxMmTBwER8uBHNgO2Ls47gAKQ46tN35tQBVgNfbV3nPfcK+kAxxPDElJyMIRCUBiJzkrcCOyFueEkDx+G7JvM2TnAeuNNBe4qXN9gQLgaeB+5M/oCWS1odeB/JA8/wQkG9ujgSJgB7AbmHgodTJkY5G1Rl8Argg592h363Q063WA53fBN5zv6v1KDtlXwDhkBaiLjG11gPxygXhjOx+4GBgSRjn7ADOAQWF+GwesVzh1Ohr1Qr5L1Wn/NOAO4JzDrVN36mWm8NPRyxi2AnldHM8DtoUc22i8PAlAy95OCHACW0Jku9PRBYHikOQzfos6yT0JvAz8H7AK+Hunc6tD8nwF+C8wHtEkso3t/wKvdpJbCNwM3GXU7w4gB7gO+DQkzw2dtj8Dxhjb/QiJRgu3Tobsm0Z7TQPeM/YjQusVbp2Ocr0uCkkzgMq9+yGypyHaXC3wEZ3+NELqdZbxbsw1nvGTyB/UDuCskDzvMtpwK3C98fsUsuD27SGyn/HVO/gDhKg8CWwAfnYo9Qq3TkerXsA6IMHY/iWwBPgtMB948Gg/KzN1Px29jGFKpxfscSPtfcGmhMjehnTCu4FbgU8QRrcBuDtEtjsdwh3GPYd2OlbcRVnXd9q2GWV9C4gA1oTIbj9Inbd32l7TaXtPiFxonlsAm7G99ED17U6djONrQ/b/H7AYYeuruyr3wep0lOvlA2YjrP4ZI7UYv0+HyK4ABhvbFwOFwPguyreFEG3AON6Lr5OCTYDLaJsWIMU4HgVsDJHdGFKWJGM7khBNMdx6hVuno1WvkO2VgKvT93BIdepuvczUvXR0MxdvlvHIP/MMY9t6ANlMINPYjjce9Ngu5MLuEIxj2YhK/3cghhB2ashs7eLY3UhHF2riWQpcAlhC6nkZsKzTsVXIH8oYhHWMNo736eJj+BnCSiYD9wD/AiYC9wIvHEqdOrWVJeTYD40Pend36xRSr7Eh9ep7OPUy2ukT4OZOx4oPUK91IfuDgW2IhtH5T6lw77sSIu8AdoQc22u6swLVIW0R2nmvAbKM7c8AZ6drNx1KvcKt09GqF8K0hxjb8/iKhTu7qP8Rf1Zm6n465gXodoG72dF1uu4Co5Oq7OLci4RoA8bx6wFfyLF84FWgBlGXC42P4lWgVye5042XdAtwMmKy2Cs7rYt7TTLyWINoHHOAG+jCVhhOnYzzfwbO6OL4FDr9KXWqU7VRp+1d1SmMel14kHqt7lSvG7uqF/KH8XOkQxzLgf+UVgLpIceygbVAS6djvzba807gCiPdaRz7dcj1zyKms3cRM9ILwJWIieG1Luq0CfgD8DDS8d2NmBh+cSj1CrdOB6nXXYdTL2AYYjp53kg7ESa9kpDxkqPxrMzU/dQjIyyVUpMQu2s/RK0rAd5BVDZ/iOwAxFVxGRAAemutNyqlpmit53WSGwtorfUKpdQgpIPbqrWec5ByJBmb/9JaXxVGuWcjAzsHnSFeKXUK8kFs0Fp/FHJuHKIaNyulIpE/sFEII/6j1rqpk+ytwNta65JvuJ8D8QwqRzrZKcAEpIPaz1Wzk2yZFrfOKw8ka8gXIFpXDtL+24CXtdbNBylPFvAPhNUXdHH+DKBGa70u5Hgc8FOt9QOdjg1C/uRC3VU3h1xrQ7QPDbyBtP8VwB7gEa11Wxf3uoKv3sFS4F2t9daD1CsT+GdX9TpIneKBWzrXyTg+kK7dcL+pXuOQ5/e1eimlrIg9vXOdPtRaNx6kTof6rLqsl4nw0SM77wNBKXWt1vqZTvu3ArcgLHEE8HOt9bvGudVa61HG9t3AOcgLOx95wT8DzkRe3s6dwXtd3Hoy8CmA1vqC7sgZssu11mON7euNMr+DfEjv6/3dKjcBw7XWfqXU40Abwn5PN45f1Em2yTi/E2Fdr2uta7pot72umi6gCbGHvk0XrprdlP05cB4yyHkuwgwbgenAT7TWC7poo+MeSqk0rXXVEc6zO66lprvm8YBjTf2PZOLrA2gbgGhjOx9R4X5u7K8JkbMiA07NQKxx3MXX7birETPLJMRcMwmoMLYndpJbE45cF2VZwf6DSqEDe1s6lyXkXOgA5RpEvT0LUZNrEHvmNUBMJ7mwXTW7Kbuh0/lIYIGxncvXB+HiEM+YrUA9UIf86f4Jw80tzHdgbqftzq6Sl4fIPRqyH5ZbpbGfjnjiPIIMBN6DuFe+BmSEyIa6dibStbvqlJC2eMrI82UgLSTPzh5XJyCD/YV07XHVlXfW12SR9/q3iGb6TW08GiE3LyIa1XzkT3kFMDJENhoxL21C/uxrEFPfD7+NPuF4Tj0uPF4ptf4AaQOQFiJu0Vq3AmitdyEd6DlKqb8jnc1e+LXWAa11O7BTGyq91tqNuOZ1xmjERPH/gCYt7NGttf5ca/15J7kTwpQDsCilEgwzjNIGO9ai0vpDZDcqpa41ttcppUYb7dIP8QLoDK21DmqtP9JaX4cMCj+KmEWKQu7vQAY/I5HOA8Tbxt5FWcOVha+ieCOQDxmt9Z4uZF8DGoBJWutErXUS4mbWYJzbB6XUqAOkExANay+eQZ7zm8DlSqk3lVIRxrnxIffvLDvzG2SfBTYj5rrPADeiYXwBPBYiW4u8B51TFtJZruwk98dO239D/uinIh3i/0LyPE9rXWts/xW4TGvdF9EU/3YQ2b8cRDYBcRT4TCm1XCl1m2Hm6QqPIuMpHyD2/v9preMRu/ujIbIvIe/a2ci41L8R98rTlFJ/xMSh41j/e3Q3IWxvBOIv3jnlI+H1nWU/BUaEHLMhAzKBTseWAZHGdufR+DgOMCLOVx4fDxPC+LsrhzCxIgxfbQz2hnR2oWw6Duk8dhrl9hnXfI6YTTrLHtAVa299je3uuGp2R/bnCHt8AmHU1xrHU4CFIbLbDlLW0LiAgPFsP+siuTvJheUqeQiyB3OXDM0nXHfV1QfJI3S/Oy6YYcmG3P8UpBOuNNr0hpDruuMuGuptsmLvd0YXXl5mCj8d8wJ0u8CiTp58gHMvh+xnEzLS3enchE7bEQeQSe780R1A5jxkoPCbyh2WXMg1kYR4e3Q6FwsMRxh+2gFk+nXjXmG5ah6C7GDj/IBvuP9HwK861wXRpO4EPg6R3Qj0PUA+JZ22w3KVPATZdZ227w85d6juqqXA7UhnX8T+0Y6H44IZlixdkBTElDgFeCbk+JeIKe4S5E98mnF8Il+PtViC8b0iA8cfdjp3wD9sM31zOuYFMJOZtNYgavtDfGXzrjc61IcwfI47yV4M9D9APtM6bYflKnkIsn/AGEsJOd4HeOMgdTyYu+rdIWnvuEc68HwX8pPo2rW0K//vb5RF5hsK91kNBz5EAvAGGH8Ijcgf3UkhssOA5Yj5axEGoUC0r1uP9XvXk9MxL4CZzPRNCcPcciRlj0ae4cgig+BDupPvsSrrd+X+Zuo6HVeugiaOTyil9mitc4+k7NHI82jJft/vb6JrHNX5vE2YCBdKqfUHOkWIF1G4skcjz+O1rMf6/ia6D7PzNvFdQRriTtYQclwhg16HIns08jxey3qs72+imzA7bxPfFcxGBgHXhp5QSi04RNmjkefxWtZjfX8T3YRp8zZhwoSJHogeF2FpwoQJEybMztuECRMmeiTMztuECRMmeiDMztuECRMmeiDMztuECRMmeiD+P1aNUGuZZXFfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {\"numThreads\" : -1,\"verbose\" : True,\n",
    "\"lambda1\" : 0.05, \"it0\" : 10, \"max_it\" : 200,\n",
    "\"L0\" : 0.1, \"tol\" : 1e-3, \"intercept\" : False,\n",
    "\"pos\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 200)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm = GLM(X, Y, reg_lambda = np.asarray([0.05]), group = np.asarray(list(range(200))),max_iter = 100, learning_rate = .1, tol = 1e-3,parameter=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "converged 0.05\n",
      "(200,)\n",
      "converged 0.05\n",
      "(200,)\n",
      "converged 0.05\n",
      "(200,)\n",
      "converged 0.05\n",
      "(200,)\n",
      "converged 0.05\n",
      "(200,)\n",
      "converged 0.05\n",
      "(200,)\n",
      "converged 0.05\n",
      "(200,)\n",
      "converged 0.05\n",
      "702 ms  32.4 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "glm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glm = GLM(replicates[0].xtrain, replicates[0].ytrain, reg_lambda = np.asarray([0.05]), group = replicates[0].groups,max_iter = 100, learning_rate = .1, tol = 1e-3,parameter=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "(24,)\n",
      "converged 0.05\n",
      "54.1 ms  1.06 ms per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "glm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 24)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replicates[0].xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 2)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replicates[0].dg_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_torchmat(dg_M):\n",
    "    \n",
    "#     p = dg_M.shape[1]\n",
    "#     n = dg_M.shape[0]\n",
    "#     d = dg_M.shape[2]\n",
    "    \n",
    "#     output = np.zeros((p,n*d, n*p))\n",
    "#     for j in range(p):\n",
    "#         for i in range(n):\n",
    "#             output[j][i,(i*d):((i+1)*d)] = dg_M[i,j,:]\n",
    "#     return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_torchmat(dg_M):\n",
    "    \n",
    "    p = dg_M.shape[1]\n",
    "    n = dg_M.shape[0]\n",
    "    d = dg_M.shape[2]   \n",
    "    \n",
    "    #dg_M = replicates[0].dg_M\n",
    "    output = list()\n",
    "    \n",
    "#     for j in range(p):\n",
    "#         print(j)\n",
    "#         toappend = np.zeros(((n*d),(n)))\n",
    "#         for i in range(n):\n",
    "#             #toappend[(i*d):((i+1)*d),i] = dg_M[i,j,:]\n",
    "            \n",
    "#         output.append(torch.Tensor(toappend))\n",
    "\n",
    "    for j in range(p):\n",
    "        print(j)\n",
    "        toappend = np.zeros(((n*d*m),(n*m)))\n",
    "        for i in range(n):\n",
    "            \n",
    "            for k in range(m):\n",
    "                toappend[((k *n*d) + (i*d)): ((k *n*d) + ((i+1)*d)), (i + k*n)] = dg_M[i,j,:]\n",
    "            \n",
    "        output.append(torch.Tensor(toappend))\n",
    "            \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "    dg_M = replicates[0].dg_M\n",
    "    output = list()\n",
    "    \n",
    "    for j in range(p):\n",
    "        print(j)\n",
    "        toappend = np.zeros(((n*d*m),(n*m)))\n",
    "        for i in range(n):\n",
    "            \n",
    "            for k in range(m):\n",
    "                toappend[((k *n*d) + (i*d)): ((k *n*d) + ((i+1)*d)), (i + k*n)] = dg_M[i,j,:]\n",
    "            \n",
    "            \n",
    "        output.append(torch.Tensor(toappend))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "torchmat = get_torchmat(replicates[0].dg_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.asarray(np.repeat(n, p), dtype = int)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = torch.tensor(np.asarray(np.repeat(n*m, p), dtype = int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = make_A(torchmat, gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gs = torch.tensor(np.asarray(np.repeat(n, p), dtype = int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.tensor(replicates[0].ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00, 37.68it/s, t=0.15, rel change=1e-06]\n"
     ]
    }
   ],
   "source": [
    "b_0, B = gel_solve(\n",
    "    A,\n",
    "    B,\n",
    "    l_1,\n",
    "    l_2,\n",
    "    gs,\n",
    "    b_init=None,     # initialization value - useful if solving multiple instances\n",
    "    t_init=None,     # initial step size for line search - default (None) is to use previous iteration's value\n",
    "    ls_beta=0.99,    # line search shrinkage factor - default (None) is to not perform line search and use t_init\n",
    "    max_iters=5000,  # maximum iterations - default (None) is unlimited\n",
    "    rel_tol=1e-5,    # relative change in b_0, B to stop the algorithm (default 1e-6)\n",
    "    verbose=True,    # verbosity (default False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 1it [00:00,  3.54it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00,  6.89it/s, t=0.00043, rel change=0.0046]\u001b[A\n",
      "\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 1it [00:00,  3.69it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00,  7.20it/s, t=0.00043, rel change=0.0046]\u001b[A\n",
      "\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 1it [00:00,  3.60it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00,  7.02it/s, t=0.00043, rel change=0.0046]\u001b[A\n",
      "\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 1it [00:00,  3.60it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00,  6.98it/s, t=0.00043, rel change=0.0046]\u001b[A\n",
      "\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 4492it [00:22, 418.45it/s, t=7.5e-09, rel change=1.5e-05]\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 1it [00:00,  3.22it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00,  6.29it/s, t=0.00043, rel change=0.0046]\u001b[A\n",
      "\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 1it [00:00,  3.69it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00,  7.11it/s, t=0.00043, rel change=0.0046]\u001b[A\n",
      "\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 1it [00:00,  3.33it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00,  6.52it/s, t=0.00043, rel change=0.0046]\u001b[A\n",
      "\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 0it [00:00, ?it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 1it [00:00,  3.65it/s, t=0.00043, rel change=1]\u001b[A\n",
      "Solving gel with FISTA (l_1 1, l_2 0): 2it [00:00,  7.09it/s, t=0.00043, rel change=0.0046]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295 ms  14.7 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "b_0, B = gel_solve(\n",
    "    A_conv_tr,\n",
    "    y_tr,\n",
    "    l_1,\n",
    "    l_2,\n",
    "    ns,\n",
    "    b_init=None,     # initialization value - useful if solving multiple instances\n",
    "    t_init=None,     # initial step size for line search - default (None) is to use previous iteration's value\n",
    "    ls_beta=0.99,    # line search shrinkage factor - default (None) is to not perform line search and use t_init\n",
    "    max_iters=5000,  # maximum iterations - default (None) is unlimited\n",
    "    rel_tol=1e-2,    # relative change in b_0, B to stop the algorithm (default 1e-6)\n",
    "    verbose=True,    # verbosity (default False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 4])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 200, 100])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_conv_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_trs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 4., 4., 4.])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.repeat(n*d, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = replicates[0].dg_M.shape[1]\n",
    "n = replicates[0].dg_M.shape[0]\n",
    "d = replicates[0].dg_M.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pip install torchgel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchgel\n",
      "  Downloading torchgel-1.0.0-py3-none-any.whl (38 kB)\n",
      "Collecting tqdm<5.0,>=4.0\n",
      "  Downloading tqdm-4.53.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     || 70 kB 8.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.10 in /Users/samsonkoelle/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages (from torchgel) (1.15.2)\n",
      "Installing collected packages: tqdm, torchgel\n",
      "Successfully installed torchgel-1.0.0 tqdm-4.53.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torchgel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gel.gelfista import make_A, gel_solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A = make_A(replicates[0].xtrain, torch.Tensor(np.asarray(replicates[0].groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replicates[0].groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_tr = 100  # number of samples\n",
    "ns = [10, 20, 5, 200, 1, 20]  # group sizes\n",
    "\n",
    "# data distribution parameters\n",
    "X_scale = 2.0\n",
    "X_bias = 3.0\n",
    "\n",
    "# coefficient distribution parameters\n",
    "_scale = 0.125\n",
    "_bias = 6.0\n",
    "\n",
    "# true support\n",
    "support = [0, 1, 0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,  20,   5, 200,   1,  20])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.tensor(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "A_trs = [torch.randn(m_tr, n_j)*X_scale + X_bias for n_j in ns]\n",
    "s = [s_j*(torch.randn(n_j, 1)*_scale + _bias) for s_j, n_j in zip(support, ns)]\n",
    "y_tr = sum([A_j@_j for A_j, _j in zip(A_trs, s)])[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 200, 100])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_conv_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_conv_tr = make_A(\n",
    "    A_trs,\n",
    "    torch.tensor(ns),  # note that this needs to be a LongTensor\n",
    "    device=torch.device(\"cpu\"),\n",
    "    dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 100])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_conv_tr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12b6fa9b0>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(A_conv_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x118ef4b70>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(A_conv_tr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x137d41320>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQQAAAJFCAYAAACLPfrcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XuYnWV97//3h3AMhLMgEBQMB22ngjpSFFEBsYgobjdtsWrxmOplKVJ3UXqi9trd9bQ9V20qKP1tRBFBrVUKpaDSSuyAIIFwCnJICAREIJxCkvn+/pjF3mM6wL3mWTOZyPt1Xc81a93P/TzPN2vWKfd87/ubqkKSJEmSJEnSU8NG6zsASZIkSZIkSdPHAUFJkiRJkiTpKcQBQUmSJEmSJOkpxAFBSZIkSZIk6SnEAUFJkiRJkiTpKcQBQUmSJEmSJOkpxAFBSZIkSZIk6SlkygYEkxyR5LokNyb5wFRdR5IkSZIkSVK7VNXgT5rMAq4HDgeWAv8JvKGqrhn4xSRJkiRJkiQ123iKznsAcGNV3QSQ5KvA0cCEA4IPfeGE5lHJ6//2Z81BzHvVo819H1i0qrnvZk8bbe4LsMWbf6u570/ec1lz3803XtPcd/HaOc19993ogea+ex/e3hfg7v+c1dz30VXtfb//yPbNff/7s29r7vvJG3Zr7ntrPdLcF+ANj2za3Pfzm7U/zl/9g/bH4oefbn+N3LJJ+9vFzmv6e408e+t7m/tutV3747z0tm2b+z64dpPmvvel/bl58eZrm/se9XCa+wL8fFZ7HD/etD2O3320/Xmxz2/+vLnvylvaH+OHVra/PgDuvH/L5r777n13c99/+Vn7e8A/cHtz3zdvtGtz391Wt//uAC7ZvL3v/779B819/26nQ5r7HrNf+/vs2Vfu3tz3v/96+3kBll+/dXPfXYfub+77F1ft3Nz3hM1XNvd92nMeau7bjwsvndtX/3/drP09YM/arLnvc1a1fza85AXLmvtucfTzm/ve8Zmrm/sCnHfvTs19X7/n0ua+qx5o/0z9ys+f3tz3ZY8+3Ny3Xz+v9vfllx6+ornv4Re2f599y6z294sXjj7Y3He7bfp77a0dbZ9ktfKB9tfItlu3//4W3vu05r77bd7+PeuBPr6fLmF2c1+Aow5f3tz3kvPaX3svfkkf5/33XZr7Pn/PO5v7/uny7Zr7AvzZFu3Pz91+t/3cG7344Oa+D3z0a819165q/4665f5bNfcFuOxrWzT33f/Q9u9wH/xR+2f1+59xR3PfzGpPqLrguvb3LIBftH+159g+/k/7tAu+399/Mp4iVt990+Cz4x7HJjs+a0b+DqZqyvBuwPhn6NJemyRJkiRJkqT1aL0VFUkyP8lIkpHTfrhofYUhSZIkSZIkPaVM1RqCLwL+qqp+q3f/ZICq+tuJ+r9zj99uDuKT72hPu//dBb9o7nvsmvZU7M37fMx2qvapN9vPbp8q8IuH2ueFnbt5+9SUE7ZvT8X+z+XtqdgA981qz5Tdam374/zKl7RPLXr7j9unkB0y2t734I3ap5sBrFzVPiXjyk3af9cvnXVfc98527RPv936Ge3P47tv6G8KSY22Py+uXNn+Wt1+tH0a0tNnt08X2nq79tfpg/e1v2ctf6C/KRbbbNK+1ME9q9ufQy84oH3qza1Xtk/Lvn51+79vbrX/2wCe94Edmvt+5BPtU/Df95r2z5EVF7dP7e1nSYTP9PG4AexU7VOzX72m/Xm/mPZp2S/Zvn264A77tz9u91/b3BWAbX+z/fW30XZ9PM6z26c3PXLJzc19Z23V/nfa3/tRewxff0d/U9lO+4f29+Q1fUyAedXm9zT3vem+9s/fe2a1f8d59Yvap/UCrLyl/bX6ylvbvwecuXn7sgE77NT+nrXlLu2fez+/sf05BPC9B3ds7vu217VPUV36vfap5Jc83L4sypG7ty/jMHtuf0udrH2g/Tvq5Ze3T1G9rJ/v6x9of13ff+ZVzX3/9cb2JQaOfl37siEAbNr+7/vOWe3vAftv2f5Z/d1V7c+ht/xG+5TMf7qiv6mh9/UxNfShPt5n3/PK9s/fFf/efuKnv7r9u/1GO/X3mbP0tPbX6jW/aP/9vfyw9mnAn7+k/T153z6Wv9im+lv2ZYfN2/+Pccuq9u8tR9/xlRk5XXV9W73ihumbMrzT3jPydzBVGYL/CeydZM8kmwLHAt+eomtJkiRJkiRJajQlGYIASY4EPgnMAk6rqr95vL5X7fma5iD6CXerOe0ZJv90X/vCtc9Z1f7XV4AVffzl+tdmtS8+/udZ3dz3r0fbs9FmbdT+IK9a08eft4B/37j9r0svWNX++1u0aR+Lmj/a/rgNH9n+l89Hbm4/L0Ctbf8jwWgfKRhV7X2vu6n9r/0Xb9GedfTHB7VnmAHct7j9bxM7HNqepfS1r7YX03nNvu1/BZ79vD6KlVzWniVx1y3t8QKsWdv+uO26V3vm6KJF7Zm/V23W/t7y9mPas2f+/uz2zACAt76g/fe36bPbn/ffOr09g+bQZ7VnKt9xa/u/b/mq/rJ4/mGz9oXKP95HkZ5HV7V/lu24d3vm4Qevai+YcHSf9RIOPL79fWvJF9tfI/0UFNh4VnsmwaabtX+/6OcxvvdnfVSaAa68uz3j9rfmt//7Lv379u8X/9FHptSJv92eQXf7P/eXfby4j2yUV/xh+/Mi27V/jvz8y4ub+15+W/v79zmb9/dY7FPtz6MT+vidrLqu/XPymz9pz8K6eFb7a2T+qv5yJD7Xx2t1y7Q/l98z2v5d8nMbtX/+/s289qyxlXe2/55nbdxfZuW2v96eLbXlp77Y3Pfg576tue9frG3P2Hz209qzmv/+gfb3CoAXP9L+ff3Ojdufn3P7KET23Ge0Py8uur39cTtou7ua+wI89GD7c/m199/a3Pf0TfZp7vvMXduzTC+7vX3c4KDntH83hP7+L3DhmvbPkRNu/T8zMjttfVt953XTlyG4874z8ncwVVWGqarvAt+dqvNLkiRJkiRJ6t+UDQhKkiRJkiRJM85of1nOv4qmbMpwP855+u81B3H7Jn1MkVvd/gs++Dfa03n/46rdmvsCbFHtcWyZ9ikIu89tT22+6bb2NPYD/qp9+tbST9/Q3Ldf99zXPr14Vtqfx3sOtaf/33FDe9r2Vlv3N/Vmx8PbF4K99sz2f999q9unT/cznf35W7c/bh9+pH1aL8COtE/rm13t7wHPaa+Dwuq0Z3HvuLb9dfofW7RPq3//O/ubgr/8jDub+159T/sUwOV9vM9u0sdHyOv7WMx/kz36W5D6gYXt74cX9rFg+k5r26dv7TfUvnj1j65u/xw55DXthZ4ANnnJ85v7jrz/xua+/UyoG3pV+xTAVbe2P8b/dF1/i7bvsKb983dpH8/7t/x2+/T3G85uf2+5ek37Z85etE+H/I339FeYZsk/tL+ePkH7VK+/mNO+LEo/U9SfccKzmvt+9iPtz02AX3+kffrd7lu0T5Nd+nD75+Sum7f/rvuZoj7aR0EvgAtGt2nu+2gfp37Zo+1rATz7wPb3wx//R/sUx+fOa/88Bdj6wPbX6qe+0d73Hc9q/5zsZ2rhNju0P8ZLbm3/P8PwG/v77vvASPt75+zntH+f/eZ3ntbcd99qfz3dMdo+fbrfKfiHrW5fDuSIZ7cvi7Llwe3FMT7zpfbvnb9I+3vhax/pbxml3zi6/bPh1gva/89w2mj7Z98hfUzhvmnT9sftN/so3gaw1ebt/3nZ4/XtcWz1sW/NyOmq69vq5Yunb8rwLs+Zkb+DKc0QTDILGAGWVdVRU3ktSZIkSZIk6clUH4lbv6qmqsrwY04A2ldCliRJkiRJkjSlpixDMMlc4NXA3wB//ER9X7h7e5r+Hbe3V2ZcNKt9OsZX+piG9Ok11zT3BfjSxvOa+/aTMr30X9vT2DfdqH30u+5tT+f/yMP9TUPaq48KdQePtle//MFm7ecdestzm/vO/l8/au57wj3tUxsA/u4H7RUtr6r29P+X7dReMWztivZpIaN9VEXetY8pZABb9TENeG4fsxBW9PEO95od2t+Hzr6nvYrjWtpfe9m0v7fk0T4qSr/8Ve3Piw9d1F49bd/V7b+7Tfdpn9Lzf07v7/W0Xx9T3/qZarm4j8roLxpun5b9xRva32cP36d9ijPAP3+gfWrR0Nbtj8UuB7VPY5m1Xfvj9oGL2t8v/u6j7Z+nAP9yYvuyFvdt1D5r5OcXtz9uSftjsWcfn3tDb21/zr9jQfvzDeCzQ+39//yW9s/f6uM9a9l97dMh//wj7VUn/7+PDjf3BVh4/FXNff+p2r+j7t/HZ8MPR9vPu10fn5FDm/T3vHjz3u3vLZ9Y0r4swl3V/n5/2Uj79/Wjnt7+ub5iWfvzDeDfvtr+Ozmgj6UnHrqn/f2wn9fT2Xe2T59e3UdR8h98o7/ve8cf0P6ce/i69im47ZNZYd+X9bHk0vfbv38ftaq/au7nb9b+7zvgtj6W4vnh7c1dj9u9/TvchTe1v6af+cz+puCP3tv++Xvhmvbvkv/z5PblZ5b/3fXNfbm7/f9Pj1R/ywHd9VD7e9EZfSxJ8rcf6ysMPYVM5ZThTwInAf19wkqSJEmSJElTxaIiUzNlOMlRwIqquuwJ+sxPMpJk5Iy72gt6SJIkSZIkSZq8KakynORvgTcDa4DNga2Bc6rqTRP1P2GPY9d/qWNJkiRJkqRfIZ+6+aszssLt+vbobVdO2zjUprvvNyN/B1OSIVhVJ1fV3KraAzgW+LfHGwyUJEmSJEmSNH2mcg1BSZIkSZIkaWYZ7ack0a+mKR8QrKqLgYun+jqSJEmSJEmSnpwZgpIkSZIkSXrqKKsMT8kagpIkSZIkSZJmpinLEExyIvAOoICrgLdW1SNTdT1JkiRJkiTpSY2aITglGYJJdgP+CBiuqiFgFmPVhiVJkiRJkiStR1O5huDGwBZJVgOzgdun8FqSJEmSJEnSkyrXEJyaDMGqWgZ8DLgVWA7cV1XnT8W1JEmSJEmSJLWbqinD2wFHA3sCuwJbJnnTOn3mJxlJMrJo5ZKpCEOSJEmSJEn6ZaOj07fNUFNVZfgVwM+q6q6qWg2cA7x4fIeqWlBVw1U1PDRn3hSFIUmSJEmSJGm8qVpD8FbgwCSzgYeBw4CRKbqWJEmSJEmS1MY1BKdsDcGFwNnA5cBVvessmIprSZIkSZIkSWo3ZVWGq+oU4JSpOr8kSZIkSZKk/k3ZgKAkSZIkSZI044yuXd8RrHdTVVREkiRJkiRJ0gzUaUAwyWlJViRZNK7to0muTfLTJOcm2bZ7mJIkSZIkSdIA1Oj0bTNU1wzBLwNHrNN2ATBUVc8FrgdO7ngNSZIkSZIkSQPSaQ3BqvpBkj3WaTt/3N1LgWO6XEOSJEmSJEkamNGZm7k3XaZ6DcG3Ad+baEeS+UlGkowsWrlkisOQJEmSJEmSBFM4IJjkz4A1wBkT7a+qBVU1XFXDQ3PmTVUYkiRJkiRJ0v/jGoLdpgw/niRvAY4CDquqmoprSJIkSZIkSerfwAcEkxwBnAS8rKoeGvT5JUmSJEmSpElzDcFuU4aTnAn8CNg3ydIkbwc+C8wBLkhyRZIvDCBOSZIkSZIkSQPQtcrwGyZoPrXLOSVJkiRJkqSpUrV2fYew3k11lWFJkiRJkiRJM8iUFBWRJEmSJEmSZqQZXP13unRdQ/C0JCuSLFqn/fgk1ya5OslHuoUoSZIkSZIkaVC6Zgh+mbEiIv/4WEOSQ4Cjgf2qalWSnTpeQ5IkSZIkSRoMqwx3yxCsqh8A96zT/G7gQ1W1qtdnRZdrSJIkSZIkSRqcqSgqsg9wcJKFSb6f5IUTdUoyP8lIkpFFK5dMQRiSJEmSJEmS1jUVA4IbA9sDBwJ/ApyVJOt2qqoFVTVcVcNDc+ZNQRiSJEmSJEnSOmp0+rYZaioGBJcC59SYHwOjwI5TcB1JkiRJkiRJfepaVGQi3wQOAS5Ksg+wKXD3FFxHkiRJkiRJ6s/o2vUdwXrXaUAwyZnAy4EdkywFTgFOA05Lsgh4FDiuqqproJIkSZIkSZK66zQgWFVveJxdb+pyXkmSJEmSJGlKzOC1/abLVKwhKEmSJEmSJGmGmvSAYJLdk1yU5JokVyc5ode+fZILktzQ+7nd4MKVJEmSJEmSOhgdnb5thuqSIbgGeF9V/RpwIPCeJL8GfAC4sKr2Bi7s3ZckSZIkSZI0A0x6DcGqWg4s791emWQxsBtwNGOFRgBOBy4G3t8pSkmSJEmSJGkQXENwMGsIJtkDeB6wENi5N1gIcAew8yCuIUmSJEmSJKm7zgOCSbYCvgG8t6ruH7+vqgqoxzlufpKRJCOLVi7pGoYkSZIkSZL05FxDsNuAYJJNGBsMPKOqzuk135lkl97+XYAVEx1bVQuqariqhofmzOsShiRJkiRJkqRGXaoMBzgVWFxVHx+369vAcb3bxwHfmnx4kiRJkiRJ0gCZITj5oiLAQcCbgauSXNFr+1PgQ8BZSd4O3AL8TrcQJUmSJEmSJA1KlyrDlwB5nN2HTfa8kiRJkiRJ0lSpWru+Q1jvBlJlWJIkSZIkSdKGwQFBSZIkSZIk6SmkS1GR3ZNclOSaJFcnOWGd/e9LUkl27B6mJEmSJEmSNAAWFelUVGQN8L6qujzJHOCyJBdU1TVJdgdeCdw6kCglSZIkSZIkDcSkMwSranlVXd67vRJYDOzW2/0J4CSgOkcoSZIkSZIkDUqNTt82Qw1kDcEkewDPAxYmORpYVlVXDuLckiRJkiRJkgany5RhAJJsBXwDeC9j04j/lLHpwk923HxgPsCh2w8zNGde11AkSZIkSZKkJzaD1/abLp0yBJNswthg4BlVdQ4wD9gTuDLJzcBc4PIkT1/32KpaUFXDVTXsYKAkSZIkSZI0PSadIZgkwKnA4qr6OEBVXQXsNK7PzcBwVd3dMU5JkiRJkiSpuxm8tt906ZIheBDwZuDQJFf0tiMHFJckSZIkSZKkKTDpDMGqugTIk/TZY7LnlyRJkiRJkgbONQQHU2VYkiRJkiRJ0oahc5VhSZIkSZIkaYPhGoKTzxBMsnuSi5Jck+TqJCf02vdPcmlvTcGRJAcMLlxJkiRJkiRJXXTJEFwDvK+qLk8yB7gsyQXAR4APVtX3ekVGPgK8vHuokiRJkiRJUkeuIdipqMhyYHnv9soki4HdgAK27nXbBri9a5CSJEmSJEmSBmMgawgm2QN4HrAQeC/wL0k+xtiU5BcP4hqSJEmSJElSZ2YIdq8ynGQr4BvAe6vqfuDdwIlVtTtwInDq4xw3v7fG4MiilUu6hiFJkiRJkiSpQacBwSSbMDYYeEZVndNrPg547PbXgQmLilTVgqoarqrhoTnzuoQhSZIkSZIkqdGkpwwnCWPZf4ur6uPjdt0OvAy4GDgUuKFLgJIkSZIkSdLAlFOGu6wheBDwZuCqJFf02v4UeCfwqSQbA48A87uFKEmSJEmSJGlQulQZvgTI4+x+wWTPK0mSJEmSJE0Zi4p0LyoiSZIkSZIkacPRZcqwJEmSJEmStGFxDcHJZwgm2TzJj5NcmeTqJB/stZ+R5Loki5Kc1qtELEmSJEmSJGkG6DJleBVwaFXtB+wPHJHkQOAM4NnAbwBbAO/oHKUkSZIkSZI0CKOj07fNUJMeEKwxD/TubtLbqqq+29tXwI+BuQOIU5IkSZIkSfqVkmTfJFeM2+5P8t51+rw8yX3j+vxl1+t2WkMwySzgMmAv4O+qauG4fZsAbwZO6BShJEmSJEmSNCgzaA3BqrqOsZm3j42zLQPOnaDrD6vqqEFdt1OV4apaW1X7M5YFeECSoXG7Pwf8oKp+ONGxSeYnGUkysmjlki5hSJIkSZIkSRu6w4AlVXXLVF+o04DgY6rqXuAi4AiAJKcATwP++AmOWVBVw1U1PDRn3iDCkCRJkiRJkp7YzF1D8FjgzMfZ96JeYd/vJfn1bg9AtyrDT0uybe/2FsDhwLVJ3gH8FvCGqhmUgylJkiRJkiRNo/EzZHvb/MfptynwWuDrE+y+HHhmr7DvZ4Bvdo2ryxqCuwCn9+Y3bwScVVXfSbIGuAX4URKAc6rqr7sGKkmSJEmSJHU2jdV/q2oBsKCh66uAy6vqzgnOcf+4299N8rkkO1bV3ZONa9IDglX1U+B5E7R3KlQiSZIkSZIkPcW8gceZLpzk6cCdVVVJDmAsMe/nXS7m4J0kSZIkSZKeOqrWdwS/JMmWjC3F9wfj2t4FUFVfAI4B3t2blfswcGxVt3+EA4KSJEmSJEnSelJVDwI7rNP2hXG3Pwt8dpDX7FJUZPMkP+5VOLk6yQd77UnyN0muT7I4yR8NLlxJkiRJkiRJXXTJEFwFHFpVDyTZBLgkyfeA5wC7A8+uqtEkOw0iUEmSJEmSJKmzaSwqMlN1KSpSwAO9u5v0tgLeDfxeVY32+q3oGqQkSZIkSZKkwZj0lGGAJLOSXAGsAC6oqoXAPOB3k4wk+V6SvQcRqCRJkiRJktTZ6Oj0bTNUpwHBqlpbVfsDc4EDkgwBmwGPVNUw8A/AaRMdm2R+b9BwZNHKJV3CkCRJkiRJktSo04DgY6rqXuAi4AhgKXBOb9e5wHMf55gFVTVcVcNDc+YNIgxJkiRJkiTpidXo9G0zVJcqw09Lsm3v9hbA4cC1wDeBQ3rdXgZc3zVISZIkSZIkSYPRpcrwLsDpSWYxNrB4VlV9J8klwBlJTmSs6Mg7BhCnJEmSJEmS1N0MXttvunSpMvxT4HkTtN8LvLpLUJIkSZIkSZKmRpcMQUmSJEmSJGnDUrW+I1jvBlJURJIkSZIkSdKGoXOGYG8NwRFgWVUdlWRP4KvADsBlwJur6tGu15EkSZIkSZI6cw3BgWQIngAsHnf/w8Anqmov4BfA2wdwDUmSJEmSJEkD0GlAMMlcxgqIfLF3P8ChwNm9LqcDr+tyDUmSJEmSJGlgRkenb5uhumYIfhI4CXjsX7gDcG9VrendXwrs1vEakiRJkiRJkgZk0gOCSY4CVlTVZZM8fn6SkSQji1YumWwYkiRJkiRJUrsanb5thuqSIXgQ8NokNzNWRORQ4FPAtkkeK1YyF1g20cFVtaCqhqtqeGjOvA5hSJIkSZIkSWo16QHBqjq5quZW1R7AscC/VdUbgYuAY3rdjgO+1TlKSZIkSZIkaQBqtKZtm6kGUWV4Xe8H/jjJjYytKXjqFFxDkiRJkiRJ0iRs/ORdnlxVXQxc3Lt9E3DAIM4rSZIkSZIkabAGMiAoSZIkSZIkbRBGZ26xj+kyFVOGJUmSJEmSJM1QnQcEk8xK8pMk31mn/dNJHuh6fkmSJEmSJGlganT6thlqEBmCJwCLxzckGQa2G8C5JUmSJEmSJA1QpwHBJHOBVwNfHNc2C/gocFK30CRJkiRJkqQBG63p22aorhmCn2Rs4G98DuQfAt+uquVPdGCS+UlGkowsWrmkYxiSJEmSJEmSWkx6QDDJUcCKqrpsXNuuwG8Dn3my46tqQVUNV9Xw0Jx5kw1DkiRJkiRJajc6On3bDLVxh2MPAl6b5Ehgc2Br4GpgFXBjEoDZSW6sqr06RypJkiRJkiSps0kPCFbVycDJAEleDvyPqjpqfJ8kDzgYKEmSJEmSpBljBmfuTZdBVBmWJEmSJEmStIHoMmX4/6qqi4GLJ2jfahDnlyRJkiRJkgaiZm713+lihqAkSZIkSZL0FDKQDEFJkiRJkiRpg+Aagt0zBJPMSvKTJN/p3T8syeVJrkhySRKLikiSJEmSJEkzxCCmDJ8ALB53//PAG6tqf+ArwJ8P4BqSJEmSJElSd6M1fdsM1WlAMMlc4NXAF8c1F7B17/Y2wO1driFJkiRJkiRpcLquIfhJ4CRgzri2dwDfTfIwcD9w4EQHJpkPzAc4dPthhubM6xiKJEmSJEmSpCcz6QzBJEcBK6rqsnV2nQgcWVVzgS8BH5/o+KpaUFXDVTXsYKAkSZIkSZKmRY1O3zZDdckQPAh4bZIjgc2BrZP8M/DsqlrY6/M14LyOMUqSJEmSJEkakElnCFbVyVU1t6r2AI4F/g04GtgmyT69bofzywVHJEmSJEmSpPXHoiKd1xD8JVW1Jsk7gW8kGQV+AbxtkNeQJEmSJEmSNHkDGRCsqouBi3u3zwXOHcR5JUmSJEmSpEGq0Zm7tt90mfSUYUmSJEmSJEkbnk4ZgkluBlYCa4E1VTWc5KPAa4BHgSXAW6vq3q6BSpIkSZIkSZ3N4LX9pssgMgQPqar9q2q4d/8CYKiqngtcD5w8gGtIkiRJkiRJGoCBFhUBqKrzx929FDhm0NeQJEmSJEmSJqVcQ7BrhmAB5ye5LMn8Cfa/Dfhex2tIkiRJkiRJGpCuGYIvqaplSXYCLkhybVX9ACDJnwFrgDMmOrA3gDgf4NDthxmaM69jKJIkSZIkSdKTcA3BbhmCVbWs93MFcC5wAECStwBHAW+sqgkf5apaUFXDVTXsYKAkSZIkSZI0PSadIZhkS2CjqlrZu/1K4K+THAGcBLysqh4aUJySJEmSJElSd6OuIdhlyvDOwLlJHjvPV6rqvCQ3ApsxNoUY4NKqelfnSCVJkiRJkiR1NukBwaq6Cdhvgva9OkUkSZIkSZIkTRXXEOxcZViSJEmSJEnSBsQBQUmSJEmSJOkppMsagiS5GVgJrAXWVNVwr/144D299n+uqpM6xilJkiRJkiR1VxYV6TQg2HNIVd392J0khwBHA/tV1aokOw3gGpIkSZIkSZIGYBADgut6N/ChqloFUFUrpuAakiRJkiRJUv8sKtJ5DcECzk9yWZL5vbZ9gIOTLEzy/SQv7HgNSZIkSZIkSQPSNUPwJVW1rDct+IIk1/bOuT1wIPBC4Kwkz6qqXxp+7Q0gzgc4dPthhubM6xiKJEmSJEmS9MRq1DUEO2UIVtWy3s8VwLnAAcBS4Jwa82NgFNhxgmMXVNVwVQ07GChJkiRJkiRNj0kPCCbZMsmcx24DrwQWAd8EDum17wNsCtz9eOeRJEmSJEmSps1oTd82Q3WZMrwzcG6Sx87zlao6L8mmwGlJFgGPAsetO11YkiRJkiRJ0vox6QHBqroJ2G+C9keBN3V9qB2VAAAgAElEQVQJSpIkSZIkSZoSMzhzb7p0rTIsSZIkSZIkaQPStcqwJEmSJEmStOEoqwx3yhBMsm2Ss5Ncm2Rxkhcl2T7JBUlu6P3cblDBSpIkSZIkSeqm65ThTwHnVdWzGVtPcDHwAeDCqtobuLB3X5IkSZIkSVr/rDI8+QHBJNsALwVOhbFiIlV1L3A0cHqv2+nA67oGKUmSJEmSJGkwuqwhuCdwF/ClJPsBlwEnADtX1fJenzuAnbuFKEmSJEmSJA1GzeDMvenSZcrwxsDzgc9X1fOAB1lnenBVFTDho5xkfpKRJCOLVi7pEIYkSZIkSZKkVl0GBJcCS6tqYe/+2YwNEN6ZZBeA3s8VEx1cVQuqariqhofmzOsQhiRJkiRJkqRWkx4QrKo7gNuS7NtrOgy4Bvg2cFyv7TjgW50ilCRJkiRJkgbFoiKd1hAEOB44I8mmwE3AWxkbZDwryduBW4Df6XgNSZIkSZIkSQPSaUCwqq4AhifYdViX80qSJEmSJElTYnR0fUew3nVZQ1CSJEmSJEnSBqbrlGFJkiRJkiRpwzGD1/abLp0yBJNsm+TsJNcmWZzkReP2vS9JJdmxe5iSJEmSJEmSBqFrhuCngPOq6pheYZHZAEl2B14J3Nrx/JIkSZIkSdLgmCE4+QzBJNsALwVOBaiqR6vq3t7uTwAnAT7CkiRJkiRJ0gzSJUNwT+Au4EtJ9gMuA04AXgEsq6orkwwgREmSJEmSJGkwqsxf67KG4MbA84HPV9XzgAeBvwL+FPjLJzs4yfwkI0lGFq1c0iEMSZIkSZIkSa26DAguBZZW1cLe/bMZGyDcE7gyyc3AXODyJE9f9+CqWlBVw1U1PDRnXocwJEmSJEmSpEajNX3bDDXpAcGqugO4Lcm+vabDgMuraqeq2qOq9mBs0PD5vb6SJEmSJEmS1rOuVYaPB87oVRi+CXhr95AkSZIkSZKkKTKDM/emS6cBwaq6Ahh+gv17dDm/JEmSJEmSpMHqmiEoSZIkSZIkbTDKDMFORUUkSZIkSZIkbWA6DQgm2TbJ2UmuTbI4yYuS7J/k0iRXJBlJcsCggpUkSZIkSZLUTdcpw58CzquqY3qFRWYDZwEfrKrvJTkS+Ajw8o7XkSRJkiRJkrpzyvDkBwSTbAO8FHgLQFU9CjyapICte922AW7vGKMkSZIkSZKkAemSIbgncBfwpST7AZcBJwDvBf4lyccYm5L84s5RSpIkSZIkSYMwur4DWP+6rCG4MfB84PNV9TzgQeADwLuBE6tqd+BE4NSJDk4yv7fG4MiilUs6hCFJkiRJkiRtmJLcnOSqx+pxTLA/ST6d5MYkP03y/K7X7DIguBRYWlULe/fPZmyA8DjgnF7b14EJi4pU1YKqGq6q4aE58zqEIUmSJEmSJLWp0Zq2rQ+HVNX+VTU8wb5XAXv3tvnA57s+BpMeEKyqO4DbkuzbazoMuIaxNQNf1ms7FLihU4SSJEmSJEnSU9fRwD/WmEuBbZPs0uWEXasMHw+c0aswfBPwVuBbwKeSbAw8wtjIpSRJkiRJkrT+zbwqwwWc3yvU+/dVtWCd/bsBt427v7TXtnyyF+w0IFhVVwDrpjJeArygy3klSZIkSZKkDV2S+fxystyCCQb8XlJVy5LsBFyQ5Nqq+sFUxtU1Q1CSJEmSJEnacExjleHe4N+6A4Dr9lnW+7kiybmM1eMYPyC4DNh93P25vbZJ61JURJIkSZIkSdIkJdkyyZzHbgOvBBat0+3bwO/3qg0fCNxXVZOeLgwdMgR7xUS+Nq7pWcBfMjaH+TXAo8AS4K1VdW+XICVJkiRJkqRB6LP671TbGTg3CYyN032lqs5L8i6AqvoC8F3gSOBG4CHGanh0MukBwaq6DtgfIMksxlIVzwX2BU6uqjVJPgycDLy/a6CSJEmSJEnSr5KqugnYb4L2L4y7XcB7BnndQa0heBiwpKpuAW4Z134pcMyAriFJkiRJkiR1M41rCM5Ug1pD8FjgzAna3wZ8b0DXkCRJkiRJktRR5wHBJJsCrwW+vk77nwFrgDMe57j5SUaSjCxauaRrGJIkSZIkSdKTqtGatm2mGkSG4KuAy6vqzscakrwFOAp4Y2+e839RVQuqariqhofmzBtAGJIkSZIkSZKezCDWEHwD46YLJzkCOAl4WVU9NIDzS5IkSZIkSRqQTgOCSbYEDgf+YFzzZ4HNgAt6JZMvrap3dbmOJEmSJEmSNBAWFek2IFhVDwI7rNO2V6eIJEmSJEmSJE2ZQUwZliRJkiRJkjYIZYbgQIqKSJIkSZIkSdpATHpAMMm+Sa4Yt92f5L29fccnuTbJ1Uk+MrhwJUmSJEmSpA5Gp3GboSY9ZbiqrgP2B0gyC1gGnJvkEOBoYL+qWpVkp4FEKkmSJEmSJKmzQa0heBiwpKpuSfJR4ENVtQqgqlYM6BqSJEmSJElSJ64hOLg1BI8Fzuzd3gc4OMnCJN9P8sKJDkgyP8lIkpFFK5cMKAxJkiRJkiRJT6TzgGCSTYHXAl/vNW0MbA8cCPwJcFaSrHtcVS2oquGqGh6aM69rGJIkSZIkSdKTcw3BgWQIvgq4vKru7N1fCpxTY37M2D9/xwFcR5IkSZIkSVJHg1hD8A38v+nCAN8EDgEuSrIPsClw9wCuI0mSJEmSJHXiGoIdMwSTbAkcDpwzrvk04FlJFgFfBY6rqupyHUmSJEmSJEmD0SlDsKoeBHZYp+1R4E1dzitJkiRJkiRNBTMEB1dlWJIkSZIkSdIGoOuU4ROTXJ1kUZIzk2yeZM8kC5PcmORrvSrEkiRJkiRJ0npXo9O3zVSTHhBMshvwR8BwVQ0Bs4BjgQ8Dn6iqvYBfAG8fRKCSJEmSJEmSuus6ZXhjYIskGwOzgeXAocDZvf2nA6/reA1JkiRJkiRJAzLpoiJVtSzJx4BbgYeB84HLgHurak2v21Jgt85RSpIkSZIkSYNQWd8RrHddpgxvBxwN7AnsCmwJHNHH8fOTjCQZWbRyyWTDkCRJkiRJktSHLlOGXwH8rKruqqrVwDnAQcC2vSnEAHOBZRMdXFULqmq4qoaH5szrEIYkSZIkSZLUxqIi3QYEbwUOTDI7SYDDgGuAi4Bjen2OA77VLURJkiRJkiRJg9JlDcGFSc4GLgfWAD8BFgD/DHw1yf/stZ06iEAlSZIkSZKkrmrUNQQnPSAIUFWnAKes03wTcECX80qSJEmSJEmaGp0GBCVJkiRJkqQNyUxe22+6dFlDUJIkSZIkSdIGptOAYJITk1ydZFGSM5NsPm7fp5M80D1ESZIkSZIkaTCqMm3bTDXpAcEkuwF/BAxX1RAwCzi2t28Y2G4gEUqSJEmSJEkamK5rCG4MbJFkNTAbuD3JLOCjwO8B/63j+SVJkiRJkqSBcQ3BDhmCVbUM+BhwK7AcuK+qzgf+EPh2VS0fTIiSJEmSJEmSBqXLlOHtgKOBPYFdgS2T/D7w28BnGo6fn2QkyciilUsmG4YkSZIkSZLUrEYzbdtM1aWoyCuAn1XVXVW1GjgH+CCwF3BjkpuB2UlunOjgqlpQVcNVNTw0Z16HMCRJkiRJkiS16rKG4K3AgUlmAw8DhwEfr6r/mx2Y5IGq2qtjjJIkSZIkSdJAVK3vCNa/LmsILgTOBi4Hruqda8GA4pIkSZIkSZI0BTpVGa6qU4BTnmD/Vl3OL0mSJEmSJGmwOg0ISpIkSZIkSRuSmVzsY7p0KSoiSZIkSZIkaQPTKUMwyYnAO4BibB3BtwIHAR9lbLDxAeAtVTVhpWFJkiRJkiRpOpkh2CFDMMluwB8Bw1U1BMwCjgU+D7yxqvYHvgL8+SAClSRJkiRJktRd1zUENwa2SLIamA3czli24Na9/dv02iRJkiRJkqT1rmp9R7D+TXpAsKqWJfkYcCvwMHB+VZ2f5B3Ad5M8DNwPHDiYUCVJkiRJkiR11WXK8HbA0cCewK7AlkneBJwIHFlVc4EvAR9/nOPnJxlJMrJo5ZLJhiFJkiRJkiQ1q9FM2zZTdaky/ArgZ1V1V1WtBs5hrKDIflW1sNfna8CLJzq4qhZU1XBVDQ/NmdchDEmSJEmSJEmtugwI3gocmGR2kgCHAdcA2yTZp9fncGBxxxglSZIkSZKkgajKtG0zVZc1BBcmORu4HFgD/ARYACwFvpFkFPgF8LZBBCpJkiRJkiSpu05VhqvqFOCUdZrP7W2SJEmSJEnSjFKj6zuC9a/LlGFJkiRJkiRJG5hOGYKSJEmSJEnShmR0Bq/tN106ZQgmOSHJoiRXJ3lvr+2jSa5N8tMk5ybZdjChSpIkSZIkSepq0gOCSYaAdwIHAPsBRyXZC7gAGKqq5wLXAycPIlBJkiRJkiSpK6sMd8sQfA6wsKoeqqo1wPeB11fV+b37AJcCc7sGKUmSJEmSJGkwugwILgIOTrJDktnAkcDu6/R5G/C9DteQJEmSJEmSNECTLipSVYuTfBg4H3gQuAJY+9j+JH8GrAHOmOj4JPOB+QCHbj/M0Jx5kw1FkiRJkiRJalKjM3cq73TpVFSkqk6tqhdU1UuBXzC2ZiBJ3gIcBbyxqupxjl1QVcNVNexgoCRJkiRJkjQ9Jp0hCJBkp6pakeQZwOuBA5McAZwEvKyqHhpEkJIkSZIkSdIgTJy69tTSaUAQ+EaSHYDVwHuq6t4knwU2Ay5IAnBpVb2r43UkSZIkSZIkDUCnAcGqOniCtr26nFOSJEmSJEmaKq4h2HENQUmSJEmSJEkblq5ThiVJkiRJkqQNxmiZIdgpQzDJCUkWJbk6yXvHtR+f5Npe+0e6hylJkiRJkiRpECadIZhkCHgncADwKHBeku8AuwNHA/tV1aokOw0kUkmSJEmSJKmjMkOw05Th5wALq+ohgCTfB14PDAMfqqpVAFW1onOUkiRJkiRJkgaiy5ThRcDBSXZIMhs4krHswH167QuTfD/JCwcRqCRJkiRJktRV1fRtM9WkBwSrajHwYeB84DzgCmAtY1mH2wMHAn8CnJXkv+RiJpmfZCTJyKKVSyYbhiRJkiRJkqQ+dCoqUlWnVtULquqlwC+A64GlwDk15sfAKLDjBMcuqKrhqhoemjOvSxiSJEmSJElSk9HKtG0zVZc1BEmyU1WtSPIMxtYPPJCxAcBDgIuS7ANsCtzdOVJJkiRJkiRJnXUaEAS+kWQHYDXwnqq6N8lpwGlJFjFWffi4qpk8a1qSJEmSJElPFVYZ7jggWFUHT9D2KPCmLueVJEmSJEmSNDU6rSEoSZIkSZIkacPSdcqwJEmSJEmStMFwYbvGDMEkpyVZ0VsX8LG27ZNckOSG3s/teu1J8ukkNyb5aZLnT1XwkiRJkiRJkvrTOmX4y8AR67R9ALiwqvYGLuzdB3gVsHdvmw98vnuYkiRJkiRJUnejlWnbZqqmAcGq+gFwzzrNRwOn926fDrxuXPs/1phLgW2T7DKIYCVJkiRJkiR102UNwZ2rannv9h3Azr3buwG3jeu3tNe2HEmSJEmSJGk9qhmcuTddBlJluKoK6GtJxiTzk4wkGVm0cskgwpAkSZIkSZL0JLoMCN752FTg3s8VvfZlwO7j+s3ttf2SqlpQVcNVNTw0Z16HMCRJkiRJkqQ2riHYbUDw28BxvdvHAd8a1/77vWrDBwL3jZtaLEmSJEmSJGk9alpDMMmZwMuBHZMsBU4BPgScleTtwC3A7/S6fxc4ErgReAh464BjliRJkiRJkialrzXvfkU1DQhW1RseZ9dhE/Qt4D1dgpIkSZIkSZI0NbpUGZYkSZIkSZI2KDN5bb/pMpAqw5IkSZIkSZI2DE0DgklOS7IiyaJxbdsnuSDJDb2f261zzAuTrElyzKCDliRJkiRJkiajKtO2zVStGYJfBo5Yp+0DwIVVtTdwYe8+AElmAR8Gzh9AjJIkSZIkSZIGpGlAsKp+ANyzTvPRwOm926cDrxu373jgG8CKrgFKkiRJkiRJgzI6jdtM1WUNwZ2rannv9h3AzgBJdgP+G/D5Jzo4yfwkI0lGFq1c0iEMSZIkSZIkSa0GUlSkqgqo3t1PAu+vqiccCK2qBVU1XFXDQ3PmDSIMSZIkSZIk6QkVmbZtpuoyIHhnkl0Aej8fmx48DHw1yc3AMcDnkrxu4lNIkiRJkiRJT01Jdk9yUZJrklyd5IQJ+rw8yX1Jruhtf9n1uht3OPbbwHHAh3o/vwVQVXs+1iHJl4HvVNU3O1xHkiRJkiRJ+lW0BnhfVV2eZA5wWZILquqadfr9sKqOGtRFmwYEk5wJvBzYMclS4BTGBgLPSvJ24BbgdwYVlCRJkiRJkjQVRuvJ+0yXXn2O5b3bK5MsBnYD1h0QHKimAcGqesPj7DrsSY57S78BSZIkSZIkSU81SfYAngcsnGD3i5JcCdwO/I+qurrLtbpMGZYkSZIkSZI2KKPTWOwjyXxg/rimBVW1YIJ+WwHfAN5bVfevs/ty4JlV9UCSI4FvAnt3icsBQUmSJEmSJGkK9Ab//ssA4HhJNmFsMPCMqjpngnPcP+72d5N8LsmOVXX3ZONqqjKc5LQkK5IsGte2fZILktzQ+7ldr32bJP+U5MpedZS3TjY4SZIkSZIkaZCKTNv2ZJIEOBVYXFUff5w+T+/1I8kBjI3n/bzLY9A0IAh8GThinbYPABdW1d7Ahb37AO8Brqmq/RgrRPK/k2zaJUhJkiRJkiTpV9BBwJuBQ5Nc0duOTPKuJO/q9TkGWNRbQ/DTwLFV1ak0yv/P3r3Hy1XV9/9/vTWCIgEUijdoo1GwNVy+ekTb6gMExdRWEZVqtBWUNt5Qa62gtV/Qqq13qw+qfqPEVLHx0qKioEgvNq0FNSIkJ4JKvAasEbEa608Uz+f3x+zUcTInZ85cTvaB15PHfmTvtT6z53NmzY01a6816KIiG5qJDbudRKfDD+DvgE8BZwEFLG16LvcFbqSzhLIkSZIkSZK0R83s6QS6VNV/wO6HElbVucC547zfQUcI9nOXZmlkgP8C7tLsnwv8Op1VTzYDz6+qXR7rJKuTbEyycXrH1hHSkCRJkiRJkjSoUToE/1czTHHnUMVHAlcCdweOBs5Nsl+f26ypqqmqmlqxdPk40pAkSZIkSZJ2q01zCO4po3QIfifJ3QCaf7c35U8DLqiOa4GvAfcdLU1JkiRJkiRJ4zBKh+CFwKnN/qnAR5r9bwInACS5C3A48NUR7keSJEmSJEkai5kF3NpqoEVFkqyns4DIQUm2AecArwY+kOR04BvA7zfhrwDWJdlMZ1LEs6rqhnEnLkmSJEmSJGn+Bl1leNUsVSf0ib0eOHGUpCRJkiRJkqRJaPPIvYUylkVFJEmSJEmSJC0Oc3YIJlmbZHuS6a6yU5JsSTKTZKqr/BFJPp9kc/Pv8ZNKXJIkSZIkSZovVxkebITgOmBlT9k08DhgQ0/5DcCjq+oIOguNvGfUBCVJkiRJkiSNz5xzCFbVhiTLesquBkjSG/uFrsMtwB2S7F1VN42cqSRJkiRJkqSRDbSoyJAeD1xhZ6AkSZIkSZLaYqa9V/IumIksKpLkfsBrgGfsJmZ1ko1JNk7v2DqJNCRJkiRJkiT1GHuHYJJDgA8BT62qWXv6qmpNVU1V1dSKpcvHnYYkSZIkSZK0ixmyYFtbjbVDMMkBwEXAi6vq0+M8tyRJkiRJkqTRzdkhmGQ9cBlweJJtSU5PcnKSbcBvAhcluaQJPwO4N3B2kiub7eCJZS9JkiRJkiTNQy3g1laDrDK8apaqD/WJfSXwylGTkiRJkiRJkjQZk1xlWJIkSZIkSWqVmT2dQAtMZJVhSZIkSZIkSe00yByCa5NsTzLdVXZKki1JZpJM9cQfmeSypn5zkttPInFJkiRJkiRpvmaSBdvaapARguuAlT1l08DjgA3dhUmWAOcDz6yq+wHHAT8bOUtJkiRJkiRJYzHIoiIbkizrKbsaILv2dJ4IbKqqq5q4740lS0mSJEmSJGkM2rz670IZ9xyChwGV5JIkVyQ5c8znlyRJkiRJkjSCcXcILgEeAjyl+ffkJCf0C0yyOsnGJBund2wdcxqSJEmSJEnSrmYWcGurcXcIbgM2VNUNVfVj4GLg/v0Cq2pNVU1V1dSKpcvHnIYkSZIkSZKkfsbdIXgJcESSfZoFRo4Fvjjm+5AkSZIkSZKGMpOF29pqzg7BJOuBy4DDk2xLcnqSk5NsA34TuCjJJQBV9X3gjcDngCuBK6rqosmlL0mSJEmSJGk+BllleNUsVR+aJf584PxRkpIkSZIkSZI0GXN2CEqSJEmSJEm3FDO0+FreBTLuOQQlSZIkSZIktdggcwiuTbI9yXRX2euSXJNkU5IPJTmgq+4lSa5N8qUkj5xU4pIkSZIkSdJ81QJubTXICMF1wMqeskuBFVV1JPBl4CUASX4DeBJwv+Y2b01y27FlK0mSJEmSJGkkc3YIVtUG4Maesk9W1c3N4eXAIc3+ScD7quqmqvoacC1wzBjzlSRJkiRJkoY2k4Xb2moccwg+Hfh4s38P4FtddduaMkmSJEmSJEktMFKHYJKXAjcD7x3itquTbEyycXrH1lHSkCRJkiRJkgYys4BbWw3dIZjkNOD3gKdU1c55Eq8DDu0KO6Qp20VVramqqaqaWrF0+bBpSJIkSZIkSZqHoToEk6wEzgQeU1U/7qq6EHhSkr2T3BO4D/DZ0dOUJEmSJEmSRucqw7BkroAk64HjgIOSbAPOobOq8N7ApUkALq+qZ1bVliQfAL5I51Li51TVzyeVvCRJkiRJkqT5mbNDsKpW9Sk+bzfxrwJeNUpSkiRJkiRJ0iS0efXfhTKOVYYlSZIkSZIkLRJzjhCUJEmSJEmSbinavPrvQplzhGCStUm2J5nuKntdkmuSbEryoSQH9NzmV5P8KMmfTSJpSZIkSZIkScMZ5JLhdcDKnrJLgRVVdSTwZTqLjHR7I/DxkbOTJEmSJEmSxmhmAbe2mrNDsKo2ADf2lH2yqm5uDi8HDtlZl+SxwNeALWPMU5IkSZIkSdIYjGNRkafTjAZMsi9wFvDyMZxXkiRJkiRJ0piN1CGY5KXAzcB7m6KXAW+qqh8NcNvVSTYm2Ti9Y+soaUiSJEmSJEkDqSzc1lZDrzKc5DTg94ATqqqa4gcBT0jyWuAAYCbJT6rq3N7bV9UaYA3A85c9qXrrJUmSJEmSJI3fUB2CSVYCZwLHVtWPd5ZX1UO7Yl4G/KhfZ6AkSZIkSZK0J7R5sY+FMuclw0nWA5cBhyfZluR04FxgKXBpkiuTvH3CeUqSJEmSJEkagzlHCFbVqj7F5w1wu5cNk5AkSZIkSZI0KY4QHM8qw5IkSZIkSZIWiaEXFZEkSZIkSZIWG1e2HWwOwbVJtieZ7ip7XZJrkmxK8qEkBzTlt0vyd0k2J7k6yUsmmbwkSZIkSZKk+RnkkuF1wMqeskuBFVV1JPBlYGfH3ynA3lV1BPAA4BlJlo0lU0mSJEmSJGlEM1m4ra3m7BCsqg3AjT1ln6yqm5vDy4FDdlYBd0yyBLgD8FPgh+NLV5IkSZIkSdIoxrGoyNOBjzf7/wD8D/Bt4JvA66vqxtluKEmSJEmSJC2kmQXc2mqkDsEkLwVuBt7bFB0D/By4O3BP4IVJ7jXLbVcn2Zhk4/SOraOkIUmSJEmSJGlAQ3cIJjkN+D3gKVW1c4GWJwOfqKqfVdV24NPAVL/bV9WaqpqqqqkVS5cPm4YkSZIkSZI0MEcIDtkhmGQlcCbwmKr6cVfVN4Hjm5g7Ag8Grhk1SUmSJEmSJEnjMWeHYJL1wGXA4Um2JTkdOBdYClya5Mokb2/C/xbYN8kW4HPAu6pq04RylyRJkiRJkualFnBrqyVzBVTVqj7F580S+yPglFGTkiRJkiRJkjQZ41hlWJIkSZIkSdIiMecIQUmSJEmSJOmWYiZ7OoM9b5A5BNcm2Z5kuqvsFUk2NfMHfjLJ3ZvypzTlm5P8Z5KjJpm8JEmSJEmSpPkZ5JLhdcDKnrLXVdWRVXU08DHg7Kb8a8CxVXUE8ApgzbgSlSRJkiRJkkY1s4BbWw2yqMiGJMt6yn7YdXhHmoVTquo/u8ovBw4ZPUVJkiRJkiRJ4zL0HIJJXgU8FfgB8LA+IacDHx/2/JIkSZIkSdK41Z5OoAWGXmW4ql5aVYcC7wXO6K5L8jA6HYJnzXb7JKuTbEyycXrH1mHTkCRJkiRJkjQPQ3cIdnkv8PidB0mOBN4JnFRV35vtRlW1pqqmqmpqxdLlY0hDkiRJkiRJ2r0ZasG2thqqQzDJfboOTwKuacp/FbgA+MOq+vLo6UmSJEmSJEkapznnEEyyHjgOOCjJNuAc4FFJDqezYMo3gGc24WcDBwJvTQJwc1VNTSBvSZIkSZIkad7avPrvQhlkleFVfYrPmyX2j4A/GjUpSZIkSZIkSZMx9CrDkiRJkiRJ0mLT3pn9Fs44FhWRJEmSJEmStEjM2SGYZG2S7Ummu8pekWRTkiuTfDLJ3bvqjmvKtyT5t0klLkmSJEmSJM3XzAJubTXICMF1wMqestdV1ZFVdTTwMTqLiZDkAOCtwGOq6n7AKWPMVZIkSZIkSdKIBllUZEOSZT1lP+w6vCO/uPz6ycAFVfXNJm77eNKUJEmSJEmSRjeTPZ3Bnjf0HIJJXpXkW8BTaEYIAocBd0ryqSSfT/LU3dx+dZKNSTZO79g6bBqSJEmSJEmS5mHoDsGqemlVHQq8FzijKV4CPAD4XeCRwP9Nctgst19TVVNVNbVi6fJh05AkSZIkSZI0D+NYZfi9wOOb/W3AJVX1P1V1A7ABOGoM9yFJkiRJkiSNbIZasK2thuoQTHKfrsOTgGua/Y8AD0myJMk+wIOAq0dLUZIkSZIkSdK4zLmoSJL1wHHAQUm2AecAj0pyOJ0VlL8BPBOgqq5O8glgU1P3zqqanlDukiRJkiRJ0ry0d9zewhlkleFVfYrP203864DXjZKUJEmSJEmSpMmYs28kRoYAACAASURBVENQkiRJkiRJuqWY2dMJtMA4FhWRJEmSJEmStEgM1CGYZG2S7Ul2mQ8wyQuTVJKDmuMkeUuSa5NsSnL/cSctSZIkSZIkDcNVhgcfIbgOWNlbmORQ4ETgm13FvwPcp9lWA28bLUVJkiRJkiRJ4zJQh2BVbQBu7FP1JuBMfnmBlpOAd1fH5cABSe42cqaSJEmSJEnSiGoBt7Yaeg7BJCcB11XVVT1V9wC+1XW8rSnrvf3qJBuTbJzesXXYNCRJkiRJkiTNw1Adgkn2Af4cOHvYO66qNVU1VVVTK5YuH/Y0kiRJkiRJ0sBmFnBrqyVD3m45cE/gqiQAhwBXJDkGuA44tCv2kKZMkiRJkiRJ0h42VIdgVW0GDt55nOTrwFRV3ZDkQuCMJO8DHgT8oKq+PY5kJUmSJEmSpFG0efXfhTLQJcNJ1gOXAYcn2Zbk9N2EXwx8FbgWeAfw7JGzlCRJkiRJkm6BkqxM8qUk1yZ5cZ/6vZO8v6n/TJJlo97nQCMEq2rVHPXLuvYLeM5oaUmSJEmSJEnj16bxgUluC/wt8Ag6C/N+LsmFVfXFrrDTge9X1b2TPAl4DfDEUe536FWGJUmSJEmSJI3kGODaqvpqVf0UeB9wUk/MScDfNfv/AJyQZlGPYQ16yfDaJNuTTPepe2GSSnJQT/kDk9yc5AmjJChJkiRJkiQtRklWJ9nYta3uCbkH8K2u421NWd+YqroZ+AFw4Ch5DbqoyDrgXODd3YVJDgVOBL7ZU35bOsMXPzlKcpIkSZIkSdI4zSzgfVXVGmDNAt7lQAYaIVhVG4Ab+1S9CTiTXS+/fi7wj8D2kbKTJEmSJEmSbrmuAw7tOj6kKesbk2QJsD/wvVHudOg5BJOcBFxXVVf1lN8DOBl42yiJSZIkSZIkSeNWC/jfAD4H3CfJPZPsBTwJuLAn5kLg1Gb/CcC/NIv6Dm2oDsEk+wB/Dpzdp/pvgLOqarcjMLuvoZ7esXWYNCRJkiRJkqRFq5kT8AzgEuBq4ANVtSXJXyZ5TBN2HnBgkmuBPwVePOr9DjqHYK/lwD2Bq5pFTQ4BrkhyDDAFvK8pPwh4VJKbq+rD3Sfovob6+cue1KYVnyVJkiRJknQLtZBzCA6iqi4GLu4pO7tr/yfAKeO8z6E6BKtqM3DwzuMkXwemquoGOh2FO8vXAR/r7QyUJEmSJEmStGcMdMlwkvXAZcDhSbYlOX2yaUmSJEmSJEnjN0Mt2NZWA40QrKpVc9Qvm6X8tPmnJEmSJEmSJGlShp1DUJIkSZIkSVp02jtub+EMtcqwJEmSJEmSpMVp0DkE1ybZnmS6T90Lk1SSg5rj/ZN8NMlVSbYkedq4k5YkSZIkSZKG4RyCg48QXAes7C1McihwIvDNruLnAF+sqqOA44A3JNlrtDQlSZIkSZIkjcNAHYJVtQG4sU/Vm4Az+eXLrwtYmiTAvs3tbh4xT0mSJEmSJGlkMwu4tdXQi4okOQm4rqqu6vT9/a9zgQuB64GlwBOrqs2PgSRJkiRJknSrMdSiIkn2Af4cOLtP9SOBK4G7A0cD5ybZr885VifZmGTj9I6tw6QhSZIkSZIkzUst4H9tNewqw8uBewJXJfk6cAhwRZK7Ak8DLqiOa4GvAfftPUFVramqqaqaWrF0+ZBpSJIkSZIkSZqPoS4ZrqrNwME7j5tOwamquiHJN4ETgH9PchfgcOCrY8hVkiRJkiRJ0ogGGiGYZD1wGXB4km1JTt9N+CuA30qyGfhn4KyqumH0VCVJkiRJkqTRuKjIgCMEq2rVHPXLuvavB04cLS1JkiRJkiRJkzD0KsOSJEmSJEnSYtPmxT4WyrCLikiSJEmSJElahObsEEyyNsn2JNNdZS9Lcl2SK5vtUU35I5J8Psnm5t/jJ5m8JEmSJEmSNB/OITjYCMF1wMo+5W+qqqOb7eKm7Abg0VV1BHAq8J7xpClJkiRJkiRpHOacQ7CqNiRZNsjJquoLXYdbgDsk2buqbhouPUmSJEmSJGl8Zso5BEeZQ/CMJJuaS4rv1Kf+8cAVdgZKkiRJkiRJ7TFsh+DbgOXA0cC3gTd0Vya5H/Aa4BmznSDJ6iQbk2yc3rF1yDQkSZIkSZKkwdUCbm01VIdgVX2nqn5eVTPAO4BjdtYlOQT4EPDUqpq1p6+q1lTVVFVNrVi6fJg0JEmSJEmSJM3TnHMI9pPkblX17ebwZGC6KT8AuAh4cVV9ejwpSpIkSZIkSeMx0+qxewtjzg7BJOuB44CDkmwDzgGOS3I0ndGPX+cXlwafAdwbODvJ2U3ZiVW1fcx5S5IkSZIkSRrCIKsMr+pTfN4ssa8EXjlqUpIkSZIkSdIklCMER1plWJIkSZIkSdIiM9QcgpIkSZIkSdJiNLOnE2iBOUcIJlmbZHuS6a6ylyW5LsmVzfaorrojk1yWZEuSzUluP6nkJUmSJEmSJM3PIJcMrwNW9il/U1Ud3WwXAyRZApwPPLOq7kdnMZKfjSlXSZIkSZIkSSMaZFGRDUmWDXi+E4FNVXVVc9vvDZ+aJEmSJEmSNF4zLioy0qIiZyTZ1FxSfKem7DCgklyS5IokZ44hR0mSJEmSJEljMmyH4NuA5cDRwLeBNzTlS4CHAE9p/j05yQn9TpBkdZKNSTZO79g6ZBqSJEmSJEnS4GoB/2uroToEq+o7VfXzqpoB3gEc01RtAzZU1Q1V9WPgYuD+s5xjTVVNVdXUiqXLh0lDkiRJkiRJ0jwN1SGY5G5dhycDO1cgvgQ4Isk+zQIjxwJfHC1FSZIkSZIkaTxmFnBrqzkXFUmyns5qwQcl2QacAxyX5GiggK8DzwCoqu8neSPwuabu4qq6aDKpS5IkSZIkSZqvQVYZXtWn+LzdxJ8PnD9KUpIkSZIkSdIkVLV3br+FMsoqw5IkSZIkSZIWmTlHCEqSJEmSJEm3FDMtXv13ocw5QjDJ2iTbk0z3lD83yTVJtiR5bVf5S5Jcm+RLSR45iaQlSZIkSZIkDWeQEYLrgHOBd+8sSPIw4CTgqKq6KcnBTflvAE8C7gfcHfinJIdV1c/HnbgkSZIkSZI0X21e/XehzDlCsKo2ADf2FD8LeHVV3dTEbG/KTwLeV1U3VdXXgGuBY8aYryRJkiRJkqQRDLuoyGHAQ5N8Jsm/JXlgU34P4FtdcduaMkmSJEmSJGmPqwX8r62G7RBcAtwZeDDwIuADSTKfEyRZnWRjko3TO7YOmYYkSZIkSZKk+Ri2Q3AbcEF1fJbO5dcHAdcBh3bFHdKU7aKq1lTVVFVNrVi6fMg0JEmSJEmSpMHNUAu2tdWwHYIfBh4GkOQwYC/gBuBC4ElJ9k5yT+A+wGfHkagkSZIkSZKk0c25ynCS9cBxwEFJtgHnAGuBtUmmgZ8Cp1ZVAVuSfAD4InAz8BxXGJYkSZIkSVJbdLqwbt3m7BCsqlWzVP3BLPGvAl41SlKSJEmSJEmSJmPYS4YlSZIkSZIkLUJzjhCUJEmSJEmSbilm9nQCLTDnCMEka5Nsb+YL7C5/bpJrkmxJ8tqeul9N8qMkfzbuhCVJkiRJkiQNb5ARguuAc4F37yxI8jDgJOCoqropycE9t3kj8PFxJSlJkiRJkiSNQ+GiIoMsKrIhybKe4mcBr66qm5qY7TsrkjwW+BrwP+NLU5IkSZIkSdI4DLuoyGHAQ5N8Jsm/JXkgQJJ9gbOAl48rQUmSJEmSJGlcZqgF29pq2A7BJcCdgQcDLwI+kCTAy4A3VdWP5jpBktVJNibZOL1j65BpSJIkSZIkSZqPYVcZ3gZcUFUFfDbJDHAQ8CDgCc0iIwcAM0l+UlXn9p6gqtYAawCev+xJ7e0ylSRJkiRJ0i1Gpzvr1m3YDsEPAw8D/jXJYcBewA1V9dCdAUleBvyoX2egJEmSJEmSpD1jzg7BJOuB44CDkmwDzgHWAmuTTAM/BU4tu1clSZIkSZLUcm2e22+hDLLK8KpZqv5gjtu9bJiEJEmSJEmSJE3OsJcMS5IkSZIkSYtOOUJw6FWGJUmSJEmSJC1Cc3YIJlmbZHszX2B3+XOTXJNkS7OqMElul+TvkmxOcnWSl0wqcUmSJEmSJGm+ZqoWbGurQS4ZXgecC7x7Z0GShwEnAUdV1U1JDm6qTgH2rqojkuwDfDHJ+qr6+njTliRJkiRJkjSMQRYV2ZBkWU/xs4BXV9VNTcz2neHAHZMsAe5AZwXiH44tW0mSJEmSJGkE7R23t3CGnUPwMOChST6T5N+SPLAp/wfgf4BvA98EXl9VN/Y7QZLVSTYm2Ti9Y+uQaUiSJEmSJEmaj2E7BJcAdwYeDLwI+ECSAMcAPwfuDtwTeGGSe/U7QVWtqaqpqppasXT5kGlIkiRJkiRJmo9B5hDsZxtwQVUV8NkkM8BBwJOBT1TVz4DtST4NTAFfHUu2kiRJkiRJ0ghmvGh46BGCHwYeBpDkMGAv4AY6lwkf35Tfkc4IwmtGT1OSJEmSJEnSOMw5QjDJeuA44KAk24BzgLXA2iTTdBYOObWqKsnfAu9KsgUI8K6q2jSx7CVJkiRJkqR5cITgYKsMr5ql6g/6xP4IOGXUpCRJkiRJkiRNxrBzCEqSJEmSJEmLTmdJjFu3YecQlCRJkiRJkrQIzdkhmGRtku3NfIE7y96f5Mpm+3qSK5vyRyT5fJLNzb/HTzJ5SZIkSZIkaT5mqAXb2mqQS4bXAecC795ZUFVP3Lmf5A3AD5rDG4BHV9X1SVYAlwD3GFu2kiRJkiRJkkYyyKIiG5Is61eXJMDvA8c3sV/oqt4C3CHJ3lV10+ipSpIkSZIkSaOpFo/cWyijziH4UOA7VfWVPnWPB66YrTMwyeokG5NsnN6xdcQ0JEmSJEmSJA1i1A7BVcD63sIk9wNeAzxjthtW1ZqqmqqqqRVLl4+YhiRJkiRJkjS3qlqwra0GmUOwryRLgMcBD+gpPwT4EPDUqnLonyRJkiRJktQiQ3cIAg8HrqmqbTsLkhwAXAS8uKo+PWpykiRJkiRJ0ji1efXfhTLnJcNJ1gOXAYcn2Zbk9KbqSex6ufAZwL2Bs5Nc2WwHjzVjSZIkSZIkSUMbZJXhVbOUn9an7JXAK0dPS5IkSZIkSRq/Ns/tt1BGXVREkiRJkiRJ0iIyyCXDa5NsTzLdVfb+rkuCv57kyq66I5NclmRLks1Jbj+p5CVJkiRJkiTNzyCLiqwDzgXevbOgqp64cz/JG4AfNPtLgPOBP6yqq5IcCPxsnAlLkiRJkiRJw3JRkcHmENyQZFm/uiQBfh84vik6EdhUVVc1t/3eeNKUJEmSJEmSNA6jziH4UOA7VfWV5vgwoJJckuSKJGeOeH5JkiRJkiRpbGoB/2urUTsEVwHru46XAA8BntL8e3KSE/rdMMnqJBuTbJzesXXENCRJkiRJkiQNYugOwWa+wMcB7+8q3gZsqKobqurHwMXA/fvdvqrWVNVUVU2tWLp82DQkSZIkSZKkgc1ULdjWVqOMEHw4cE1VbesquwQ4Isk+TYfhscAXR0lQkiRJkiRJ0vjM2SGYZD1wGXB4km1JTm+qnsQvXy5MVX0feCPwOeBK4Iqqumi8KUuSJEmSJEnDcQ7BwVYZXjVL+WmzlJ8PnD9aWpIkSZIkSZImYc4OQUmSJEmSJOmWos1z+3VL8jrg0cBPga3A06rqv/vEfR3YAfwcuLmqpuY696irDEuSJEmSJEkav0uBFVV1JPBl4CW7iX1YVR09SGcgDNghmGRtku1JprvKjk5yeZIrk2xMckxTniRvSXJtkk1J+q4yLEmSJEmSJC20xTKHYFV9sqpubg4vBw4Z+Y9vDDpCcB2wsqfstcDLq+po4OzmGOB3gPs022rgbaOnKUmSJEmSJN1qPR34+Cx1BXwyyeeTrB7kZAPNIVhVG5Is63Nn+zX7+wPXN/snAe+uqgIuT3JAkrtV1bcHuS9JkiRJkiRpUhZyDsGmg667k25NVa3pqv8n4K59bvrSqvpIE/NS4GbgvbPczUOq6rokBwOXJrmmqjbsLq9RFhX5E+CSJK+nM9Lwt5ryewDf6orb1pTZIShJkiRJkqRbjabzb81u6h++u9snOQ34PeCEZvBdv3Nc1/y7PcmHgGOA3XYIjrKoyLOAF1TVocALgPPmc+Mkq5u5BzdO79g6QhqSJEmSJEnSYBbLHIJJVgJnAo+pqh/PEnPHJEt37gMnAtP9YruN0iF4KnBBs/9BOr2PANcBh3bFHdKU/ZKqWlNVU1U1tWLp8hHSkCRJkiRJkm5xzgWW0rkM+MokbwdIcvckFzcxdwH+I8lVwGeBi6rqE3OdeJRLhq8HjgU+BRwPfKUpvxA4I8n7gAcBP3D+QEmSJEmSJGlwVXXvWcqvBx7V7H8VOGq+5x6oQzDJeuA44KAk24BzgD8G3pxkCfATfjFB4sVNUtcCPwaeNt+kJEmSJEmSpElYyEVF2mrQVYZXzVL1gD6xBTxnlKQkSZIkSZIkTcYolwxLkiRJkiRJi8qoi33cEoyyqIgkSZIkSZKkRWagDsEka5NsTzLdVXZ0ksubVU42Jjmm5zYPTHJzkieMO2lJkiRJkiRpGFUzC7a11aAjBNcBK3vKXgu8vKqOBs5ujgFIclvgNcAnx5CjJEmSJEmSpDEZdFGRDUmW9RYD+zX7+wPXd9U9F/hH4IEj5idJkiRJkiSNzYxzCI60qMifAJckeT2dkYa/BZDkHsDJwMOwQ1CSJEmSJElqlVEWFXkW8IKqOhR4AXBeU/43wFk1x4XSSVY3cw9unN6xdYQ0JEmSJEmSpMFU1YJtbTVKh+CpwAXN/geBnYuKTAHvS/J14AnAW5M8tvfGVbWmqqaqamrF0uUjpCFJkiRJkiRpUKNcMnw9cCzwKeB44CsAVXXPnQFJ1gEfq6oPj3A/kiRJkiRJ0lg4h+CAHYJJ1gPHAQcl2QacA/wx8OYkS4CfAKsnlaQkSZIkSZKk8Rh0leFVs1Q9YI7bnTbfhCRJkiRJkqRJafPcfgtllDkEJUmSJEmSJC0yo8whKEmSJEmSJC0qM44QHGyEYJK1SbYnme4qOzrJ5UmuTLIxyTFN+f5JPprkqiRbkjxtUslLkiRJkiRJmp9BLxleB6zsKXst8PKqOho4uzkGeA7wxao6is5CJG9IstfoqUqSJEmSJEka1aCLimxIsqy3GNiv2d8fuL6rfGmSAPsCNwI3j5ypJEmSJEmSNKLCS4ZHmUPwT4BLkryezkjD32rKzwUupNNBuBR4YlXNjJSlJEmSJEmSpLEYZZXhZwEvqKpDgRcA5zXljwSuBO4OHA2cm2S/3hsnWd3MPbhxesfWEdKQJEmSJEmSBlNVC7a11SgdgqcCFzT7HwSOafafBlxQHdcCXwPu23vjqlpTVVNVNbVi6fIR0pAkSZIkSZI0qFE6BK8Hjm32jwe+0ux/EzgBIMldgMOBr45wP5IkSZIkSdJYzFALtrXVQHMIJllPZ8Xgg5JsA84B/hh4c5IlwE+A1U34K4B1STYDAc6qqhvGnbgkSZIkSZKk+Rt0leFVs1Q9oE/s9cCJoyQlSZIkSZIkTUKb5/ZbKKNcMixJkiRJkiRpkRlohKAkSZIkSZJ0SzDjCMG5RwgmWZtke5LprrKjklyWZHOSjybZryl/RJLPN+WfT3L8JJOXJEmSJEmSND+DXDK8DljZU/ZO4MVVdQTwIeBFTfkNwKOb8lOB94wpT0mSJEmSJGlkVbVgW1vN2SFYVRuAG3uKDwM2NPuXAo9vYr/QLCoCsAW4Q5K9x5SrJEmSJEmSpBENu6jIFuCkZv8U4NA+MY8Hrqiqm4a8D0mSJEmSJGmsZqgF29pq2A7BpwPPTvJ5YCnw0+7KJPcDXgM8Y7YTJFmdZGOSjdM7tg6ZhiRJkiRJkqT5GGqV4aq6BjgRIMlhwO/urEtyCJ15BZ9aVbP29FXVGmANwPOXPam9XaaSJEmSJEm6xWjz3H4LZagRgkkObv69DfAXwNub4wOAi+gsOPLpcSUpSZIkSZIkaTzm7BBMsh64DDg8ybYkpwOrknwZuAa4HnhXE34GcG/g7CRXNtvBE8pdkiRJkiRJ0jzNeclwVa2aperNfWJfCbxy1KQkSZIkSZKkSZjxkuGhFxWRJEmSJEmStAgNtaiIJEmSJEmStBgVjhAcZA7BtUm2J5nuKjsqyWVJNif5aJL9uuqObOq2NPW3n1TykiRJkiRJkuZnkEuG1wEre8reSWcl4SOADwEvAkiyBDgfeGZV3Q84DvjZuJKVJEmSJEmSRjFTtWBbW83ZIVhVG4Abe4oPAzY0+5cCj2/2TwQ2VdVVzW2/V1U/H1OukiRJkiRJkkY07KIiW4CTmv1TgEOb/cOASnJJkiuSnDlqgpIkSZIkSdK4VNWCbW01bIfg04FnJ/k8sBT4aVO+BHgI8JTm35OTnNDvBElWJ9mYZOP0jq1DpiFJkiRJkiRpPobqEKyqa6rqxKp6ALAe2Nmjtw3YUFU3VNWPgYuB+89yjjVVNVVVUyuWLh8mDUmSJEmSJGleagH/a6uhOgSTHNz8exvgL4C3N1WXAEck2adZYORY4IvjSFSSJEmSJEnS6JbMFZBkPZ3Vgg9Ksg04B9g3yXOakAuAdwFU1feTvBH4HFDAxVV10SQSlyRJkiRJkuarzXP7LZQ5OwSratUsVW+eJf584PxRkpIkSZIkSZI0GXN2CEqSJEmSJEm3FI4QHH6VYUmSJEmSJEmLkCMEJUmSJEmSdKvh+EBHCEqSJEmSJEm3LlXV2g1YvZhi25JHG2Lbksdii21LHm2IbUsebYhtSx5tiG1LHostti15tCG2LXm0IbYtebQhti15LLbYtuTRhti25NGG2Lbk0YbYtuSx2GLbkkcbYid9brdb97bHE9htcrBxMcW2JY82xLYlj8UW25Y82hDbljzaENuWPNoQ25Y8FltsW/JoQ2xb8mhDbFvyaENsW/JYbLFtyaMNsW3Jow2xbcmjDbFtyWOxxbYljzbETvrcbrfuzUuGJUmSJEmSpFsROwQlSZIkSZKkW5G2dwiuWWSxbcmjDbFtyWOxxbYljzbEtiWPNsS2JY82xLYlj8UW25Y82hDbljzaENuWPNoQ25Y8FltsW/JoQ2xb8mhDbFvyaENsW/JYbLFtyaMNsZM+t27FUlV7OgdJkiRJkiRJC6TtIwQlSZIkSZIkjZEdgpIkSZIkSdKtiB2CkiRJkiRJ0q1IqzoEk9w3yVlJ3tJsZyX59TGd94Qk+/aUr+wTe0ySBzb7v5HkT5M8asD7efc8cnpIc+4T+9Q9KMl+zf4dkrw8yUeTvCbJ/j2xz0ty6ID3uVeSpyZ5eHP85CTnJnlOktv1ib9Xkj9L8uYkb0zyzJ15SW2S5OAJnvvASZ1bkvYk3zsXt0m1n223MGw/ac/wtSf9Qms6BJOcBbwPCPDZZguwPsmL53Gep/UcPw/4CPBcYDrJSV3Vf9UTew7wFuBtSf4aOBe4I/DiJC/tib2wZ/so8Lidx33y+mzX/h83514KnNPn71sL/LjZfzOwP/CapuxdPbGvAD6T5N+TPDvJr/R/ZKC57e8Cz0/yHuAU4DPAA4F39uT7PODtwO2b+r2BQ4HLkxy3m/tYtG6t/2OUZP8kr05yTZIbk3wvydVN2QHzOM/He473S/LXSd6T5Mk9dW/tOb5rkrcl+dskByZ5WZLNST6Q5G49sXfu2Q4EPpvkTknu3CevlV37+yc5L8mmJH+f5C49sa9OclCzP5Xkq3ReX99IcmxP7BVJ/iLJ8gEem6kk/5rk/CSHJrk0yQ+SfC7J/+mJ3TfJXybZ0sR8N8nlSU7rc94lSZ6R5BPN37QpycebzvtdOvnnyHFNz/Ftm3O/Islv99T9Rc/xPknOTPKiJLdPclrzXvja9PwQM8t9f3mW8iO79m/XPN4XJvmrJPv0xJ7R1Xb3TrIhyX8n+UySI/qc+4IkfzBgfvdKsjbJK5v2eUeS6SQfTLKsJ/Y2SZ6e5KIkVzXPk/f1e98cV/u1se2auom0Xxvaron3vfMX9YvtvXMibdeU7fH2a0PbdZ1v0bTfpNquiV9U7TfBtpvI515Ttsc/+9KCz70mflLfW3zt/aJ+Ub32pFlVVSs24MvA7fqU7wV8ZR7n+WbP8WZg32Z/GbAReH5z/IU+sbcF9gF+COzXlN8B2NQTewVwPnAccGzz77eb/WP75PWFrv3PAb/S7N8R2NwTe3X3/fTUXdl7XjoduycC5wHfBT4BnAos7Ynd1Py7BPgOcNvmOH3+vs1d9fsAn2r2f7X3cWvK9wdeDVwD3Ah8D7i6KTtgHu338Z7j/YC/Bt4DPLmn7q09x3cF3gb8LXAg8LLm7/gAcLee2Dv3bAcCXwfuBNy5J3Zlz995HrAJ+HvgLn3+hlcDBzX7U8BXgWuBb/Q+N5rn0V8Aywd4bKaAf22ed4cClwI/aJ5P/6cndl/gL4EtTcx3gcuB0/qc9xLgLOCuPY/lWcAne2LvP8v2AODbPbH/2DwWjwUubI73nuV5/Qk6nfYvbh7bs5q/8bnAR3piZ4Cv9Ww/a/79ap+/74qu/XcCrwR+DXgB8OHe533X/r8CD2z2DwM29sR+DXg98E06P2C8ALj7LG33WeB3gFXAt4AnNOUnAJf1xH4EOA04BPhT4P8C9wH+Dvirntj1dJ7zD27iD2n23wa8v08evc/77uf/tp7Yd9J5jv8J8Hngjbt5X/oA8AbgrcA/0/nB46HA64D39MTuoPP++sNmfwfw853lu2m7NwDr6LzHvgl4d0/slq79i4CTm/3jgE/3eSyuA/6BzvvVB4CTgb1mab8NwLPoPD+ngRfSeX6eDvxLT+y76Lz3PAT4Gzqvw0cA/wQ8d9j2W2xtN8n2a0Pb+d656N87J9J2bWm/NrTdYmy/SbXdYmy/CbbdRD73uh6rW+T3FubxudfET+p7i6+9Rfrac3ObbdvjCfxvIp2OpF/rU/5rwJd6yjbNsm0GbuqJ3dJzsKcMgwAADCVJREFUvG/zRvJG+nSu9dtvjntjb9O8mC8Fjm7KdvlC3RV/FZ3OpgP7vEn03tcHgac1++8Cppr9w4DP9cT2vkneDngMnQ/c7/bUTdPpYL0TnQ+xOzflt6erE7Ip28wv3ojv1J0zMN3n71tUHxBM6MNh52PXtd/6Dwh6Xl+7q6PzBehfmr+rd/v/5njNvBT4NJ3XQG/bdb/2ejv1e8/zwqatj+h+HHfzN1yxm3P1Hl8NLGn2L5+tXfuc96F0vlT+V/NYrJ7H39f7+r+q5/hzzb+3Aa7pqfvybv7uXeqa9vtqz/N+5/FPe2I3de0vAdYAF9AZLdz3/ZHOjwv/BaTruPfHhrcA76arM3229ut53K6k+dFolvN+qWu/931y02znpvOjwx8CF9PpOH8XcOII7deb1+XNv3uz6/vswO232Npuku3XhrbrzXmuOnzvbNt750Tari3t14a2W4ztN6m2W4ztN8G2m8jnXhO/xz/7aMHn3hDtN5/vLb72fnG8qF57bm6zbXs8gf9NBFbSGUX18eYNfE3z4r2WrhFaTex3gKPpdMp0b8uA63ti/4Wmw66rbAmdN/af95R/Btin2b9NV/n+vW9GXXWH0OnAO7f3RdsT93V+8SH2VZoRa3Q6KHvfYPan84vS1iannzW3+TfgqJ7YXUbrddXt03P8guY83wCeR+cXsXfQ6fw7pyf2+XQ61d5Bp7N2ZwflrwAb+tzXovqAYEIfDk3ZovqAAD4JnMkvf9G5C50O1X/qiZ0G7jPL4/StPo/DbXrKTqMzavEbs+ULvHJ3j1n98uvujXQuvd9dZ/w2Op2iL2ye/+mq6/0S9Nzm8Tiezq+lb6bzy+7L2fUX413eE+iMMF4JvKun/DI6o3hPofP6e2xTfiy7dhL/J/CQZv8xwCW7eS1d3pyz+/3qNsATgc/0ye8rwK8O2H67fJEAzqHz+vtKT/mVXftrd/dcbMoeQOc94HlNvn3br2mvxwGPZ9cvpb3P8VfRed+8F/DndEYI/BrwNOBjfc7dr/0OBJ7Jrr+ef55Oh/4xwA384keae/d5Dn2eZsQvnR86NnTVfXHY9ltsbdfVfiePu/2GbLsHjrPtmrK2v3f26wj3vXOCbdeW9ptg2/W7QqRv2y3G9ptk2y229ptg203kc68p2+OffUzoewvz+Nxr6mb73nKfPs+h+XxvWeyvvTexZ157C/G5dxK7ee25uc227fEEfimZzhvsg5s30cc3+7ftE3fezid/n7q/7zk+hK5Raz11v91zvPcscQfR1XE0S8zvMsTQXDqX495zlrr9gKPofAjtcmlqE3PYPO/v7jQj0IADgCcAx8wSe7+m/r4DnLftHxBDdyrN58OhKWvbB8RcX87uRGeOymuA79O5HOHqpqz38uknAIfP8jg9tuf4tcDD+8StZNcvZn9Jc2l/T/m9gX/YzfPuMXS+XP7XbmLO6dl2Xq5/V3ou32jKjwPeT+dy/M10foVdTc+UBsD75vG6O4rOKNqPA/dtnhP/3TyPf6tP7GebtviPnY83nc745/XELmty3U5n2oUvN/vvp8/7CvAcen5U6H7e9hyfT8+PMU35HwE/6yl75yzttxz4j1nu7zZ0vlj/Oz0/5HTFvKtnu0tX2/1zn/jT6PyIcgOdUdBfpDNX7P59Ynf5YWM37XcC8KXmdfEQOiOVv9I81if1xB5PZ8TvV+j8APSgrvZ77Szt992m7Xaec5f2W2xt18Stm2f7PW2Q9htj2/W+Z+1su2ubtnvwbG3XlPve+cvxxzH+986j2fW98/t03jt7v8P1vnce1tV+ve+dE2m7trRfG9quq00Gbb8j93T7LUTbTaj9Hjbu9pul7Wb73rKz7f57gLZbxoDfW5jH515Ttsc/++h81q1lzN9bmMfnXhM/qe8tO197V9N53d2SX3tnM/h75xX84rX3DMb/2hv5c8/NbbZtjyfgdsvY+OUvZ70fEHfqiW3VB8Q8Pxzm/GLW1O38gOj9crakJ27SX84G+oBozvfw3seP/l+s7kvni8Yosb8zrvPSmeNzxWyxY8x51Nhfn2fsoO3xIDq/AB8I/DbwZ8CjdvM8OoZfXMb+G3Q6u/vGjyH2d+nqQJ8l9qF0vnTNdt4HDZnD/eh04o/rsXhQz7lnfZyB3xz0vF23ObDZzt9dXM9t+r7/jCu2X9v1xN0N+N6g5x0ij/dM6Lwfo+eHpq660MwBO8R5H9o853a5fKtP7EOa58WejH0onTls54wd8tyTeCzGct7m9bx/s78Pne8PH6PznaX3f8QfxC/PKf2XwEf7xfY5927j+5z75QPG7kPn+9E/7Sa29+8b9LyTfix2d+7unHf3WDwPOHTA5+0ej+0XT9d3lzbmPMHYvejMc/4IOp95T6Fzdcxz2LUTZW/gqTT/HwA8mc4VWbvEDhC/17DnbnLujv1DOgMEnj1LzqcOed6n0JkHfRyPxV49eezucd6Lzg9ypwx47uXAi+hcSv0mOqMU95ulve9F5/vSm+kMwFjw2D7x/4/O4I3Zzr3z7xs0j0k8Fr05PGuA2LcM8li4ufVuO+dLkCYmydOq6l1tjk1yBzrD5acnlcN84xcqNp0VpZ9DpwP3aDqL7nykqbuiqu4/ZOxzgTMmEDtwDpM89xDnfTadDvNxxp5DZ17JJXTmMz0G+BSdL9qXVNWreh6L3vgH0bk8fZf4BYydNedJxbYo5wvZ1fF0Lkuiqh6zm9jQGRWyJ2NnzXcMf998YseS8xB/32er6phm/4/ovHd8mM5I7o9W1atnif3jJvZDezj22bPlO8vfd8aA557kYzFrzvPMdwudkUc3p7Na6f/QGUFzQlP+uN3E/pjO5P67xM43fsTYseS8wI/FuHL+QXOurXQWkfhgVd1AHz2x65vY7y5k7CzxH5hAzm14LP6ezo/ws8W+l87n4x3oLHp3Rzqv0xPo/Ah1ap/Yfej8AL4vnTkBd4kdIJ6qOm1MscPmPKnz7u6xGPVx7s35ecDv0Vm05FF0Bj78N53pQZ5dVZ9qU2xX/KPpTL01zjyeT+eH70XzWEh9zdZT6OY2ro3dzK14a4ptSx69scx/Je5FE9uWPCYcO9DK6PONvyXHtiUP5rFaPZ0veXs6duB8J3nuNjwWva9HOqu97xw9fkd2nS92UcW2JY8Jxl7dtd87H/EuE8cPGjvJcy+22Ann8QU6l2+eSGcaoe/SmRf6VGBp22LbkkdLYjc1/y6hMyf8bZvjfotuDBw7yXMvttgJ57G5q34f4P9v7+5Vo4qiKI6vPSQKQfwoLEyhnQiWCda+hqVPIBZWFnkB38BX8AkEwfQWmsomGH0AC1FTGLMtzinCfMicmO3d597/Dw4ZZtYcVmZISC537n5bb9/Wir9Rh8xm6ZEhy2KtWhsCLoCZHax6SOVagpPIZunR2Hnm7t8lyd2PzOyhpFdmdqfme85m6RGVPXH335J+mtmhu3+rzzs2s9Mlr0VLfszZLD12VQY4PZf0zN3fm9mxu+8v6buTINvSN3LvDK+FJM3M7IbKP8Lm9YwYd/9hZiedZ7P0iMqe/TTABzPbdfd3ZnZXZZDbebORe/eWjdzb3f1U5XrNr81sU+XM7EeSXqhcHiVTNkuPDNmZmV1SOVC/pTJI8avKR2I3516zlmzk3r1lo/feUBkQeVnlbEK5+5f6vmfMZumRIQss8gRHJVn9L7VNfh5tNkuPxmzLJO6usll6BGabJqO35MeczdSjPrbWtPoes1l6RGQlHalcR+pT/Xqr3n9Fi2c0dZXN0iMwe01l6M2hys/sr/qcfc0NMWjJRu7dWza4x8ozX1R//2bKZumRJPu0vq+fVa49+EbSS5UznfbOm43cu7dscI8nkg7q4x8lPa7339Tc4JMM2Sw9MmRZrFVr8AKscSy1TX4ebTZLj8ZsyyTurrJZegRmmyajt+THnM3UYy6z9rT63rJZekR+f2eetyUtTvkeQzZLj4vKSrqqMrBrR3Ua6F/2WTsbuXdv2Yi9VQelrfn+D57N0iNDtua3JW3X29dVhg0++Nds5N69ZYN73K+P31vjvR48m6VHhiyLtWwxVAQAAAAAAACYkNnQBQAAAAAAAAD8PxwQBAAAAAAAACaEA4IAAAAAAADAhHBAEAAAAAAAAJgQDggCAAAAAAAAE/IHCjsPkaa190YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(A_conv_tr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 100])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_conv_tr[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_trs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_1 = 1.0  # _1\n",
    "l_2 = 0.  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solving gel with FISTA (l_1 1, l_2 0): 5000it [00:12, 413.80it/s, t=8.9e-09, rel change=2.2e-05]\n"
     ]
    }
   ],
   "source": [
    "b_0, B = gel_solve(\n",
    "    A_conv_tr,\n",
    "    y_tr,\n",
    "    l_1,\n",
    "    l_2,\n",
    "    ns,\n",
    "    b_init=None,     # initialization value - useful if solving multiple instances\n",
    "    t_init=None,     # initial step size for line search - default (None) is to use previous iteration's value\n",
    "    ls_beta=0.99,    # line search shrinkage factor - default (None) is to not perform line search and use t_init\n",
    "    max_iters=5000,  # maximum iterations - default (None) is unlimited\n",
    "    rel_tol=1e-5,    # relative change in b_0, B to stop the algorithm (default 1e-6)\n",
    "    verbose=True,    # verbosity (default False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The true champion\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import elementwise_grad\n",
    "from autograd import grad\n",
    "\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "#import numpy as np\n",
    "from scipy.special import expit\n",
    "from pyglmnet import utils\n",
    "\n",
    "\n",
    "class GLM:\n",
    "    \n",
    "    def __init__(self, xs, ys, reg_lambda, group,max_iter, learning_rate, tol,parameter):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.group = np.asarray(group)\n",
    "        #print(self.group.shape)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.parameter = parameter\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "        \n",
    "    def _prox(self,beta, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "        \n",
    "        #print(thresh, beta)\n",
    "        #print('beginprox', beta[0:2],thresh)\n",
    "        group_ids = np.unique(self.group)\n",
    "        result = np.zeros(beta.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        #print('gids',group_ids)\n",
    "        for i in range(len(group_ids)):\n",
    "            gid = i \n",
    "            #print(self.group)\n",
    "            idxs_to_update = np.where(self.group == gid)[0]\n",
    "            #print('idx',idxs_to_update)\n",
    "            #print('norm', np.linalg.norm(beta[idxs_to_update]))\n",
    "            if np.linalg.norm(beta[idxs_to_update]) > 0.:\n",
    "                #print('in here', len(idxs_to_update))\n",
    "                potentialoutput = beta[idxs_to_update] - (thresh / np.linalg.norm(beta[idxs_to_update])) * beta[idxs_to_update]\n",
    "                posind = np.where(beta[idxs_to_update] > 0.)[0]\n",
    "                negind = np.where(beta[idxs_to_update] < 0.)[0]\n",
    "                po = beta[idxs_to_update].copy()\n",
    "                #print('potention', potentialoutput[0:2])\n",
    "                po[posind] = np.asarray(np.clip(potentialoutput[posind],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind] = np.asarray(np.clip(potentialoutput[negind],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[idxs_to_update] = po\n",
    "        #print('end', result[0:2])\n",
    "        return result\n",
    "\n",
    "    def _grad_L2loss(self, beta, X, y):\n",
    "        #print(beta.shape,X.shape,y.shape)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "        n_samples = np.float(X.shape[0])\n",
    "        z = np.dot(X, beta)\n",
    "        #grad_beta = 1. / n_samples * np.transpose(np.dot(np.transpose(z - y), X))\n",
    "        grad_beta = np.transpose(np.dot(np.transpose(z - y), X))\n",
    "        #print('gb',grad_beta.shape)\n",
    "        return grad_beta\n",
    "    \n",
    "    def _loss(self,beta, reg_lambda, X, y):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta, X, y)\n",
    "        P = self._L1penalty(beta)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "    \n",
    "    def _logL(self,beta, X, y):\n",
    "        \"\"\"The log likelihood.\"\"\"\n",
    "        #print('beginlogL', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y),y.shape,beta.shape,X.shape,)\n",
    "        l = np.dot(X, beta)\n",
    "        logL = -0.5 * np.sum((y - l)**2)\n",
    "        #print('endlogL',logL)\n",
    "        return logL\n",
    "    \n",
    "    def _L2loss(self,beta,X,y):\n",
    "        #print('beginl2', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y), y.shape)\n",
    "        output = -self._logL(beta, X, y)\n",
    "        #print('outl2',output)\n",
    "        return(output)\n",
    "    \n",
    "    def _L1penalty(self, beta):\n",
    "        \"\"\"The L1 penalty\"\"\"\n",
    "        # Compute the L1 penalty\n",
    "        group = self.group\n",
    "        group_ids = np.unique(self.group)\n",
    "        L1penalty = 0.0\n",
    "        for group_id in group_ids:\n",
    "            L1penalty += np.linalg.norm(beta[self.group == group_id], 2)\n",
    "        return L1penalty\n",
    "    \n",
    "    #def fhatlambda(self,lamb,x,y):\n",
    "    def fhatlambda(self,lamb,betanew,betaold):\n",
    "        xs = self.xs\n",
    "        ys = self.ys\n",
    "        #print(ys.shape,'fhatlam')\n",
    "        #print(self._L2loss(betaold,xs,ys),self._L2loss(betanew,xs,ys),'old','new') \n",
    "        output = self._L2loss(betaold,xs,ys) + np.dot(self._grad_L2loss(betaold,xs,ys).transpose(),(betanew-betaold)) + (1/(2*lamb)) * np.linalg.norm(betanew-betaold)**2\n",
    "        return(output)\n",
    "    \n",
    "    #_btalgorithm(yk,lamb,.5,1000, rl)\n",
    "    def _btalgorithm(self,bet,lam,b,maxx,rl):\n",
    "        \n",
    "        #print('lam',lam)\n",
    "        X = self.xs\n",
    "        y = self.ys\n",
    "        #print('beginbt', np.linalg.norm(y))\n",
    "        #print('beginbt',self._L2loss(bet,X,y))\n",
    "        #print(np.linalg.norm(bet))\n",
    "        grad_beta = self._grad_L2loss(beta = bet, X = X, y = y)\n",
    "        for i in range(maxx):\n",
    "            betn = bet - lam * grad_beta\n",
    "            z = self._prox(betn, lam * rl)\n",
    "            fz = self._L2loss(z,X,y)\n",
    "            #print(fz,'fz')\n",
    "            fhatz = self.fhatlambda(lam,z, bet)\n",
    "            if fz <= fhatz:\n",
    "            #print(fhatz - fz)\n",
    "            #if 0 <= 1:\n",
    "                break\n",
    "            lam = b*lam\n",
    "        return(z,lam)\n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "\n",
    "        group  = self.group\n",
    "        print(group.shape)\n",
    "        lambdas = self.reg_lambda\n",
    "        parameter = self.parameter\n",
    "        X = self.xs\n",
    "        #print(X.shape)\n",
    "        y = self.ys\n",
    "        \n",
    "        np.random.RandomState(0)\n",
    "        group = np.asarray(group, dtype = np.int64)\n",
    "\n",
    "        #print(group.shape[0])\n",
    "        group.dtype = np.int64\n",
    "        #print(group.shape[0])\n",
    "        #print(X.shape[1])\n",
    "        if group.shape[0] != X.shape[1]:\n",
    "            raise ValueError('group should be (n_features,)')\n",
    "\n",
    "        # type check for data matrix\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError('Input data should be of type ndarray (got %s).'\n",
    "                             % type(X))\n",
    "\n",
    "        n_features = np.float(X.shape[1])\n",
    "        n_features = np.int64(n_features)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "            self.ys = y\n",
    "        #print(y.shape)\n",
    "        n_classes = 1\n",
    "        n_classes = np.int64(n_classes)\n",
    "\n",
    "        beta_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        fit_params = list()\n",
    "        \n",
    "        for l, rl in enumerate(lambdas):\n",
    "            fit_params.append({'beta': beta_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            tol = self.tol\n",
    "            alpha = 1.\n",
    "            beta = np.zeros([n_features, n_classes])\n",
    "            beta = fit_params[-1]['beta']\n",
    "            #print('losser',self._L2loss(beta,X,y))\n",
    "            g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            lamb = self.learning_rate\n",
    "            bm1 = beta.copy()\n",
    "            bm2 = beta.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                L.append(self._loss(beta, rl, X, y))\n",
    "                L2.append(self._L2loss(beta,X,y))\n",
    "                PEN.append(self._L1penalty(beta))\n",
    "                w = (t / (t+ 3))\n",
    "                yk = beta + w*(bm1 - bm2)\n",
    "                #print('losser',self._L2loss(yk,X,y))\n",
    "                #print('beforebt',np.linalg.norm(yk),np.linalg.norm(X),np.linalg.norm(y))\n",
    "                beta , lamb = self._btalgorithm(yk,lamb,.5,1000, rl)\n",
    "                #X = self.xs\n",
    "                #y = self.ys\n",
    "                #print('losser2',self._L2loss(beta,X,y))\n",
    "                bm2 = bm1.copy()\n",
    "                bm1 = beta.copy()\n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "                    \n",
    "            #print(beta)\n",
    "            fit_params[-1]['beta'] = beta\n",
    "            self.lossresults[rl] = L\n",
    "            self.l2loss[rl] = L2\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "            #print(L)\n",
    "        # Update the estimated variables\n",
    "        \n",
    "        self.fit_ = fit_params\n",
    "        self.ynull_ = np.mean(y)\n",
    "\n",
    "        # Return\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "        self = glm\n",
    "        group  = self.group\n",
    "        print(group.shape)\n",
    "        lambdas = self.reg_lambda\n",
    "        parameter = self.parameter\n",
    "        X = self.xs\n",
    "        #print(X.shape)\n",
    "        y = self.ys\n",
    "        \n",
    "        np.random.RandomState(0)\n",
    "        group = np.asarray(group, dtype = np.int64)\n",
    "\n",
    "        #print(group.shape[0])\n",
    "        group.dtype = np.int64\n",
    "        #print(group.shape[0])\n",
    "        #print(X.shape[1])\n",
    "        if group.shape[0] != X.shape[1]:\n",
    "            raise ValueError('group should be (n_features,)')\n",
    "\n",
    "        # type check for data matrix\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError('Input data should be of type ndarray (got %s).'\n",
    "                             % type(X))\n",
    "\n",
    "        n_features = np.float(X.shape[1])\n",
    "        n_features = np.int64(n_features)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "            self.ys = y\n",
    "        #print(y.shape)\n",
    "        n_classes = 1\n",
    "        n_classes = np.int64(n_classes)\n",
    "\n",
    "        beta_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        fit_params = list()\n",
    "        \n",
    "        for l, rl in enumerate(lambdas):\n",
    "            fit_params.append({'beta': beta_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            tol = self.tol\n",
    "            alpha = 1.\n",
    "            beta = np.zeros([n_features, n_classes])\n",
    "            beta = fit_params[-1]['beta']\n",
    "            #print('losser',self._L2loss(beta,X,y))\n",
    "            g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            lamb = self.learning_rate\n",
    "            bm1 = beta.copy()\n",
    "            bm2 = beta.copy()\n",
    "            break\n",
    "        # Return\n",
    "        #return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513 s  5.67 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#2n-4p-2d 513 us\n",
    "#for t in range(0, self.max_iter):\n",
    "t = 0\n",
    "L.append(self._loss(beta, rl, X, y))\n",
    "L2.append(self._L2loss(beta,X,y))\n",
    "PEN.append(self._L1penalty(beta))\n",
    "w = (t / (t+ 3))\n",
    "yk = beta + w*(bm1 - bm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 1)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t = 0\n",
    "# L.append(self._loss(beta, rl, X, y))\n",
    "# L2.append(self._L2loss(beta,X,y))\n",
    "# PEN.append(self._L1penalty(beta))\n",
    "# w = (t / (t+ 3))\n",
    "# yk = beta + w*(bm1 - bm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _btalgorithm(self,bet,lam,b,maxx,rl):\n",
    "        \n",
    "        #print('lam',lam)\n",
    "        X = self.xs\n",
    "        y = self.ys\n",
    "        #print('beginbt', np.linalg.norm(y))\n",
    "        #print('beginbt',self._L2loss(bet,X,y))\n",
    "        #print(np.linalg.norm(bet))\n",
    "        grad_beta = self._grad_L2loss(beta = bet, X = X, y = y)\n",
    "        for i in range(maxx):\n",
    "            betn = bet - lam * grad_beta\n",
    "            z = self._prox(betn, lam * rl)\n",
    "            fz = self._L2loss(z,X,y)\n",
    "            #print(fz,'fz')\n",
    "            fhatz = self.fhatlambda(lam,z, bet)\n",
    "            if fz <= fhatz:\n",
    "            #print(fhatz - fz)\n",
    "            #if 0 <= 1:\n",
    "                break\n",
    "            lam = b*lam\n",
    "        return(z,lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411 s  19.2 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#2n-4p-2d 411 us\n",
    "%%timeit\n",
    "bet = yk\n",
    "lam = lamb\n",
    "b=  .5\n",
    "maxx = 1000\n",
    "#rl = rl\n",
    "\n",
    "X = self.xs\n",
    "y = self.ys\n",
    "#print('beginbt', np.linalg.norm(y))\n",
    "#print('beginbt',self._L2loss(bet,X,y))\n",
    "#print(np.linalg.norm(bet))\n",
    "grad_beta = self._grad_L2loss(beta = bet, X = X, y = y)\n",
    "for i in range(maxx):\n",
    "    betn = bet - lam * grad_beta\n",
    "    z = self._prox(betn, lam * rl)\n",
    "    fz = self._L2loss(z,X,y)\n",
    "    #print(fz,'fz')\n",
    "    fhatz = self.fhatlambda(lam,z, bet)\n",
    "    if fz <= fhatz:\n",
    "    #print(fhatz - fz)\n",
    "    #if 0 <= 1:\n",
    "        break\n",
    "    lam = b*lam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507 ns  7.5 ns per loop (mean  std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#print('losser',self._L2loss(yk,X,y))\n",
    "#print('beforebt',np.linalg.norm(yk),np.linalg.norm(X),np.linalg.norm(y))\n",
    "\n",
    "#X = self.xs\n",
    "#y = self.ys\n",
    "#print('losser2',self._L2loss(beta,X,y))\n",
    "bm2 = bm1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531 ns  17.1 ns per loop (mean  std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bm1 = beta.copy()\n",
    "if t > 1:\n",
    "    DL.append(L[-1] - L[-2])\n",
    "    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "        print('converged', rl)\n",
    "        msg = ('\\tConverged. Loss function:'\n",
    "               ' {0:.2f}').format(L[-1])\n",
    "        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'bm1' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-6ecbda238565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#print('losser',self._L2loss(yk,X,y))\\n#print('beforebt',np.linalg.norm(yk),np.linalg.norm(X),np.linalg.norm(y))\\n\\n#X = self.xs\\n#y = self.ys\\n#print('losser2',self._L2loss(beta,X,y))\\nbm2 = bm1.copy()\\nbm1 = beta.copy()\\nif t > 1:\\n    DL.append(L[-1] - L[-2])\\n    if np.abs(DL[-1] / L[-1]) < tol:\\n        print('converged', rl)\\n        msg = ('\\\\tConverged. Loss function:'\\n               ' {0:.2f}').format(L[-1])\\n        msg = ('\\\\tdL/L: {0:.6f}\\\\n'.format(DL[-1] / L[-1]))\\n        #break\\n\\n#             #print(beta)\\n#             fit_params[-1]['beta'] = beta\\n#             self.lossresults[rl] = L\\n#             self.l2loss[rl] = L2\\n#             self.penalty[rl] = PEN\\n#             self.dls[rl] = DL\\n#             #print(L)\\n#         # Update the estimated variables\\n        \\n#         self.fit_ = fit_params\\n#         self.ynull_ = np.mean(y)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2165\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2167\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'bm1' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "        #break\n",
    "\n",
    "#             #print(beta)\n",
    "#             fit_params[-1]['beta'] = beta\n",
    "#             self.lossresults[rl] = L\n",
    "#             self.l2loss[rl] = L2\n",
    "#             self.penalty[rl] = PEN\n",
    "#             self.dls[rl] = DL\n",
    "#             #print(L)\n",
    "#         # Update the estimated variables\n",
    "        \n",
    "#         self.fit_ = fit_params\n",
    "#         self.ynull_ = np.mean(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002444025577952938"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.ynull_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The true champion\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import elementwise_grad\n",
    "from autograd import grad\n",
    "\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "#import numpy as np\n",
    "from scipy.special import expit\n",
    "from pyglmnet import utils\n",
    "\n",
    "\n",
    "class GLM:\n",
    "\n",
    "    def __init__(self, xs, ys, reg_lambda, group,max_iter, learning_rate, tol,parameter):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.group = np.asarray(group)\n",
    "        #print(self.group.shape)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.parameter = parameter\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "\n",
    "    def _prox(self,beta, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "\n",
    "        #print(thresh, beta)\n",
    "        #print('beginprox', beta[0:2],thresh)\n",
    "        group_ids = np.unique(self.group)\n",
    "        result = np.zeros(beta.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        #print('gids',group_ids)\n",
    "        for i in range(len(group_ids)):\n",
    "            gid = i\n",
    "            #print(self.group)\n",
    "            idxs_to_update = np.where(self.group == gid)[0]\n",
    "            #print('idx',idxs_to_update)\n",
    "            #print('norm', np.linalg.norm(beta[idxs_to_update]))\n",
    "            if np.linalg.norm(beta[idxs_to_update]) > 0.:\n",
    "                #print('in here', len(idxs_to_update))\n",
    "                potentialoutput = beta[idxs_to_update] - (thresh / np.linalg.norm(beta[idxs_to_update])) * beta[idxs_to_update]\n",
    "                posind = np.where(beta[idxs_to_update] > 0.)[0]\n",
    "                negind = np.where(beta[idxs_to_update] < 0.)[0]\n",
    "                po = beta[idxs_to_update].copy()\n",
    "                #print('potention', potentialoutput[0:2])\n",
    "                po[posind] = np.asarray(np.clip(potentialoutput[posind],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind] = np.asarray(np.clip(potentialoutput[negind],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[idxs_to_update] = po\n",
    "        #print('end', result[0:2])\n",
    "        return result\n",
    "\n",
    "    def _grad_L2loss(self, beta, Xs, ys):\n",
    "        #print(beta.shape,X.shape,y.shape)\n",
    "        i = 0\n",
    "        for x,y in zip(Xs,ys):\n",
    "            if y.ndim == 1:\n",
    "                y = y[:, np.newaxis]\n",
    "            n_samples = np.float(X.shape[0])\n",
    "            z = np.dot(X, beta)\n",
    "            #grad_beta = 1. / n_samples * np.transpose(np.dot(np.transpose(z - y), X))\n",
    "            grad_beta[i] = np.transpose(np.dot(np.transpose(z - y), X))\n",
    "            i = i+1\n",
    "        gb = np.mean(grad_beta, axis = 0)\n",
    "        #print('gb',grad_beta.shape)\n",
    "        return gb\n",
    "\n",
    "    def _loss(self,beta, reg_lambda, X, y):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta, X, y)\n",
    "        P = self._L1penalty(beta)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "\n",
    "    def _logL(self,beta, X, y):\n",
    "        \"\"\"The log likelihood.\"\"\"\n",
    "        #print('beginlogL', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y),y.shape,beta.shape,X.shape,)\n",
    "        l = np.dot(X, beta)\n",
    "        logL = -0.5 * np.sum((y - l)**2)\n",
    "        #print('endlogL',logL)\n",
    "        return logL\n",
    "\n",
    "    def _L2loss(self,beta,X,y):\n",
    "        #print('beginl2', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y), y.shape)\n",
    "        output = -self._logL(beta, X, y)\n",
    "        #print('outl2',output)\n",
    "        return(output)\n",
    "\n",
    "    def _L1penalty(self, beta):\n",
    "        \"\"\"The L1 penalty\"\"\"\n",
    "        # Compute the L1 penalty\n",
    "        group = self.group\n",
    "        group_ids = np.unique(self.group)\n",
    "        L1penalty = 0.0\n",
    "        for group_id in group_ids:\n",
    "            L1penalty += np.linalg.norm(beta[self.group == group_id], 2)\n",
    "        return L1penalty\n",
    "\n",
    "    #def fhatlambda(self,lamb,x,y):\n",
    "    def fhatlambda(self,lamb,betanew,betaold):\n",
    "        xs = self.xs\n",
    "        ys = self.ys\n",
    "        #print(ys.shape,'fhatlam')\n",
    "        #print(self._L2loss(betaold,xs,ys),self._L2loss(betanew,xs,ys),'old','new')\n",
    "        output = self._L2loss(betaold,xs,ys) + np.dot(self._grad_L2loss(betaold,xs,ys).transpose(),(betanew-betaold)) + (1/(2*lamb)) * np.linalg.norm(betanew-betaold)**2\n",
    "        return(output)\n",
    "\n",
    "    #_btalgorithm(yk,lamb,.5,1000, rl)\n",
    "    def _btalgorithm(self,bet,lam,b,maxx,rl):\n",
    "\n",
    "        #print('lam',lam)\n",
    "        X = self.xs\n",
    "        y = self.ys\n",
    "        #print('beginbt', np.linalg.norm(y))\n",
    "        #print('beginbt',self._L2loss(bet,X,y))\n",
    "        #print(np.linalg.norm(bet))\n",
    "        grad_beta = self._grad_L2loss(beta = bet, X = X, y = y)\n",
    "        for i in range(maxx):\n",
    "            betn = bet - lam * grad_beta\n",
    "            z = self._prox(betn, lam * rl)\n",
    "            fz = self._L2loss(z,X,y)\n",
    "            #print(fz,'fz')\n",
    "            fhatz = self.fhatlambda(lam,z, bet)\n",
    "            if fz <= fhatz:\n",
    "            #print(fhatz - fz)\n",
    "            #if 0 <= 1:\n",
    "                break\n",
    "            lam = b*lam\n",
    "        return(z,lam)\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "\n",
    "        group  = self.group\n",
    "        print(group.shape)\n",
    "        lambdas = self.reg_lambda\n",
    "        parameter = self.parameter\n",
    "        X = self.xs\n",
    "        #print(X.shape)\n",
    "        y = self.ys\n",
    "\n",
    "        np.random.RandomState(0)\n",
    "        group = np.asarray(group, dtype = np.int64)\n",
    "\n",
    "        #print(group.shape[0])\n",
    "        group.dtype = np.int64\n",
    "        #print(group.shape[0])\n",
    "        #print(X.shape[1])\n",
    "        if group.shape[0] != X.shape[1]:\n",
    "            raise ValueError('group should be (n_features,)')\n",
    "\n",
    "        # type check for data matrix\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError('Input data should be of type ndarray (got %s).'\n",
    "                             % type(X))\n",
    "\n",
    "        n_features = np.float(X.shape[1])\n",
    "        n_features = np.int64(n_features)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "            self.ys = y\n",
    "        #print(y.shape)\n",
    "        n_classes = 1\n",
    "        n_classes = np.int64(n_classes)\n",
    "\n",
    "        beta_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        fit_params = list()\n",
    "\n",
    "        for l, rl in enumerate(lambdas):\n",
    "            fit_params.append({'beta': beta_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            tol = self.tol\n",
    "            alpha = 1.\n",
    "            beta = np.zeros([n_features, n_classes])\n",
    "            beta = fit_params[-1]['beta']\n",
    "            #print('losser',self._L2loss(beta,X,y))\n",
    "            g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            lamb = self.learning_rate\n",
    "            bm1 = beta.copy()\n",
    "            bm2 = beta.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                L.append(self._loss(beta, rl, X, y))\n",
    "                L2.append(self._L2loss(beta,X,y))\n",
    "                PEN.append(self._L1penalty(beta))\n",
    "                w = (t / (t+ 3))\n",
    "                yk = beta + w*(bm1 - bm2)\n",
    "                #print('losser',self._L2loss(yk,X,y))\n",
    "                #print('beforebt',np.linalg.norm(yk),np.linalg.norm(X),np.linalg.norm(y))\n",
    "                beta , lamb = self._btalgorithm(yk,lamb,.5,1000, rl)\n",
    "                #X = self.xs\n",
    "                #y = self.ys\n",
    "                #print('losser2',self._L2loss(beta,X,y))\n",
    "                bm2 = bm1.copy()\n",
    "                bm1 = beta.copy()\n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "\n",
    "            #print(beta)\n",
    "            fit_params[-1]['beta'] = beta\n",
    "            self.lossresults[rl] = L\n",
    "            self.l2loss[rl] = L2\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "            #print(L)\n",
    "        # Update the estimated variables\n",
    "\n",
    "        self.fit_ = fit_params\n",
    "        self.ynull_ = np.mean(y)\n",
    "\n",
    "        # Return\n",
    "        return self\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifold_env_april2",
   "language": "python",
   "name": "manifold_env_april2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
