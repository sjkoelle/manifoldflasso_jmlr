{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/manifoldflasso_jmlr\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import random\n",
    "import sys\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "now = datetime.datetime.now().strftime(\"%B_%d_%Y_%H_%M_%S\")\n",
    "workingdirectory = os.popen('git rev-parse --show-toplevel').read()[:-1]\n",
    "sys.path.append(workingdirectory)\n",
    "os.chdir(workingdirectory)\n",
    "#print(os.getcwd())\n",
    "from codes.experimentclasses.RigidEthanolPCA import RigidEthanolPCA\n",
    "from codes.otherfunctions.multirun import get_coeffs_reps_tangent\n",
    "from codes.otherfunctions.multirun import get_grads_reps_pca2_tangent\n",
    "from codes.otherfunctions.multiplot import plot_reg_path_ax_lambdasearch_tangent\n",
    "from codes.otherfunctions.get_dictionaries import get_atoms_4\n",
    "from codes.flasso.Replicate import Replicate\n",
    "from codes.otherfunctions.get_grads_tangent import get_grads_tangent\n",
    "from codes.otherfunctions.multirun import get_support_recovery_lambda\n",
    "from codes.otherfunctions.multirun import get_lower_interesting_lambda\n",
    "import matplotlib.pyplot as plt\n",
    "from codes.otherfunctions.multirun import get_coeffs_and_lambdas\n",
    "from codes.geometer.RiemannianManifold import RiemannianManifold\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import random\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from matplotlib.lines import Line2D\n",
    "from pylab import rcParams\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "from shutil import copyfile\n",
    "rcParams['figure.figsize'] = 25, 10\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "now = datetime.datetime.now().strftime(\"%B_%d_%Y_%H_%M_%S\")\n",
    "workingdirectory = os.popen('git rev-parse --show-toplevel').read()[:-1]\n",
    "sys.path.append(workingdirectory)\n",
    "os.chdir(workingdirectory)\n",
    "from codes.experimentclasses.RigidEthanolPCA import RigidEthanolPCA\n",
    "from codes.otherfunctions.get_dictionaries import get_all_atoms_4\n",
    "from codes.otherfunctions.get_grads import get_grads\n",
    "from codes.otherfunctions.multirun import get_support_recovery_lambda\n",
    "from codes.otherfunctions.multirun import get_lower_interesting_lambda\n",
    "from codes.otherfunctions.multirun import get_coeffs_and_lambdas\n",
    "from codes.otherfunctions.multirun import get_support\n",
    "from codes.otherfunctions.multiplot import plot_support_2d\n",
    "from codes.otherfunctions.multiplot import plot_reg_path_ax_lambdasearch\n",
    "from codes.otherfunctions.multiplot import plot_gs_v_dgnorm\n",
    "from codes.otherfunctions.multiplot import plot_dot_distributions\n",
    "from codes.otherfunctions.multirun import get_cosines\n",
    "from codes.flasso.Replicate import Replicate\n",
    "from codes.otherfunctions.multirun import get_olsnorm_and_supportsbrute\n",
    "from codes.otherfunctions.multiplot import highlight_cell\n",
    "\n",
    "#set parameters\n",
    "n = 10000 #number of data points to simulate\n",
    "nsel = 100 #number of points to analyze with lasso\n",
    "itermax = 1000 #maximum iterations per lasso run\n",
    "tol = 1e-10 #convergence criteria for lasso\n",
    "#lambdas = np.asarray([0,.01,.1,1,10,100], dtype = np.float16)#lambda values for lasso\n",
    "lambdas = np.asarray(np.hstack([np.asarray([0]),np.logspace(-3,1,11)]), dtype = np.float16)\n",
    "n_neighbors = 1000 #number of neighbors in megaman\n",
    "m = 3 #number of embedding dimensions (diffusion maps)\n",
    "#diffusion_time = 1. #diffusion time controls gaussian kernel radius per gradients paper\n",
    "diffusion_time = 0.05 #(yuchia suggestion)\n",
    "dim = 2 #manifold dimension\n",
    "dimnoise = 2\n",
    "natoms = 9\n",
    "cores = 3 #number of cores for parallel processing\n",
    "cor = 0.0 #correlation for noise\n",
    "var = 0.00001 #variance scaler for noise\n",
    "cor = 0.0 #correlation for noise\n",
    "var = 0.00001 #variance scaler for noise\n",
    "ii = np.asarray([0,0,0,0,1,1,1,2]) # atom adjacencies for dihedral angle computation\n",
    "jj = np.asarray([1,2,3,4,5,6,7,8])\n",
    "\n",
    "#run experiment\n",
    "atoms4 = np.asarray([[6,1,0,4],[4,0,2,8],[7,6,5,1],[3,0,2,4]],dtype = int)\n",
    "nreps = 25\n",
    "lambda_max = 1\n",
    "max_search = 30\n",
    "\n",
    "folder = workingdirectory + '/Figures/rigidethanol/' + now + 'n' + str(n) + 'nsel' + str(nsel) + 'nreps' + str(nreps)\n",
    "os.mkdir(folder)\n",
    "\n",
    "#src = workingdirectory + '/codes/experiments/rigidethanol_110120_nsel100_nreps25_var0.py'\n",
    "#filenamescript = folder + '/script.py'\n",
    "#copyfile(src, filenamescript)\n",
    "\n",
    "new_MN = True\n",
    "new_grad = True\n",
    "# savename = 'rigidethanol_110120_alltorsions'\n",
    "# savefolder = 'rigidethanol'\n",
    "# loadfolder = 'rigidethanol'\n",
    "# loadname = 'rigidethanol_110120_alltorsions'\n",
    "if new_MN == True:\n",
    "    experiment = RigidEthanolPCA(dim, cor, var, ii, jj, cores, False, atoms4)\n",
    "    experiment.M, experiment.Mpca, projector = experiment.generate_data(noise=False)\n",
    "    experiment.q = m\n",
    "    experiment.m = m\n",
    "    experiment.dimnoise = dimnoise\n",
    "    experiment.projector = projector\n",
    "    experiment.Mpca.geom = experiment.Mpca.compute_geom(diffusion_time, n_neighbors)\n",
    "    experiment.N = experiment.Mpca.get_embedding3(experiment.Mpca.geom, m, diffusion_time, dim)\n",
    "    # with open(workingdirectory + '/untracked_data/embeddings/' + savefolder + '/' + savename + '.pkl' ,\n",
    "    #          'wb') as output:\n",
    "    #      pickle.dump(experiment, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "atoms4,p = get_atoms_4(natoms,ii,jj)\n",
    "experiment.p = p\n",
    "experiment.atoms4 = atoms4\n",
    "experiment.itermax = itermax\n",
    "experiment.tol = tol\n",
    "experiment.dnoise = dim\n",
    "experiment.nreps = nreps\n",
    "experiment.nsel = nsel\n",
    "experiment.folder = folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<codes.experimentclasses.RigidEthanolPCA.RigidEthanolPCA at 0x105a6ce80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from codes.geometer.RiemannianManifold import RiemannianManifold\n",
    "from codes.geometer.ShapeSpace import ShapeSpace\n",
    "from codes.geometer.TangentBundle import TangentBundle\n",
    "import numpy as np\n",
    "import autograd.numpy as np\n",
    "from codes.flasso.FlassoExperiment import FlassoExperiment\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_grads() takes 5 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-abbad80f84f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mreplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnsel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnsel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mreplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselected_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mreplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_M\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_M_notan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMpca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#     replicates[i].xtrain, replicates[i].groups = experiment.construct_X_js(replicates[i].dg_M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     replicates[i].ytrain = experiment.construct_Y_js(replicates[i].df_M,dimnoise)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_grads() takes 5 positional arguments but 8 were given"
     ]
    }
   ],
   "source": [
    "cores = 1\n",
    "dim = 2\n",
    "dimnoise = 2\n",
    "q = 3\n",
    "selected_points_save = np.zeros((nreps,nsel))\n",
    "replicates = {}\n",
    "nsel = 1\n",
    "for i in range(nreps):\n",
    "    print(i)\n",
    "    selected_points = np.random.choice(list(range(n)),nsel,replace = False)\n",
    "    selected_points_save[i] = selected_points\n",
    "    replicates[i] = Replicate()\n",
    "    replicates[i].nsel = nsel\n",
    "    replicates[i].selected_points = selected_points\n",
    "    #replicates[i].df_M,replicates[i].df_M_notan = get_grads(cores, dim, dimnoise, q, experiment.Mpca, experiment.M, experiment.N, selected_points)\n",
    "#     replicates[i].xtrain, replicates[i].groups = experiment.construct_X_js(replicates[i].dg_M)\n",
    "#     replicates[i].ytrain = experiment.construct_Y_js(replicates[i].df_M,dimnoise)\n",
    "#     replicates[i].coeff_dict = {}\n",
    "#     replicates[i].coeff_dict[0] = experiment.get_betas_spam2(replicates[i].xtrain, replicates[i].ytrain, replicates[i].groups, np.asarray([0]), nsel, experiment.m, itermax, tol)\n",
    "#     replicates[i].combined_norms = {}\n",
    "#     replicates[i].combined_norms[0] = np.linalg.norm(np.linalg.norm(replicates[i].coeff_dict[0][:, :, :, :], axis=2), axis=1)[0,:]\n",
    "#     replicates[i].higher_lambda,replicates[i].coeff_dict,replicates[i].combined_norms = get_support_recovery_lambda(experiment, replicates[i],  lambda_max, max_search,dim)\n",
    "#     replicates[i].lower_lambda,replicates[i].coeff_dict,replicates[i].combined_norms = get_lower_interesting_lambda(experiment, replicates[i],  lambda_max, max_search)\n",
    "    #= experiment.get_betas_spam2(replicates[i].xtrain, replicates[i].ytrain, replicates[i].groups, lambdas, len(selected_points), n_embedding_coordinates, itermax, tol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Replicate' object has no attribute 'df_M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b295d022bca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_M\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_M_notan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Replicate' object has no attribute 'df_M'"
     ]
    }
   ],
   "source": [
    "replicates[i].df_M[0],replicates[i].df_M_notan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Replicate' object has no attribute 'df_M'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e70f4adca3e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_M\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_M_notan\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Replicate' object has no attribute 'df_M'"
     ]
    }
   ],
   "source": [
    "replicates[i].df_M[1],replicates[i].df_M_notan[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_grads(cores, dim, dimnoise, q, Mpca, Mangles, N, selected_points):\n",
    "    #dimnoise = experiment.dimnoise\n",
    "    #dim = experiment.dim\n",
    "    #cores = experiment.cores\n",
    "\n",
    "    tangent_bases = Mpca.get_wlpca_tangent_sel(Mpca, selected_points, dimnoise)\n",
    "    subM = RiemannianManifold(Mpca.data[selected_points], dim)\n",
    "    subM.tb = TangentBundle(subM, tangent_bases)\n",
    "    N.tangent_bundle = TangentBundle(N, np.swapaxes(N.geom.rmetric.Hvv[:,:dim,:],1,2))\n",
    "    \n",
    "    df_M = get_dF_js_idM(q, Mpca, N, subM.tb, N.tangent_bundle, selected_points, dimnoise)\n",
    "    df_M2 = df_M / np.sum(np.linalg.norm(df_M, axis=1) ** 2, axis=0)\n",
    "    \n",
    "    df_M_notan = get_dF_js_idM_notan(q, Mpca, N, subM.tb, N.tangent_bundle, selected_points, dimnoise)\n",
    "    df_M2_notan = df_M_notan / np.sum(np.linalg.norm(df_M_notan, axis=1) ** 2, axis=0)\n",
    "#     dg_x = experiment.get_dx_g_full(Mangles.data[selected_points])\n",
    "\n",
    "#     W = ShapeSpace(experiment.positions, Mangles.data)\n",
    "#     dw = W.get_dw(cores, experiment.atoms3, experiment.natoms, selected_points)\n",
    "#     dg_w = experiment.project(np.swapaxes(dw, 1, 2),\n",
    "#                               experiment.project(dw, dg_x))\n",
    "\n",
    "#     dg_w_pca = np.asarray([np.matmul(experiment.projector, dg_w[j].transpose()).transpose() for j in range(len(selected_points))])\n",
    "#     dgw_norm = experiment.normalize(dg_w_pca)\n",
    "#     dg_M = experiment.project(subM.tb.tangent_bases, dgw_norm)\n",
    "    return (df_M2, df_M2_notan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_dF_js_idM(q, M, N, M_tangent_bundle_sub, N_tangent_bundle, selectedpoints, dim = None):\n",
    "\n",
    "        affinity_matrix = M.geom.affinity_matrix\n",
    "\n",
    "        nsel = len(selectedpoints)\n",
    "        dF = np.zeros((nsel, dim, q))\n",
    "\n",
    "        for i in range(nsel):\n",
    "            pt = selectedpoints[i]\n",
    "            neighborspt = affinity_matrix[selectedpoints[i]].indices\n",
    "            deltap0 = M.data[neighborspt, :] - M.data[pt, :]\n",
    "            deltaq0 = N.data[neighborspt, :] - N.data[pt, :]\n",
    "            projected_M = np.matmul(M_tangent_bundle_sub.tangent_bases[i, :, :].transpose(),\n",
    "                                    deltap0.transpose()).transpose()\n",
    "            # projected_rescaled_M = np.matmul(np.diag(M_tangent_bundle_sub.rmetric.Gsvals[selectedpoints[i]]),projected_M.transpose())\n",
    "            # projected_rescaled_M = projected_M.transpose()\n",
    "            # b = np.linalg.pinv(projected_rescaled_M)\n",
    "            # a = np.zeros((len(neighborspt), q))\n",
    "            # rescaled_basis = np.matmul(N_tangent_bundle.tangent_bases[selectedpoints[i], :, :][:, :],\n",
    "            #                            np.diag(N.geom.rmetric.Gsvals[selectedpoints[i]][:dim]))\n",
    "            tan_proj = np.dot(N_tangent_bundle.tangent_bases[selectedpoints[i]],N_tangent_bundle.tangent_bases[selectedpoints[i]].transpose())\n",
    "            projected_N = np.dot(deltaq0, tan_proj)\n",
    "            # projected_N_expanded = np.matmul(N_tangent_bundle.tangent_bases[selectedpoints[i], :, :][:, :], projected_N)\n",
    "            # a = projected_N_expanded\n",
    "\n",
    "            # dF[i, :, :][:, :] = np.matmul(a, b).transpose()\n",
    "            # dF[i, :, :][:, :] = np.linalg.lstsq(projected_M, deltaq0)[0]\n",
    "\n",
    "            lr = LinearRegression()\n",
    "            weights = affinity_matrix[selectedpoints[i]].data\n",
    "            #lr.fit(projected_M, deltaq0, weights)\n",
    "            lr.fit(projected_M, projected_N, weights)\n",
    "            dF[i, :, :][:, :] = lr.coef_.transpose()#np.linalg.lstsq(projected_M, deltaq0)[0]#np.matmul(a, b).transpose()\n",
    "        return (dF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dF_js_idM_notan(q, M, N, M_tangent_bundle_sub, N_tangent_bundle, selectedpoints, dim = None):\n",
    "\n",
    "#     self = experiment\n",
    "#     if dim == None:\n",
    "#         dim = self.dim\n",
    "#     q = self.q\n",
    "    affinity_matrix = M.geom.affinity_matrix\n",
    "\n",
    "    nsel = len(selectedpoints)\n",
    "    dF = np.zeros((nsel, dim, q))\n",
    "\n",
    "    for i in range(nsel):\n",
    "        pt = selectedpoints[i]\n",
    "        neighborspt = affinity_matrix[selectedpoints[i]].indices\n",
    "        deltap0 = M.data[neighborspt, :] - M.data[pt, :]\n",
    "        deltaq0 = N.data[neighborspt, :] - N.data[pt, :]\n",
    "        projected_M = np.matmul(M_tangent_bundle_sub.tangent_bases[i, :, :].transpose(),\n",
    "                                deltap0.transpose()).transpose()\n",
    "#            tan_proj = np.dot(N_tangent_bundle.tangent_bases[selectedpoints[i]],N_tangent_bundle.tangent_bases[selectedpoints[i]].transpose())\n",
    "        projected_N = deltaq0#np.dot(deltaq0, tan_proj)\n",
    "        # projected_N_expanded = np.matmul(N_tangent_bundle.tangent_bases[selectedpoints[i], :, :][:, :], projected_N)\n",
    "        # a = projected_N_expanded\n",
    "\n",
    "        # dF[i, :, :][:, :] = np.matmul(a, b).transpose()\n",
    "        # dF[i, :, :][:, :] = np.linalg.lstsq(projected_M, deltaq0)[0]\n",
    "\n",
    "        lr = LinearRegression()\n",
    "        weights = affinity_matrix[selectedpoints[i]].data\n",
    "        #lr.fit(projected_M, deltaq0, weights)\n",
    "        lr.fit(projected_M, projected_N, weights)\n",
    "        dF[i, :, :][:, :] = lr.coef_.transpose()#np.linalg.lstsq(projected_M, deltaq0)[0]#np.matmul(a, b).transpose()\n",
    "    return (dF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dF_js_idM_sometan(q, M, N, M_tangent_bundle_sub, N_tangent_bundle, selectedpoints, dim = None):\n",
    "\n",
    "#     self = experiment\n",
    "#     if dim == None:\n",
    "#         dim = self.dim\n",
    "#     q = self.q\n",
    "    affinity_matrix = M.geom.affinity_matrix\n",
    "\n",
    "    nsel = len(selectedpoints)\n",
    "    dF = np.zeros((nsel, dim, q))\n",
    "\n",
    "    for i in range(nsel):\n",
    "        pt = selectedpoints[i]\n",
    "        neighborspt = affinity_matrix[selectedpoints[i]].indices\n",
    "        deltap0 = M.data[neighborspt, :] - M.data[pt, :]\n",
    "        deltaq0 = N.data[neighborspt, :] - N.data[pt, :]\n",
    "        projected_M = np.matmul(M_tangent_bundle_sub.tangent_bases[i, :, :].transpose(),\n",
    "                                deltap0.transpose()).transpose()\n",
    "#            tan_proj = np.dot(N_tangent_bundle.tangent_bases[selectedpoints[i]],N_tangent_bundle.tangent_bases[selectedpoints[i]].transpose())\n",
    "        projected_N = deltaq0 @ N_tangent_bundle.tangent_bases[selectedpoints[i]]#np.dot(deltaq0, tan_proj)\n",
    "        #delta\n",
    "        lr = LinearRegression()\n",
    "        weights = affinity_matrix[selectedpoints[i]].data\n",
    "        #lr.fit(projected_M, deltaq0, weights)\n",
    "        lr.fit(projected_M, projected_N, weights)\n",
    "        dF[i, :, :][:, :] = lr.coef_.transpose()#np.linalg.lstsq(projected_M, deltaq0)[0]#np.matmul(a, b).transpose()\n",
    "    return (dF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cores, dim, dimnoise, q, Mpca, Mangles, N, selected_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = experiment.M\n",
    "Mpca = experiment.Mpca\n",
    "Mangles = experiment.M\n",
    "dim = 2\n",
    "dimnoise = 2\n",
    "N = experiment.N\n",
    "selected_points = np.asarray([0])\n",
    "\n",
    "#N=  experiment.N\n",
    "#get_dF_js_idM(q, M, N, M_tangent_bundle_sub, N_tangent_bundle, selectedpoints, dim = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,2) into shape (2,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-263ab73b7227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf_M2_notan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_M_notan\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_M_notan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_M_sometan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dF_js_idM_sometan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMpca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtangent_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdf_M2_sometan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_M_sometan\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_M_sometan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ffcb4ac056f8>\u001b[0m in \u001b[0;36mget_dF_js_idM_sometan\u001b[0;34m(q, M, N, M_tangent_bundle_sub, N_tangent_bundle, selectedpoints, dim)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#lr.fit(projected_M, deltaq0, weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojected_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojected_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#np.linalg.lstsq(projected_M, deltaq0)[0]#np.matmul(a, b).transpose()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,2) into shape (2,3)"
     ]
    }
   ],
   "source": [
    "    tangent_bases = Mpca.get_wlpca_tangent_sel(Mpca, selected_points, dimnoise)\n",
    "    subM = RiemannianManifold(Mpca.data[selected_points], dim)\n",
    "    subM.tb = TangentBundle(subM, tangent_bases)\n",
    "    N.tangent_bundle = TangentBundle(N, np.swapaxes(N.geom.rmetric.Hvv[:,:dim,:],1,2))\n",
    "    \n",
    "    df_M = get_dF_js_idM(q, Mpca, N, subM.tb, N.tangent_bundle, selected_points, dimnoise)\n",
    "    df_M2 = df_M / np.sum(np.linalg.norm(df_M, axis=1) ** 2, axis=0)\n",
    "    \n",
    "    df_M_notan = get_dF_js_idM_notan(q, Mpca, N, subM.tb, N.tangent_bundle, selected_points, dimnoise)\n",
    "    df_M2_notan = df_M_notan / np.sum(np.linalg.norm(df_M_notan, axis=1) ** 2, axis=0)\n",
    "    \n",
    "    df_M_sometan = get_dF_js_idM_sometan(q, Mpca, N, subM.tb, N.tangent_bundle, selected_points, dimnoise)\n",
    "    df_M2_sometan = df_M_sometan / np.sum(np.linalg.norm(df_M_sometan, axis=1) ** 2, axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -2.68248629, -26.42504771, -28.71425395],\n",
       "        [ -0.06996217,   4.27826027,  -2.27134559]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ -2.68185082, -27.35185276, -13.51148841],\n",
       "        [ -0.0407752 ,  -2.28008977, -15.28539446]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_M2_notan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,2) into shape (2,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a170bf1f0712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#lr.fit(projected_M, deltaq0, weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojected_M\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojected_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#np.linalg.lstsq(projected_M, deltaq0)[0]#np.matmul(a, b).transpose()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,2) into shape (2,3)"
     ]
    }
   ],
   "source": [
    "    M_tangent_bundle_sub = subM.tb\n",
    "    N_tangent_bundle = N.tangent_bundle\n",
    "    \n",
    "    affinity_matrix = Mpca.geom.affinity_matrix\n",
    "\n",
    "    nsel = len(selected_points)\n",
    "    dF = np.zeros((nsel, dim, q))\n",
    "\n",
    "    for i in range(nsel):\n",
    "        pt = selected_points[i]\n",
    "        neighborspt = affinity_matrix[selected_points[i]].indices\n",
    "        deltap0 = Mpca.data[neighborspt, :] - Mpca.data[pt, :]\n",
    "        deltaq0 = N.data[neighborspt, :] - N.data[pt, :]\n",
    "        projected_M = np.matmul(M_tangent_bundle_sub.tangent_bases[i, :, :].transpose(),\n",
    "                                deltap0.transpose()).transpose()\n",
    "#            tan_proj = np.dot(N_tangent_bundle.tangent_bases[selectedpoints[i]],N_tangent_bundle.tangent_bases[selectedpoints[i]].transpose())\n",
    "        projected_N = deltaq0 @ N_tangent_bundle.tangent_bases[selected_points[i]]#np.dot(deltaq0, tan_proj)\n",
    "        #delta\n",
    "        lr = LinearRegression()\n",
    "        weights = affinity_matrix[selected_points[i]].data\n",
    "        #lr.fit(projected_M, deltaq0, weights)\n",
    "        lr.fit(projected_M, projected_N, weights)\n",
    "        dF[i, :, :][:, :] = lr.coef_.transpose()#np.linalg.lstsq(projected_M, deltaq0)[0]#np.matmul(a, b).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projected_N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37590285, -0.00609069],\n",
       "       [ 0.00917712, -0.00730227]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaq0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_tangent_bundle.tangent_bases[selected_points[i]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37590285, -0.00609069],\n",
       "       [ 0.00917712, -0.00730227]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37590285, -0.00609069],\n",
       "       [ 0.00917712, -0.00730227]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The true champion\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import elementwise_grad\n",
    "from autograd import grad\n",
    "\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "#import numpy as np\n",
    "from scipy.special import expit\n",
    "from pyglmnet import utils\n",
    "\n",
    "\n",
    "class GLM:\n",
    "\n",
    "    def __init__(self, xs, ys, reg_lambda, group,max_iter, learning_rate, tol,parameter):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.group = np.asarray(group)\n",
    "        #print(self.group.shape)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.parameter = parameter\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "\n",
    "    def _prox(self,beta, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "\n",
    "        #print(thresh, beta)\n",
    "        #print('beginprox', beta[0:2],thresh)\n",
    "        group_ids = np.unique(self.group)\n",
    "        result = np.zeros(beta.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        #print('gids',group_ids)\n",
    "        for i in range(len(group_ids)):\n",
    "            gid = i\n",
    "            #print(self.group)\n",
    "            idxs_to_update = np.where(self.group == gid)[0]\n",
    "            #print('idx',idxs_to_update)\n",
    "            #print('norm', np.linalg.norm(beta[idxs_to_update]))\n",
    "            if np.linalg.norm(beta[idxs_to_update]) > 0.:\n",
    "                #print('in here', len(idxs_to_update))\n",
    "                potentialoutput = beta[idxs_to_update] - (thresh / np.linalg.norm(beta[idxs_to_update])) * beta[idxs_to_update]\n",
    "                posind = np.where(beta[idxs_to_update] > 0.)[0]\n",
    "                negind = np.where(beta[idxs_to_update] < 0.)[0]\n",
    "                po = beta[idxs_to_update].copy()\n",
    "                #print('potention', potentialoutput[0:2])\n",
    "                po[posind] = np.asarray(np.clip(potentialoutput[posind],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind] = np.asarray(np.clip(potentialoutput[negind],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[idxs_to_update] = po\n",
    "        #print('end', result[0:2])\n",
    "        return result\n",
    "\n",
    "    def _grad_L2loss(self, beta, Xs, ys):\n",
    "        #print(beta.shape,X.shape,y.shape)\n",
    "        i = 0\n",
    "        for x,y in zip(Xs,ys):\n",
    "            if y.ndim == 1:\n",
    "                y = y[:, np.newaxis]\n",
    "            n_samples = np.float(X.shape[0])\n",
    "            z = np.dot(X, beta)\n",
    "            #grad_beta = 1. / n_samples * np.transpose(np.dot(np.transpose(z - y), X))\n",
    "            grad_beta[i] = np.transpose(np.dot(np.transpose(z - y), X))\n",
    "            i = i+1\n",
    "        gb = np.mean(grad_beta, axis = 0)\n",
    "        #print('gb',grad_beta.shape)\n",
    "        return gb\n",
    "\n",
    "    def _loss(self,beta, reg_lambda, X, y):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta, X, y)\n",
    "        P = self._L1penalty(beta)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "\n",
    "    def _logL(self,beta, X, y):\n",
    "        \"\"\"The log likelihood.\"\"\"\n",
    "        #print('beginlogL', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y),y.shape,beta.shape,X.shape,)\n",
    "        l = np.dot(X, beta)\n",
    "        logL = -0.5 * np.sum((y - l)**2)\n",
    "        #print('endlogL',logL)\n",
    "        return logL\n",
    "\n",
    "    def _L2loss(self,beta,X,y):\n",
    "        #print('beginl2', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y), y.shape)\n",
    "        output = -self._logL(beta, X, y)\n",
    "        #print('outl2',output)\n",
    "        return(output)\n",
    "\n",
    "    def _L1penalty(self, beta):\n",
    "        \"\"\"The L1 penalty\"\"\"\n",
    "        # Compute the L1 penalty\n",
    "        group = self.group\n",
    "        group_ids = np.unique(self.group)\n",
    "        L1penalty = 0.0\n",
    "        for group_id in group_ids:\n",
    "            L1penalty += np.linalg.norm(beta[self.group == group_id], 2)\n",
    "        return L1penalty\n",
    "\n",
    "    #def fhatlambda(self,lamb,x,y):\n",
    "    def fhatlambda(self,lamb,betanew,betaold):\n",
    "        xs = self.xs\n",
    "        ys = self.ys\n",
    "        #print(ys.shape,'fhatlam')\n",
    "        #print(self._L2loss(betaold,xs,ys),self._L2loss(betanew,xs,ys),'old','new')\n",
    "        output = self._L2loss(betaold,xs,ys) + np.dot(self._grad_L2loss(betaold,xs,ys).transpose(),(betanew-betaold)) + (1/(2*lamb)) * np.linalg.norm(betanew-betaold)**2\n",
    "        return(output)\n",
    "\n",
    "    #_btalgorithm(yk,lamb,.5,1000, rl)\n",
    "    def _btalgorithm(self,bet,lam,b,maxx,rl):\n",
    "\n",
    "        #print('lam',lam)\n",
    "        X = self.xs\n",
    "        y = self.ys\n",
    "        #print('beginbt', np.linalg.norm(y))\n",
    "        #print('beginbt',self._L2loss(bet,X,y))\n",
    "        #print(np.linalg.norm(bet))\n",
    "        grad_beta = self._grad_L2loss(beta = bet, X = X, y = y)\n",
    "        for i in range(maxx):\n",
    "            betn = bet - lam * grad_beta\n",
    "            z = self._prox(betn, lam * rl)\n",
    "            fz = self._L2loss(z,X,y)\n",
    "            #print(fz,'fz')\n",
    "            fhatz = self.fhatlambda(lam,z, bet)\n",
    "            if fz <= fhatz:\n",
    "            #print(fhatz - fz)\n",
    "            #if 0 <= 1:\n",
    "                break\n",
    "            lam = b*lam\n",
    "        return(z,lam)\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "\n",
    "        group  = self.group\n",
    "        print(group.shape)\n",
    "        lambdas = self.reg_lambda\n",
    "        parameter = self.parameter\n",
    "        X = self.xs\n",
    "        #print(X.shape)\n",
    "        y = self.ys\n",
    "\n",
    "        np.random.RandomState(0)\n",
    "        group = np.asarray(group, dtype = np.int64)\n",
    "\n",
    "        #print(group.shape[0])\n",
    "        group.dtype = np.int64\n",
    "        #print(group.shape[0])\n",
    "        #print(X.shape[1])\n",
    "        if group.shape[0] != X.shape[1]:\n",
    "            raise ValueError('group should be (n_features,)')\n",
    "\n",
    "        # type check for data matrix\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError('Input data should be of type ndarray (got %s).'\n",
    "                             % type(X))\n",
    "\n",
    "        n_features = np.float(X.shape[1])\n",
    "        n_features = np.int64(n_features)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "            self.ys = y\n",
    "        #print(y.shape)\n",
    "        n_classes = 1\n",
    "        n_classes = np.int64(n_classes)\n",
    "\n",
    "        beta_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        fit_params = list()\n",
    "\n",
    "        for l, rl in enumerate(lambdas):\n",
    "            fit_params.append({'beta': beta_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            tol = self.tol\n",
    "            alpha = 1.\n",
    "            beta = np.zeros([n_features, n_classes])\n",
    "            beta = fit_params[-1]['beta']\n",
    "            #print('losser',self._L2loss(beta,X,y))\n",
    "            g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            lamb = self.learning_rate\n",
    "            bm1 = beta.copy()\n",
    "            bm2 = beta.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                L.append(self._loss(beta, rl, X, y))\n",
    "                L2.append(self._L2loss(beta,X,y))\n",
    "                PEN.append(self._L1penalty(beta))\n",
    "                w = (t / (t+ 3))\n",
    "                yk = beta + w*(bm1 - bm2)\n",
    "                #print('losser',self._L2loss(yk,X,y))\n",
    "                #print('beforebt',np.linalg.norm(yk),np.linalg.norm(X),np.linalg.norm(y))\n",
    "                beta , lamb = self._btalgorithm(yk,lamb,.5,1000, rl)\n",
    "                #X = self.xs\n",
    "                #y = self.ys\n",
    "                #print('losser2',self._L2loss(beta,X,y))\n",
    "                bm2 = bm1.copy()\n",
    "                bm1 = beta.copy()\n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "\n",
    "            #print(beta)\n",
    "            fit_params[-1]['beta'] = beta\n",
    "            self.lossresults[rl] = L\n",
    "            self.l2loss[rl] = L2\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "            #print(L)\n",
    "        # Update the estimated variables\n",
    "\n",
    "        self.fit_ = fit_params\n",
    "        self.ynull_ = np.mean(y)\n",
    "\n",
    "        # Return\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import matplotlib.colors\n",
    "import spams\n",
    "from collections import OrderedDict\n",
    "from pylab import rcParams\n",
    "from codes.flasso.GLMaccelerated import GLM\n",
    "\n",
    "rcParams['figure.figsize'] = 25, 10\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    output = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return (output)\n",
    "\n",
    "\n",
    "class FlassoExperiment:\n",
    "    \"\"\"\n",
    "    FlassoExperiment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        2 + 2\n",
    "\n",
    "    def get_norms(self, differential):\n",
    "        n = differential.shape[0]\n",
    "        # could be p, or q\n",
    "        p = differential.shape[1]\n",
    "        d = differential.shape[2]\n",
    "\n",
    "        differential_normalized = np.zeros(differential.shape)\n",
    "        vectornorms = np.zeros((n, p))\n",
    "        for i in range(n):\n",
    "            for j in range(p):\n",
    "                if np.linalg.norm(differential[i, j, :]) > 0:\n",
    "                    vectornorms[i, j] = np.linalg.norm(differential[i, j, :])\n",
    "\n",
    "        psum = np.sum(vectornorms, axis=0)\n",
    "        return (psum / n)\n",
    "\n",
    "    def _flatten_coefficient(self, coeff):\n",
    "        n = coeff.shape[1]\n",
    "        p = coeff.shape[2]\n",
    "        q = coeff.shape[0]\n",
    "\n",
    "        output = np.zeros((n * p * q))\n",
    "        for k in range(q):\n",
    "            for i in range(n):\n",
    "                output[((k * n * p) + (i * p)):((k * n * p) + (i + 1) * p)] = coeff[k, i, :]\n",
    "        return (output)\n",
    "\n",
    "    def get_l2loss(self, coeffs, ys, xs):\n",
    "\n",
    "        n = coeffs.shape[2]\n",
    "        nlam = coeffs.shape[0]\n",
    "        output = np.zeros(nlam)\n",
    "        for i in range(nlam):\n",
    "            coeffvec = self._flatten_coefficient(coeffs[i])\n",
    "            output[i] = np.sum((ys - np.dot(coeffvec, xs.transpose())) ** 2)\n",
    "        output = output / n\n",
    "        return (output)\n",
    "\n",
    "    def normalize(self, differential):\n",
    "        n = differential.shape[0]\n",
    "        # could be p, or q\n",
    "        p = differential.shape[1]\n",
    "        d = differential.shape[2]\n",
    "\n",
    "        gammas = np.sum(np.sum(differential ** 2, axis=2), axis=0) ** (.5)\n",
    "        normed = np.swapaxes(differential, 1, 2) / gammas\n",
    "        #print(normed.shape)\n",
    "        normed = np.swapaxes(normed, 1, 2)\n",
    "    #\n",
    "    #     differential_normalized = np.zeros(differential.shape)\n",
    "    #     vectornorms = np.zeros((n, p))\n",
    "    #     for i in range(n):\n",
    "    #         for j in range(p):\n",
    "    #             if np.linalg.norm(differential[i, j, :]) > 0:\n",
    "    #                 vectornorms[i, j] = np.linalg.norm(differential[i, j, :])\n",
    "    #     # psum = np.sum(vectornorms, axis = 0)\n",
    "    #     psum = np.sqrt(np.sum(vectornorms ** 2, axis=0))#np.sum(vectornorms ** 2, axis=0)#\n",
    "    #     for j in range(p):\n",
    "    #         if psum[j] > 0:\n",
    "    #             differential_normalized[:, j, :] = (differential[:, j, :] / psum[j])  # *n\n",
    "    #\n",
    "    #     return (differential_normalized)\n",
    "        return(normed)\n",
    "\n",
    "\n",
    "    def get_betas_sam(self, xtrain, ytrain, groups, lambdas, n, q, max_iter, tol, learning_rate):\n",
    "\n",
    "        p = len(np.unique(groups))\n",
    "        models = GLM(xs=xtrain, ys=ytrain,\n",
    "                     tol=tol,\n",
    "                     group=groups,\n",
    "                     learning_rate=learning_rate,\n",
    "                     max_iter=max_iter,\n",
    "                     # reg_lambda=np.logspace(np.log(100), np.log(0.01), 5, base=np.exp(1)))\n",
    "                     reg_lambda=lambdas,\n",
    "                     parameter=.5)\n",
    "        models.fit()\n",
    "        nlam = len(lambdas)\n",
    "        organizedbetas = np.zeros((nlam, q, n, p))\n",
    "        for l in range(nlam):\n",
    "            organizedbetas[l, :, :, :] = np.reshape(models.fit_[l]['beta'], (q, n, p))\n",
    "        # return(models, organizedbetas)\n",
    "        return (organizedbetas)\n",
    "\n",
    "    def construct_X(self, dg_M):\n",
    "        \"\"\" dg_M should have shape n x p x dim\n",
    "        \"\"\"\n",
    "        n = dg_M.shape[0]\n",
    "        dim = dg_M.shape[2]\n",
    "        p = dg_M.shape[1]\n",
    "\n",
    "        xmat = np.zeros((n * dim, n * p))\n",
    "        for i in range(n):\n",
    "            xmat[(i * dim):(i * dim + dim), (i * p):(i * p + p)] = dg_M[i, :, :].transpose()\n",
    "        b = [xmat] * dim\n",
    "        xmatq = scipy.linalg.block_diag(*b)\n",
    "        groups = np.tile(np.tile(np.asarray(np.linspace(start=0, stop=(p - 1), num=p), dtype=int), n), dim)\n",
    "\n",
    "        return (xmatq, list(groups))\n",
    "\n",
    "    def construct_X_js(self, dg_M):\n",
    "        \"\"\" dg_M should have shape n x p x dim\n",
    "        \"\"\"\n",
    "        n = dg_M.shape[0]\n",
    "        dim = dg_M.shape[2]\n",
    "        p = dg_M.shape[1]\n",
    "        q = self.q\n",
    "\n",
    "        xmat = np.zeros((n * dim, n * p))\n",
    "        for i in range(n):\n",
    "            xmat[(i * dim):(i * dim + dim), (i * p):(i * p + p)] = dg_M[i, :, :].transpose()\n",
    "        b = [xmat] * q\n",
    "        xmatq = scipy.linalg.block_diag(*b)\n",
    "        groups = np.zeros(n * p * q)\n",
    "        groups = np.tile(np.tile(np.asarray(np.linspace(start=0, stop=(p - 1), num=p), dtype=int), n), q)\n",
    "\n",
    "        return (xmatq, list(groups))\n",
    "\n",
    "    def construct_X_js_subset(self, dg_M, selind):\n",
    "        dg_M_subset = np.zeros(dg_M.shape)\n",
    "        dg_M_subset[:, selind, :] = dg_M[:, selind, :]\n",
    "        output = self.construct_X_js(dg_M_subset)\n",
    "        return (output)\n",
    "\n",
    "    def construct_Y(self, df_M):\n",
    "        \"\"\" df_M should have shape n x dim x dim\n",
    "        \"\"\"\n",
    "        n = df_M.shape[0]\n",
    "        dim = df_M.shape[1]\n",
    "\n",
    "        #reorg1 = np.swapaxes(df_M, 0, 1)\n",
    "        yvec = np.reshape(np.swapaxes(df_M,0,1), (n * dim * dim))\n",
    "        return (yvec)\n",
    "\n",
    "    def construct_Y_js(self, df_M, dim=None):\n",
    "        \"\"\" df_M should have shape n x dim x q\n",
    "        \"\"\"\n",
    "        n = df_M.shape[0]\n",
    "        q = self.q\n",
    "        if dim == None:\n",
    "            dim = self.dim\n",
    "\n",
    "        reorg1 = np.swapaxes(df_M, 0, 2)\n",
    "        reorg2 = np.swapaxes(reorg1, 2, 1)\n",
    "        # yvec = np.reshape(reorg2, (n*dim*dim))\n",
    "        yvec = np.reshape(reorg2, (n * dim * q))\n",
    "        return (yvec)\n",
    "\n",
    "    def plot_convergence(self, models, name='lossplot.pdf'):\n",
    "        for key in list(models.keys()):\n",
    "            # key = list(models.keys())[0]\n",
    "            xval = np.log(np.asarray(list(range(len(list(models[key].lossresults.values())[0])))) + 1)\n",
    "            y = list(models[key].lossresults.values())[0]\n",
    "            plt.plot(xval, y)\n",
    "        plt.legend(list(models.keys()), loc='upper right')\n",
    "        plt.savefig(name)\n",
    "\n",
    "    def plot_convergence_sam(self, models, name='lossplot.pdf'):\n",
    "        for key in list(models.lossresults.keys()):\n",
    "            # key = list(models.keys())[0]\n",
    "            xval = np.log(np.asarray(list(range(len(models.lossresults[key])))) + 1)\n",
    "            y = models.lossresults[key]\n",
    "            plt.plot(xval, y)\n",
    "        plt.legend(list(models.lossresults.keys()), loc='upper right')\n",
    "        plt.savefig(name)\n",
    "\n",
    "    def plot_bh(self, coeffs, nsample_pts, p, name='beta'):\n",
    "        dim = self.dim\n",
    "        # p = self.p\n",
    "        n = nsample_pts\n",
    "        nlam = coeffs.shape[0]\n",
    "\n",
    "        for l in range(len(lambdas)):\n",
    "            if dim > 1:\n",
    "                fig, axes = plt.subplots(1, q, figsize=(15, 30))\n",
    "                for k in range(q):\n",
    "                    tempplot = axes[k].imshow(coeffs[l, k, :, :])\n",
    "                    plt.colorbar(tempplot, ax=axes[k])\n",
    "            if dim == 1:\n",
    "                k = 0\n",
    "                fig, ax = plt.subplots(figsize=(5, 5))\n",
    "                ax.imshow(coeffs[l, k, :, :])\n",
    "            fig.savefig('lambda' + name + 'heatmap.pdf')\n",
    "\n",
    "    def compute_penalty2(self, coeffs):\n",
    "        n = coeffs.shape[2]\n",
    "        nlam = coeffs.shape[0]\n",
    "        q = coeffs.shape[1]\n",
    "        p = coeffs.shape[3]\n",
    "\n",
    "        # p = self.p\n",
    "        pen = np.zeros(nlam)\n",
    "        for l in range(nlam):\n",
    "            norm2 = np.zeros(p)\n",
    "            for j in range(p):\n",
    "                norm2[j] = np.linalg.norm(coeffs[l, :, :, j])\n",
    "            pen[l] = np.sum(norm2)\n",
    "        pen = pen / n\n",
    "        return (pen)\n",
    "\n",
    "    def plot_penalty(self, coeffs, xaxis, xlabel, xlog, ylog, title, filename):\n",
    "        ylabel = r\"$\\frac{1}{n}\\displaystyle \\|\\beta\\|_{1,2}$ \"\n",
    "\n",
    "        if xlog:\n",
    "            filename = filename + 'xlog'\n",
    "        if ylog:\n",
    "            filename = filename + 'ylog'\n",
    "\n",
    "        pens = self.compute_penalty2(coeffs)\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(15, 15))\n",
    "        axes.plot(xaxis, pens, 'go--', linewidth=5, markersize=0, alpha=1)\n",
    "        axes.set_ylim(bottom=1e-4, top=pens.max())\n",
    "        if xlog:\n",
    "            axes.semilogx()\n",
    "        if ylog:\n",
    "            axes.semilogy()\n",
    "        axes.tick_params(labelsize=50)\n",
    "        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=50)\n",
    "        fig.text(0.05, 0.5, ylabel, ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        fig.savefig(filename + 'penalty' + str(n))\n",
    "\n",
    "    def plot_predictions(self, coeffs, xs, ys, xaxis, xlabe, xlog, ylog, title, filename):\n",
    "        2 + 2\n",
    "\n",
    "    def plot_l2loss(self, coeffs, xs, ys, xaxis, xlabel, xlog, ylog, title, filename):\n",
    "\n",
    "        ylabel = r\"$\\displaystyle \\frac{1}{n}\\|y - x\\beta\\|_2^2$\"\n",
    "\n",
    "        if xlog:\n",
    "            filename = filename + 'xlog'\n",
    "        if ylog:\n",
    "            filename = filename + 'ylog'\n",
    "\n",
    "        losses = self.get_l2loss(coeffs, ys, xs)\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(15, 15))\n",
    "        axes.plot(xaxis, losses, 'go--', linewidth=5, markersize=0, alpha=1)\n",
    "        axes.set_ylim(bottom=1e-4, top=losses.max())\n",
    "        if xlog:\n",
    "            axes.semilogx()\n",
    "        if ylog:\n",
    "            axes.semilogy()\n",
    "        axes.tick_params(labelsize=50)\n",
    "        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=50)\n",
    "        fig.text(0.05, 0.5, ylabel, ha='center', va='center', rotation='vertical', fontsize=60)\n",
    "        plt.suptitle(title, fontsize=55)\n",
    "        fig.savefig(filename + 'loss' + str(n))\n",
    "\n",
    "    def get_cosines(self, dg):\n",
    "        n = dg.shape[0]\n",
    "        p = dg.shape[1]\n",
    "        d = dg.shape[2]\n",
    "\n",
    "        coses = np.zeros((n, p, p))\n",
    "        for i in range(n):\n",
    "            for j in range(p):\n",
    "                for k in range(p):\n",
    "                    coses[i, j, k] = cosine_similarity(dg[i, j, :], dg[i, k,\n",
    "                                                                    :])  # sklearn.metrics.pairwise.cosine_similarity(X = np.reshape(dg[:,i,:], (1,d*n)),Y = np.reshape(dg[:,j,:], (1,d*n)))[0][0]\n",
    "        cos_summary = np.abs(coses).sum(axis=0) / n\n",
    "        return (cos_summary)\n",
    "\n",
    "    def normalize_xaxis(self, coeff, groups):\n",
    "\n",
    "        # groups = np.asarray(may_experiment.groups)\n",
    "        tempsum = 0\n",
    "        i = 0\n",
    "        ngroups = coeff.shape[3]\n",
    "        for j in range(ngroups):\n",
    "            tempsum = tempsum + np.linalg.norm(coeff[i, :, :, j])\n",
    "        beta0norm = tempsum\n",
    "\n",
    "        xaxis = np.zeros(len(lambdas))\n",
    "        for i in range(len(lambdas)):\n",
    "            tempsum = 0\n",
    "            for j in range(ngroups):\n",
    "                tempsum = tempsum + np.linalg.norm(coeff[i, :, :, j])\n",
    "            xaxis[i] = tempsum / beta0norm\n",
    "        return (xaxis)\n",
    "\n",
    "    def get_betas_spam2(self, xs, ys, groups, lambdas, n, q, itermax, tol):\n",
    "\n",
    "        # n = xs.shape[0]\n",
    "        p = len(np.unique(groups))\n",
    "        lambdas = np.asarray(lambdas, dtype=np.float64)\n",
    "        yadd = np.expand_dims(ys, 1)\n",
    "        groups = np.asarray(groups, dtype=np.int32) + 1\n",
    "        W0 = np.zeros((xs.shape[1], yadd.shape[1]), dtype=np.float32)\n",
    "        Xsam = np.asfortranarray(xs, dtype=np.float32)\n",
    "        Ysam = np.asfortranarray(yadd, dtype=np.float32)\n",
    "        coeffs = np.zeros((len(lambdas), q, n, p))\n",
    "        for i in range(len(lambdas)):\n",
    "            # alpha = spams.fistaFlat(Xsam,Dsam2,alpha0sam,ind_groupsam,lambda1 = lambdas[i],mode = mode,itermax = itermax,tol = tol,numThreads = numThreads, regul = \"group-lasso-l2\")\n",
    "            # spams.fistaFlat(Y,X,W0,TRUE,numThreads = 1,verbose = TRUE,lambda1 = 0.05, it0 = 10, max_it = 200,L0 = 0.1, tol = 1e-3, intercept = FALSE,pos = FALSE,compute_gram = TRUE, loss = 'square',regul = 'l1')\n",
    "            output = spams.fistaFlat(Ysam, Xsam, W0, True, groups=groups, numThreads=-1, verbose=True,\n",
    "                                     lambda1=lambdas[i], it0=100, max_it=itermax, L0=0.5, tol=tol, intercept=False,\n",
    "                                     pos=False, compute_gram=True, loss='square', regul='group-lasso-l2', ista=False,\n",
    "                                     subgrad=False, a=0.1, b=1000)\n",
    "            coeffs[i, :, :, :] = np.reshape(output[0], (q, n, p))\n",
    "            # print(output[1])\n",
    "        return (coeffs)\n",
    "\n",
    "\n",
    "    def get_betas_sam_list(self, xtrain, ytrain, groups, lambdas, n, q, max_iter, tol, learning_rate):\n",
    "\n",
    "        p = len(np.unique(groups))\n",
    "        models = GLM(xs=xtrain, ys=ytrain,\n",
    "                     tol=tol,\n",
    "                     group=groups,\n",
    "                     learning_rate=learning_rate,\n",
    "                     max_iter=max_iter,\n",
    "                     # reg_lambda=np.logspace(np.log(100), np.log(0.01), 5, base=np.exp(1)))\n",
    "                     reg_lambda=lambdas,\n",
    "                     parameter=.5)\n",
    "        models.fit()\n",
    "        nlam = len(lambdas)\n",
    "        organizedbetas = np.zeros((nlam, q, n, p))\n",
    "        for l in range(nlam):\n",
    "            organizedbetas[l, :, :, :] = np.reshape(models.fit_[l]['beta'], (q, n, p))\n",
    "        # return(models, organizedbetas)\n",
    "        return (organizedbetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifold_env_april2",
   "language": "python",
   "name": "manifold_env_april2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
