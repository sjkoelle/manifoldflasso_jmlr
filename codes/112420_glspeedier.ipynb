{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samsonkoelle/manifoldflasso_jmlr\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import random\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from matplotlib.lines import Line2D\n",
    "from pylab import rcParams\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "from shutil import copyfile\n",
    "rcParams['figure.figsize'] = 25, 10\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "now = datetime.datetime.now().strftime(\"%B_%d_%Y_%H_%M_%S\")\n",
    "workingdirectory = os.popen('git rev-parse --show-toplevel').read()[:-1]\n",
    "sys.path.append(workingdirectory)\n",
    "os.chdir(workingdirectory)\n",
    "from codes.experimentclasses.RigidEthanolPCA import RigidEthanolPCA\n",
    "from codes.otherfunctions.get_dictionaries import get_all_atoms_4\n",
    "from codes.otherfunctions.get_grads import get_grads\n",
    "from codes.otherfunctions.multirun import get_support_recovery_lambda\n",
    "from codes.otherfunctions.multirun import get_lower_interesting_lambda\n",
    "from codes.otherfunctions.multirun import get_coeffs_and_lambdas\n",
    "from codes.otherfunctions.multirun import get_support\n",
    "from codes.otherfunctions.multiplot import plot_support_2d\n",
    "from codes.otherfunctions.multiplot import plot_reg_path_ax_lambdasearch\n",
    "from codes.otherfunctions.multiplot import plot_gs_v_dgnorm\n",
    "from codes.otherfunctions.multiplot import plot_dot_distributions\n",
    "from codes.otherfunctions.multirun import get_cosines\n",
    "from codes.flasso.Replicate import Replicate\n",
    "from codes.otherfunctions.multirun import get_olsnorm_and_supportsbrute\n",
    "from codes.otherfunctions.multiplot import highlight_cell\n",
    "\n",
    "#set parameters\n",
    "n = 10000 #number of data points to simulate\n",
    "nsel = 100 #number of points to analyze with lasso\n",
    "itermax = 1000 #maximum iterations per lasso run\n",
    "tol = 1e-10 #convergence criteria for lasso\n",
    "#lambdas = np.asarray([0,.01,.1,1,10,100], dtype = np.float16)#lambda values for lasso\n",
    "lambdas = np.asarray(np.hstack([np.asarray([0]),np.logspace(-3,1,11)]), dtype = np.float16)\n",
    "n_neighbors = 1000 #number of neighbors in megaman\n",
    "m = 3 #number of embedding dimensions (diffusion maps)\n",
    "#diffusion_time = 1. #diffusion time controls gaussian kernel radius per gradients paper\n",
    "diffusion_time = 0.05 #(yuchia suggestion)\n",
    "dim = 2 #manifold dimension\n",
    "dimnoise = 2\n",
    "natoms = 9\n",
    "cores = 3 #number of cores for parallel processing\n",
    "cor = 0.0 #correlation for noise\n",
    "var = 0.00001 #variance scaler for noise\n",
    "cor = 0.0 #correlation for noise\n",
    "var = 0.00001 #variance scaler for noise\n",
    "ii = np.asarray([0,0,0,0,1,1,1,2]) # atom adjacencies for dihedral angle computation\n",
    "jj = np.asarray([1,2,3,4,5,6,7,8])\n",
    "\n",
    "#run experiment\n",
    "atoms4 = np.asarray([[6,1,0,4],[4,0,2,8],[7,6,5,1],[3,0,2,4]],dtype = int)\n",
    "nreps = 25\n",
    "lambda_max = 1\n",
    "max_search = 30\n",
    "\n",
    "new_MN = True\n",
    "new_grad = True\n",
    "savename = 'rigidethanol_110120_alltorsions'\n",
    "savefolder = 'rigidethanol'\n",
    "loadfolder = 'rigidethanol'\n",
    "loadname = 'rigidethanol_110120_alltorsions'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(workingdirectory + '/untracked_data/embeddings/re_test_exp.pkl' ,\n",
    "         'rb') as file:\n",
    "     experiment = pickle.load(file)\n",
    "\n",
    "with open(workingdirectory + '/untracked_data/embeddings/re_test_replicate.pkl' ,\n",
    "         'rb') as file:\n",
    "     replicate= pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = replicate.xtrain\n",
    "Y = replicate.ytrain\n",
    "group = replicate.groups\n",
    "glm = GLM(X, Y, reg_lambda = np.asarray([0.05]), group = group,max_iter = 100, learning_rate = .1, tol = 1e-3,parameter=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226800,)\n"
     ]
    }
   ],
   "source": [
    "        self = glm\n",
    "        group  = self.group\n",
    "        print(group.shape)\n",
    "        lambdas = self.reg_lambda\n",
    "        parameter = self.parameter\n",
    "        X = self.xs\n",
    "        #print(X.shape)\n",
    "        y = self.ys\n",
    "        \n",
    "        np.random.RandomState(0)\n",
    "        group = np.asarray(group, dtype = np.int64)\n",
    "\n",
    "        #print(group.shape[0])\n",
    "        group.dtype = np.int64\n",
    "        #print(group.shape[0])\n",
    "        #print(X.shape[1])\n",
    "        if group.shape[0] != X.shape[1]:\n",
    "            raise ValueError('group should be (n_features,)')\n",
    "\n",
    "        # type check for data matrix\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError('Input data should be of type ndarray (got %s).'\n",
    "                             % type(X))\n",
    "\n",
    "        n_features = np.float(X.shape[1])\n",
    "        n_features = np.int64(n_features)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "            self.ys = y\n",
    "        #print(y.shape)\n",
    "        n_classes = 1\n",
    "        n_classes = np.int64(n_classes)\n",
    "\n",
    "        beta_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        fit_params = list()\n",
    "        \n",
    "        for l, rl in enumerate(lambdas):\n",
    "            fit_params.append({'beta': beta_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            tol = self.tol\n",
    "            alpha = 1.\n",
    "            beta = np.zeros([n_features, n_classes])\n",
    "            beta = fit_params[-1]['beta']\n",
    "            #print('losser',self._L2loss(beta,X,y))\n",
    "            g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            lamb = self.learning_rate\n",
    "            bm1 = beta.copy()\n",
    "            bm2 = beta.copy()\n",
    "            break\n",
    "        # Return\n",
    "        #return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556 ms ± 24.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "#2n-4p-2d 513 us\n",
    "#for t in range(0, self.max_iter):\n",
    "t = 0\n",
    "L.append(self._loss(beta, rl, X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.7 ms ± 3.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "L2.append(self._L2loss(beta,X,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# PEN.append(self._L1penalty(beta))\n",
    "# w = (t / (t+ 3))\n",
    "# yk = beta + w*(bm1 - bm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.p = experiment.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.group_ids = {}\n",
    "for i in range(experiment.p):\n",
    "    self.group_ids[i] = np.where(glm.group == i)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _L1penalty(self, beta):\n",
    "        \"\"\"The L1 penalty\"\"\"\n",
    "        # Compute the L1 penalty\n",
    "        group = self.group\n",
    "        gs = np.asarray(list(range(self.p)))\n",
    "        #group_ids = np.unique(self.group)\n",
    "        L1penalty = 0.0\n",
    "        for j in gs:\n",
    "            L1penalty += np.linalg.norm(beta[self.group_ids[j]], 2)\n",
    "        return L1penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self._L1penalty_fast = _L1penalty_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _L1penalty_fast(self, beta):\n",
    "        \n",
    "        p = self.p\n",
    "        m = self.m\n",
    "        n = self.n \n",
    "#         group = self.group\n",
    "#         gs = np.asarray(list(range(p)))\n",
    "        resh = np.reshape(beta, ((m*n,p)))\n",
    "        L1penalty = np.linalg.norm(resh, axis = 0).sum()\n",
    "        \n",
    "        return L1penalty\n",
    "    def _loss(self,beta, reg_lambda, X, y):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta, X, y)\n",
    "        P = self._L1penalty(beta)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "    \n",
    "    def _logL_fast(self,beta, X, y):\n",
    "        \"\"\"The log likelihood.\"\"\"\n",
    "        #print('beginlogL', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y),y.shape,beta.shape,X.shape,)\n",
    "        l = np.dot(X, beta)\n",
    "        logL = -0.5 * np.sum((y - l)**2)\n",
    "        #print('endlogL',logL)\n",
    "        return logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = np.dot(X, beta)\n",
    "logL = -0.5 * np.sum((y - l)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "l = np.dot(X, beta)\n",
    "logL = -0.5 * np.sum((y - l)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15311784236143922"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 ms ± 7.48 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.dot(X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "resh_mnp = np.reshape(beta, ((m,nsel,p)))\n",
    "yhat = np.einsum('ijk,lij->lki',replicate.dg_M, resh_mnp)\n",
    "yhat_mat = np.swapaxes(yhat,0,2)\n",
    "y_mat = replicate.df_M\n",
    "logL = -0.5 * np.sum((y_mat - yhat_mat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1penalty = np.linalg.norm(resh_mnp, axis = 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 756)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(resh_mnp, axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15311784236143922"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = self.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.56 µs ± 55.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "resh_mnp = np.reshape(beta, ((m,nsel,p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'autograd.numpy' has no attribute 'duplicate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-ad46ca5bc585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#%%timeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdg_M\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;31m#* resh_mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'autograd.numpy' has no attribute 'duplicate'"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "np.duplicate(replicate.dg_M ,3).shape#* resh_mnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resh = np.reshape(beta, ((m*n,p)))\n",
    "L1penalty = np.linalg.norm(resh, axis = 0).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 28, 46, 64],\n",
       "       [13, 40, 67, 94]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(6).reshape((3,2))\n",
    "b = np.arange(12).reshape((4,3))\n",
    "np.einsum('ki,jk->ij', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 756, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replicate.dg_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 756)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resh_mnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.98 ms ± 42 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10, 28, 46, 64],\n",
       "       [13, 40, 67, 94]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose() @ b.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = replicate.xtrain\n",
    "y = replicate.ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524 ms ± 14.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "self._loss(beta, rl, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.m = 3\n",
    "self.n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239 µs ± 8.24 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "_L1penalty_fast(self, beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.5 ms ± 2.71 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "_L1penalty(self, beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057582456338873614"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_L1penalty(self, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05758245633887356"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_L1penalty_fast(self, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        # Compute the L1 penalty\n",
    "        group = self.group\n",
    "        gs = np.asarray(list(range(self.p)))\n",
    "        #group_ids = np.unique(self.group)\n",
    "        L1penalty = 0.0\n",
    "        for j in gs:\n",
    "            L1penalty += np.linalg.norm(beta[self.group_ids[j]], 2)\n",
    "        #return L1penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.8 µs ± 1.39 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.linalg.norm(beta[self.group_ids[j]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "resh = np.reshape(beta, ((100*3,experiment.p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.77800858e-06,  1.76436159e-06,  4.31542321e-06,  9.88048148e-06,\n",
       "        8.23438267e-06, -4.30898536e-06,  4.18910237e-06, -6.67359825e-07,\n",
       "       -4.55109576e-07,  1.81039904e-06,  6.35112748e-07,  6.41214068e-06,\n",
       "        3.35554553e-06,  5.36485963e-07,  1.95706893e-06,  1.47122719e-06,\n",
       "        6.58765023e-06, -9.04577883e-07,  1.38036905e-06, -3.76585423e-06,\n",
       "       -1.12565689e-05,  2.88191621e-06,  3.81144708e-06, -3.27233254e-06,\n",
       "        1.00077364e-05, -6.41254707e-06,  2.01757131e-07, -8.25325617e-07,\n",
       "        6.75828578e-06,  6.47865419e-06,  6.83189708e-07,  1.66738324e-06,\n",
       "       -3.91439924e-06, -8.73367049e-06, -1.53400419e-06,  6.89369352e-07,\n",
       "        5.42456208e-06,  5.30149845e-06, -1.70779020e-06, -1.33290454e-06,\n",
       "       -4.62324941e-06, -6.26110202e-06, -7.52323717e-06,  8.60130245e-06,\n",
       "       -2.24714366e-06, -1.93154454e-06, -5.52378907e-06,  3.42808799e-06,\n",
       "       -7.11595171e-06, -9.38008290e-07, -3.94826526e-06,  1.70591930e-06,\n",
       "       -2.25222724e-06, -5.20560928e-06, -1.24260266e-07,  1.88858849e-06,\n",
       "        2.93285813e-07,  1.33365034e-06, -2.79683463e-06, -1.59938786e-06,\n",
       "       -2.96499316e-06, -1.58533140e-06, -3.58530107e-06, -7.61147532e-06,\n",
       "        7.82302215e-07, -1.77152088e-06, -7.18782340e-06,  2.04048614e-06,\n",
       "       -4.00043371e-06,  2.29036137e-07,  3.21468502e-06,  5.68707719e-07,\n",
       "        5.02381254e-06, -5.44455829e-06,  1.77399313e-06, -3.01944485e-06,\n",
       "       -3.83949360e-06, -2.55224720e-06, -1.37368841e-06,  2.47642602e-07,\n",
       "       -5.13734498e-06,  3.97189809e-06,  2.05318536e-06, -6.77356123e-06,\n",
       "        6.56195853e-06,  8.35929972e-06,  5.19744079e-06, -7.93319382e-07,\n",
       "       -4.72113149e-06,  4.64925806e-06, -1.77767613e-06,  5.38996945e-06,\n",
       "        9.18320009e-07,  4.30616859e-06,  1.57128041e-06,  3.11540198e-06,\n",
       "        4.62963877e-08,  7.87420853e-06,  5.59577128e-07,  1.77243987e-06,\n",
       "        8.30313358e-06, -5.94250027e-06, -5.60178571e-06,  4.27423593e-06,\n",
       "       -5.17250179e-06,  8.56975831e-06, -1.82371685e-06, -3.29565614e-06,\n",
       "        8.47858036e-06,  6.52784300e-06,  8.23438695e-06,  3.99490590e-06,\n",
       "       -3.79729138e-06,  8.42180314e-06, -1.18167271e-06,  3.53816753e-06,\n",
       "        4.17659598e-06, -6.83466019e-07,  2.70758100e-06,  4.06616698e-06,\n",
       "        1.65972456e-06, -4.84744617e-06,  1.31498313e-06,  5.84826233e-06,\n",
       "       -3.06246852e-06, -6.59764287e-07, -1.91866645e-06,  8.15372014e-06,\n",
       "        2.96426260e-06,  1.79656894e-06, -3.39469169e-06,  2.37764194e-06,\n",
       "       -2.97324806e-06,  1.40346377e-07, -2.80355414e-06,  2.98251012e-06,\n",
       "        2.54228755e-06, -9.18424848e-07,  1.74606134e-06, -4.81949519e-06,\n",
       "       -6.57520984e-06,  1.93735318e-06,  7.34891955e-07,  2.79996224e-06,\n",
       "        1.05076930e-05,  4.16437164e-06, -4.02478935e-06,  4.92511591e-06,\n",
       "       -5.80206089e-06, -2.03520549e-06, -3.00888912e-07,  7.55442117e-06,\n",
       "       -3.28375142e-06, -3.64390890e-06, -4.34094023e-07, -2.92538927e-06,\n",
       "        4.96753052e-06, -4.76160277e-06, -5.05938559e-06, -1.93042348e-06,\n",
       "       -2.19591028e-06,  8.50763692e-06,  4.18615876e-06,  3.86028401e-07,\n",
       "       -5.40315484e-06,  3.72294081e-06, -4.41012058e-06, -6.81116004e-06,\n",
       "        5.23822660e-06,  1.39745420e-06,  4.06022409e-06,  1.40532475e-06,\n",
       "        3.77791275e-06, -2.87048322e-06, -4.56015362e-06,  3.00526684e-06,\n",
       "       -3.54237065e-06, -3.04034294e-06, -2.00852074e-06,  7.70686024e-08,\n",
       "       -1.56081971e-06, -6.06239547e-06, -2.83782365e-06, -9.80336487e-06,\n",
       "        2.75675243e-06, -7.06374628e-06, -4.86941508e-06,  2.30004759e-07,\n",
       "       -3.26085977e-06,  6.80341532e-06, -5.70042729e-06,  1.17747297e-06,\n",
       "       -1.73204666e-07, -5.15032406e-06,  2.30721632e-06, -7.56377122e-07,\n",
       "        3.40295658e-06,  3.63097070e-06,  9.53807738e-06,  5.89298038e-06,\n",
       "       -1.62778588e-06, -1.05546375e-06,  4.84858728e-06,  2.88916989e-06,\n",
       "        2.82244941e-06, -7.12943582e-06, -1.07258044e-07, -3.25410454e-06,\n",
       "        1.23423545e-06, -4.32761859e-07,  4.01313452e-06,  1.39866938e-06,\n",
       "        3.46705451e-06, -2.05652159e-06, -4.16422511e-06, -1.80797925e-06,\n",
       "       -7.50459165e-08,  1.67174487e-06,  9.96167968e-06, -1.86319011e-07,\n",
       "       -4.21492505e-06, -1.52549284e-06, -2.04407396e-06,  2.12293419e-06,\n",
       "       -6.79363763e-06,  2.78932955e-07,  6.90064100e-07,  1.02372591e-06,\n",
       "       -2.63366873e-06, -1.04903761e-06, -6.27892817e-06, -2.17513176e-06,\n",
       "       -2.39356912e-06,  1.83443583e-06, -5.09780614e-06,  3.44443607e-06,\n",
       "        6.58943803e-06, -9.12691810e-06,  1.87944767e-06,  2.98460333e-06,\n",
       "       -2.81056890e-06, -1.75163939e-06, -5.85893199e-07, -1.31301093e-06,\n",
       "       -1.36249105e-06, -7.38978751e-06,  5.08082701e-06,  4.76022307e-06,\n",
       "       -3.58626217e-06, -6.46571573e-06,  2.29746418e-06, -2.53874766e-06,\n",
       "        6.25895782e-07, -1.40797362e-06,  3.04911266e-06,  3.06326783e-06,\n",
       "       -3.19928297e-06, -6.09948834e-06, -6.97944620e-06,  2.69126710e-06,\n",
       "       -5.24188385e-06, -2.23464001e-06, -2.62925061e-06, -2.31778202e-07,\n",
       "       -8.53738891e-06,  8.32357129e-07,  2.30992515e-06,  3.89868109e-07,\n",
       "       -1.37075032e-06,  4.29453996e-07,  1.75946361e-06, -1.22248358e-05,\n",
       "        8.62395198e-06,  1.71998820e-06, -2.87658105e-06, -1.72378031e-06,\n",
       "        2.17699196e-06, -5.11922130e-07, -8.95363522e-06,  9.10270221e-06,\n",
       "       -4.87392669e-07,  4.49811601e-06, -3.05136617e-06,  6.77414927e-06,\n",
       "        1.26253831e-06,  2.68449662e-06, -4.60870091e-06,  5.34014678e-06,\n",
       "        3.04152630e-06,  5.74006274e-06, -2.76934550e-06, -2.12093086e-06,\n",
       "        1.01583629e-05, -4.67379111e-06, -5.99425488e-07,  5.01274851e-06,\n",
       "        4.30886101e-07,  2.57034250e-06, -1.76123911e-06,  1.63163972e-06,\n",
       "       -5.76070040e-06,  7.31098183e-06, -5.21005490e-07, -2.99902206e-06,\n",
       "        2.93819701e-06, -2.03139236e-06, -5.88297386e-06, -5.93790787e-06,\n",
       "        3.05896452e-06, -7.03586588e-07, -5.89513050e-07,  4.75195682e-06,\n",
       "       -4.96836776e-06, -3.22168321e-06, -1.69700092e-06,  4.16012299e-07,\n",
       "       -1.85941143e-07, -1.26493471e-06, -2.71721350e-07, -4.73127321e-07,\n",
       "       -3.17285886e-06, -3.58462517e-06,  1.21038958e-06, -3.92819701e-06,\n",
       "       -5.10297733e-06, -1.37694996e-06, -6.95180847e-07,  9.95027997e-06,\n",
       "       -3.10714407e-06,  4.15899791e-06,  3.29448119e-06, -5.24226171e-06,\n",
       "        3.40940466e-06, -5.21993228e-06, -1.17247453e-05,  2.67336651e-06,\n",
       "       -7.74202197e-06,  1.98824719e-06, -3.01592107e-06,  7.31724337e-06,\n",
       "        4.71124074e-06, -1.99905557e-06, -3.03279370e-06, -5.35307497e-06,\n",
       "       -1.94410332e-06, -1.23613534e-06, -1.60799623e-06,  6.90934106e-07,\n",
       "        2.55080025e-06,  1.54168632e-06, -3.36924129e-06, -6.33946858e-06,\n",
       "        6.01645436e-06, -3.03989940e-06, -2.87607407e-06, -2.29801284e-06,\n",
       "       -8.12640895e-06, -2.10746915e-06, -2.11488454e-06,  2.73526587e-06,\n",
       "        3.07961706e-06,  1.66264951e-08,  4.10867890e-06,  1.49896377e-06,\n",
       "       -6.91451129e-08,  7.09559825e-07, -8.40623869e-07, -1.74095906e-06,\n",
       "       -1.18048297e-06, -4.97359494e-06,  1.23651546e-06, -4.37885190e-06,\n",
       "        3.71089623e-06, -1.09990556e-06,  2.18231841e-07,  2.17741083e-06,\n",
       "        2.83648353e-06, -6.92514730e-06, -9.12273704e-07,  3.88085940e-06,\n",
       "       -7.48723906e-06,  1.70758587e-06, -9.94516856e-06, -4.50840760e-06,\n",
       "        1.70328712e-07, -7.30474031e-06, -4.34528544e-06, -6.48957234e-06,\n",
       "        7.26690887e-06,  7.24108269e-07,  2.50127988e-06, -9.81812613e-07,\n",
       "       -1.55834104e-06, -7.12731124e-06, -1.28676086e-06, -3.35754943e-06,\n",
       "        3.78273335e-06,  5.03131335e-06,  6.46639645e-06,  3.75904735e-06,\n",
       "       -2.63956762e-06, -4.92018071e-06,  3.38034913e-06,  1.57095599e-06,\n",
       "       -7.79778858e-06,  1.56738004e-06,  3.59135724e-06,  2.59813003e-07,\n",
       "       -8.15933294e-07, -3.56106035e-06, -6.37801896e-06,  3.52865057e-06,\n",
       "       -1.36293847e-06, -1.02939445e-06,  7.63986414e-06,  3.01808248e-06,\n",
       "        1.63503087e-06,  6.26374802e-07,  6.70191738e-06,  7.58196344e-06,\n",
       "        4.09834705e-06,  2.56712783e-06, -9.23546328e-06,  5.45511086e-07,\n",
       "       -5.73663819e-07,  4.14255861e-07,  4.15805153e-06, -1.20797053e-05,\n",
       "       -2.51019424e-06,  1.19005448e-06, -2.05840188e-06, -6.24738145e-06,\n",
       "        3.83140867e-06,  1.22077560e-06, -4.28176618e-06,  1.38808291e-06,\n",
       "        3.62251196e-06,  2.33361830e-08,  3.52982718e-06,  3.45062501e-07,\n",
       "       -1.74263220e-06, -5.11208341e-06, -3.78883452e-07,  8.56670803e-07,\n",
       "        3.86169648e-06, -5.07528521e-07,  2.01682366e-06, -4.25313939e-06,\n",
       "       -3.45074584e-06, -4.86725304e-07, -4.65003732e-06,  3.61661304e-06,\n",
       "        2.04202085e-06,  1.23058097e-06,  1.49428627e-06,  8.91112681e-06,\n",
       "       -2.06730242e-06, -9.70653124e-06,  8.78748664e-07, -2.23119669e-07,\n",
       "       -2.28182999e-06, -4.31582830e-06, -1.93646174e-06,  7.99552157e-07,\n",
       "       -2.21700485e-06,  1.06369210e-05, -4.23502814e-06, -3.49699014e-06,\n",
       "       -1.00909173e-05,  1.10883781e-06, -8.89068178e-06, -2.37854777e-06,\n",
       "       -1.21547855e-06, -3.12931202e-06,  7.66698711e-06,  4.38445499e-06,\n",
       "        5.81630016e-06, -3.89073553e-06,  4.97616431e-06,  2.18695303e-06,\n",
       "        3.40126080e-06,  4.53897191e-06, -4.00689262e-06, -1.87088898e-06,\n",
       "        3.80333338e-06, -1.17090789e-05,  6.67252241e-06,  2.43885390e-06,\n",
       "       -2.01516581e-07,  9.72255978e-07, -4.54116086e-06, -1.54296016e-06,\n",
       "        4.85134188e-06,  5.72320094e-06,  1.18881131e-05, -3.25946500e-07,\n",
       "       -2.90367269e-06, -2.26734553e-06, -4.48872079e-06, -3.43274938e-07,\n",
       "        1.68753276e-06, -1.50980073e-07,  4.83398080e-06, -1.03269754e-06,\n",
       "       -1.53196937e-06, -2.56291216e-06, -7.19856493e-06, -6.91255610e-06,\n",
       "       -5.19910904e-06,  5.73821901e-06,  3.94735570e-06,  6.06245179e-06,\n",
       "       -5.87394909e-06, -8.68000304e-06, -2.91030124e-06,  7.75215843e-07,\n",
       "        2.19881074e-06,  4.62068852e-06,  1.25343770e-06,  7.68372478e-06,\n",
       "       -9.81506530e-07, -4.02592248e-06, -7.41277873e-06, -3.91962680e-06,\n",
       "        1.06753951e-06, -3.91851965e-06,  4.13025778e-06,  6.22719447e-06,\n",
       "       -1.04479140e-05,  3.80975441e-06, -9.87479744e-06,  1.77027802e-06,\n",
       "        5.40066386e-06,  2.85961668e-07, -5.64236849e-06, -2.58126633e-06,\n",
       "       -1.15363953e-06, -8.03548429e-07, -8.94606882e-07, -4.84491972e-07,\n",
       "        9.41270057e-07, -5.32880800e-06, -1.06710683e-06,  6.69427324e-06,\n",
       "       -1.69596747e-06, -1.95694926e-06,  4.75395637e-06, -1.12838830e-05,\n",
       "        5.20890036e-06, -2.78617177e-06,  7.22789120e-07,  4.24697337e-07,\n",
       "        4.15550317e-06, -1.17987102e-06, -2.98953166e-06,  5.72242412e-06,\n",
       "       -1.04240468e-05,  8.96568858e-08, -5.94323379e-06, -3.35790736e-06,\n",
       "        8.86797479e-06, -1.96628864e-07,  8.60095666e-07, -7.85521541e-06,\n",
       "       -3.21448262e-06,  8.66655206e-07,  1.56418736e-06,  2.71995835e-06,\n",
       "        3.80418824e-08,  2.32365171e-06,  2.00080208e-06, -8.06763850e-06,\n",
       "        1.63164559e-07,  3.38581309e-06,  2.60088104e-06, -1.60431574e-06,\n",
       "       -3.55214509e-06, -4.93082859e-06, -5.77839557e-07,  4.99594303e-06,\n",
       "       -8.60583819e-06, -2.90957553e-06, -5.02558402e-06,  3.46101200e-06,\n",
       "       -2.44404597e-06, -2.07512195e-06, -9.56567769e-07,  1.96381504e-06,\n",
       "       -1.73011022e-06, -1.34309659e-05,  2.39555508e-06,  1.93581551e-06,\n",
       "       -9.67993952e-07, -4.77970291e-06,  1.55105869e-06,  1.67211435e-06,\n",
       "       -2.07245539e-06, -9.55606131e-07, -4.10121915e-06, -7.87429859e-07,\n",
       "       -6.83610822e-06,  1.84003008e-06, -4.16388223e-06,  1.04983751e-06,\n",
       "       -6.19913102e-06, -2.60166511e-06, -4.87166689e-07, -7.32230958e-06,\n",
       "        5.07706672e-07, -1.67172647e-06, -7.68234655e-06, -5.74622026e-06,\n",
       "        2.66807797e-06,  3.94865955e-06, -5.81607759e-07,  1.78466407e-06,\n",
       "        9.86964565e-07,  1.45336412e-06,  5.67012349e-06, -6.64461375e-06,\n",
       "        2.98263110e-06, -1.68434284e-06, -9.88796006e-07, -1.33267077e-06,\n",
       "       -1.65408782e-06, -5.40650878e-06,  8.08373894e-07,  7.36747369e-06,\n",
       "       -2.47500090e-07, -6.10689037e-09, -3.03041904e-06, -5.17965372e-07,\n",
       "        2.05540752e-06, -1.63246226e-06, -2.00089965e-06,  1.77806235e-06,\n",
       "       -4.04764008e-06,  1.11330082e-06,  3.61693914e-06,  5.99624577e-06,\n",
       "       -3.98509732e-07,  6.02997019e-06,  4.56089015e-06, -4.39247196e-06,\n",
       "       -5.37009926e-06, -1.34463685e-06,  4.53675261e-06, -3.18725783e-07,\n",
       "       -2.64840193e-06,  6.84410573e-06,  1.26501097e-06, -1.02318972e-05,\n",
       "        1.39841546e-06,  2.29294804e-06,  9.94747154e-07,  1.98285758e-06,\n",
       "       -2.96629669e-07, -5.81303293e-06, -1.63449737e-06, -4.16938181e-06,\n",
       "       -4.11261425e-06, -5.56908443e-06,  1.99510182e-06,  4.31640853e-07,\n",
       "       -1.97603775e-06, -2.86304201e-06, -1.03276477e-07,  4.75835418e-06,\n",
       "       -8.83692996e-06,  1.66171305e-06, -2.40613745e-06, -8.30946140e-06,\n",
       "       -8.57893776e-06, -4.02461858e-06,  9.67855184e-07,  1.73308172e-06,\n",
       "       -4.14013039e-06,  4.48421954e-06,  6.27417767e-06,  1.74641351e-06,\n",
       "       -2.60759554e-06,  4.95775655e-06,  3.33066885e-06,  3.82454767e-06,\n",
       "       -2.89446065e-06, -1.24980357e-05,  9.33329375e-06, -7.10263846e-06,\n",
       "       -1.57707548e-07,  1.04971135e-05,  1.45756947e-06,  4.18539010e-06,\n",
       "       -6.62432350e-06, -7.83803772e-06, -2.34877774e-06,  4.80930218e-06,\n",
       "       -1.52667305e-06, -3.50368748e-06,  8.72871649e-07,  4.77043747e-06,\n",
       "       -6.37098853e-06, -5.33749116e-06, -3.47737767e-06,  4.82644786e-06,\n",
       "        1.03536828e-06,  9.40102915e-06,  4.12894941e-06, -1.54740639e-07,\n",
       "        5.57794461e-06,  9.32526511e-07, -3.10811884e-06,  2.99812542e-06,\n",
       "       -3.07022334e-06, -1.28041050e-06,  5.85442106e-06, -4.46567400e-07,\n",
       "       -3.54118777e-06, -2.04734432e-06,  4.50524950e-06, -2.43624636e-06,\n",
       "       -1.70577975e-06, -2.24996799e-06,  8.10958970e-07, -1.69969030e-06,\n",
       "       -7.06276918e-06, -3.91173255e-06, -4.11282646e-06,  5.48200787e-06,\n",
       "        3.58321888e-06,  2.58932707e-06, -2.22821128e-06, -3.59696447e-06,\n",
       "       -2.23773193e-06, -4.63791932e-06,  1.10105837e-05, -9.90000727e-06,\n",
       "        2.48681012e-06, -5.66381084e-06, -4.60068305e-07, -4.35626959e-06,\n",
       "       -5.19236756e-06, -5.02732055e-06,  7.73803419e-06, -5.86368705e-07,\n",
       "       -3.37611197e-06,  2.45055981e-06,  4.56318984e-08,  3.17475203e-06,\n",
       "       -8.04345968e-06,  1.33864155e-06,  3.40694373e-06, -7.32627112e-06,\n",
       "        1.97616968e-06,  7.47875473e-06, -6.55101559e-08,  3.62171930e-06])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.77800858e-06],\n",
       "       [1.76436159e-06],\n",
       "       [4.31542321e-06],\n",
       "       ...,\n",
       "       [4.69115106e-06],\n",
       "       [4.12662887e-06],\n",
       "       [1.70015958e-06]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232 µs ± 8.02 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.linalg.norm(resh, axis = 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-5fb95db4ef68>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-5fb95db4ef68>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    np.linalg.norm(beta[self.group_ids[j]] 2)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "np.linalg.norm(beta[self.group_ids[j]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2 µs ± 95.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.linalg.norm(beta[self.group_ids[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reshape() missing 1 required positional argument: 'newshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b019ef2451fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: reshape() missing 1 required positional argument: 'newshape'"
     ]
    }
   ],
   "source": [
    "np.reshape(beta, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76897133e-05,  2.85016461e-05,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 4.78617118e-04,  3.15118934e-04,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 5.71765796e-04, -1.81922486e-05,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00, -3.55829629e-05, -6.72858960e-04],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00, -5.66987100e-04, -3.66236385e-03],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  2.85227930e-04,  4.84047775e-03]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226800,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.broadcast_to(replicate.groups, beta.T.shape).ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(np.broadcast_to(replicate.groups, beta.T.shape).ravel(), beta.T.ravel()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15., 30.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(10).reshape(5, 2)\n",
    "groups = np.array([0, 0, 0, 1, 1])\n",
    "np.bincount(np.broadcast_to(groups, X.T.shape).ravel(), X.T.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15., 30.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(groups, X.sum(axis=1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f13985d9449e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "np.bincount(groups, X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def groupby_np(X, groups, axis = 0, uf = np.add, out = None, minlength = 0, identity = None):\n",
    "    if minlength < groups.max() + 1:\n",
    "        minlength = groups.max() + 1\n",
    "    if identity is None:\n",
    "        identity = uf.identity\n",
    "    i = list(range(X.ndim))\n",
    "    del i[axis]\n",
    "    i = tuple(i)\n",
    "    n = out is None\n",
    "    if n:\n",
    "        if identity is None:  # fallback to loops over 0-index for identity\n",
    "            assert np.all(np.in1d(np.arange(minlength), groups)), \"No valid identity for unassinged groups\"\n",
    "            s = [slice(None)] * X.ndim\n",
    "            for i_ in i:\n",
    "                s[i_] = 0\n",
    "            out = np.array([uf.reduce(X[tuple(s)][groups == i]) for i in range(minlength)])\n",
    "        else:\n",
    "            out = np.full((minlength,), identity, dtype = X.dtype)\n",
    "    uf.at(out, groups, uf.reduce(X, i))\n",
    "    if n:\n",
    "        return out\n",
    "\n",
    "\n",
    "#array([15, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.linalg.norm(replicate.dg_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The true champion\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install einops\n",
    "from einops import rearrange\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import elementwise_grad\n",
    "from autograd import grad\n",
    "\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "#import numpy as np\n",
    "from scipy.special import expit\n",
    "from pyglmnet import utils\n",
    "\n",
    "\n",
    "class GradientGroupLasso:\n",
    "    \n",
    "    def __init__(self, dg_M, df_M, reg_l1s, reg_l2, max_iter,learning_rate, tol, beta0_npm= None):\n",
    "        self.dg_M = dg_M\n",
    "        self.df_M = df_M\n",
    "        self.reg_l1s = reg_l1s\n",
    "        self.reg_l2 = reg_l2\n",
    "        self.beta0_npm = beta0_npm\n",
    "        n = dg_M.shape[0]\n",
    "        d= dg_M.shape[1]\n",
    "        m = df_M.shape[2]\n",
    "        p = dg_M.shape[2]\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.m = m \n",
    "        self.d = d\n",
    "        dummy_beta = np.ones((n,p,m))\n",
    "        \n",
    "        self.dummy_beta = dummy_beta\n",
    "        self.group = np.asarray(group)\n",
    "        #print(self.group.shape)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.parameter = parameter\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "        \n",
    "    def _prox(self,beta_npm, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "        \n",
    "        p = self.p\n",
    "        #group_ids = np.unique(self.group)\n",
    "        result = np.zeros(beta_npm.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        for j in range(p):\n",
    "            if np.linalg.norm(beta_npm[:,j,:]) > 0.:\n",
    "                potentialoutput = beta_npm[:,j,:] - (thresh / np.linalg.norm(beta_npm[:,j,:])) * beta_npm[:,j,:]\n",
    "                posind = np.asarray(np.where(beta_npm[:,j,:] > 0.))\n",
    "                negind = np.asarray(np.where(beta_npm[:,j,:] < 0.))\n",
    "                po = beta_npm[:,j,:].copy()\n",
    "                po[posind] = np.asarray(np.clip(potentialoutput[posind],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind] = np.asarray(np.clip(potentialoutput[negind],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[:,j,:] = po\n",
    "        return result\n",
    "\n",
    "    def _grad_L2loss(self, beta_npm):\n",
    "        \n",
    "        df_M = self.df_M\n",
    "        dg_M = self.dg_M\n",
    "        reg_l2 = self.reg_l2\n",
    "        dummy_beta = self.dummy_beta\n",
    "        \n",
    "        #df_M_hat = np.einsum('npd,mpd->mdn',dg_M, beta_mnp)\n",
    "        #print(dg_M.shape, beta_npm.shape)\n",
    "        df_M_hat = np.einsum('ndp,npm->ndm',dg_M, beta_npm)\n",
    "        error = df_M_hat - df_M\n",
    "        grad_beta = np.einsum('ndm,ndp->npm',error,dg_M) #+ reg_l2 * np.ones()\n",
    "        #if \n",
    "        return grad_beta\n",
    "    \n",
    "    def _L1penalty(self, beta_npm):\n",
    "        \n",
    "        p = self.p\n",
    "        m = self.m\n",
    "        n = self.n \n",
    "        #np.einsum('npm->mn,p')\n",
    "        beta_mn_p = rearrange(beta_npm, 'n p m -> (m n) p')#np.reshape(beta_mnp, ((m*n,p)))\n",
    "        L1penalty = np.linalg.norm(beta_mn_p, axis = 0).sum()\n",
    "        \n",
    "        return L1penalty\n",
    "    \n",
    "    def _loss(self,beta_npm, reg_lambda):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta_npm)\n",
    "        P = self._L1penalty(beta_npm)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "    \n",
    "    def _logL(self,beta_npm):\n",
    "        \n",
    "        df_M = self.df_M\n",
    "        dg_M = self.dg_M\n",
    "        \n",
    "        #resh_mnp = np.reshape(beta, ((m,nsel,p)))\n",
    "        df_M_hat = np.einsum('ndp,npm -> ndm',dg_M, beta_npm)\n",
    "        #df_M_hat = np.swapaxes(df_M_hat,0,2)\n",
    "        logL = -0.5 * np.linalg.norm((df_M - df_M_hat))**2\n",
    "        return(logL)\n",
    "    \n",
    "    def _L2loss(self,beta_npm):\n",
    "        output = -self._logL(beta_npm)\n",
    "        return(output)\n",
    "    \n",
    "    \n",
    "    #def fhatlambda(self,lamb,x,y):\n",
    "    def fhatlambda(self,lamb,beta_npm_new,beta_npm_old):\n",
    "        #print(beta_npm_old.shape)\n",
    "        output = self._L2loss(beta_npm_old) + np.linalg.norm(self._grad_L2loss(beta_npm) * (beta_npm-beta_npm))**2 + (1/(2*lamb)) * np.linalg.norm(beta_npm_new-beta_npm_old)**2\n",
    "        #output = self._L2loss(beta_npm_old) + np.dot(self._grad_L2loss(beta_npm_old).transpose(),(beta_npm_new-beta_npm_old)) + (1/(2*lamb)) * np.linalg.norm(beta_npm_new-beta_npm_old)**2\n",
    "        return(output)\n",
    "    \n",
    "    #_btalgorithm(yk,lamb,.5,1000, rl)\n",
    "    def _btalgorithm(self,beta_npm ,lam,b,maxiter_bt,rl):\n",
    "        \n",
    "        grad_beta = self._grad_L2loss(beta_npm = beta_npm)\n",
    "        for i in range(maxiter_bt):\n",
    "            beta_npm_postgrad = beta_npm - lam * grad_beta\n",
    "            beta_npm_postgrad_postprox = self._prox(beta_npm_postgrad, lam * rl)\n",
    "            fz = self._L2loss(beta_npm_postgrad_postprox)\n",
    "            #fhatz = self.fhatlambda(lam,beta_npm_postgrad_postprox, beta_npm_postgrad)\n",
    "            fhatz = self.fhatlambda(lam,beta_npm_postgrad_postprox, beta_npm)\n",
    "            if fz <= fhatz:\n",
    "                print(i)\n",
    "                break\n",
    "            lam = b*lam\n",
    "        return(beta_npm_postgrad_postprox,lam)\n",
    "    \n",
    "    def fit(self, beta0_npm = None):\n",
    "\n",
    "        reg_l1s = self.reg_l1s\n",
    "        n = self.n\n",
    "        m = self.m\n",
    "        p = self.p\n",
    "        \n",
    "        dg_M = self.dg_M\n",
    "        df_M = self.df_M\n",
    "        \n",
    "        tol = self.tol\n",
    "        np.random.RandomState(0)\n",
    "        \n",
    "        if beta0_npm != None:\n",
    "            beta_npm_hat = beta0_npm #1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        else: \n",
    "            beta_npm_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n, p,m])\n",
    "            \n",
    "        for l, rl in enumerate(reg_l1s):\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_npm_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            \n",
    "            alpha = 1.\n",
    "            beta_npm_hat = fit_params[-1]['beta']\n",
    "            #g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            lamb = self.learning_rate\n",
    "            beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "            beta_npm_hat_2 = beta_npm_hat.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                print(t)\n",
    "                L.append(self._loss(beta_npm_hat, rl))\n",
    "                L2.append(self._L2loss(beta_npm_hat))\n",
    "                PEN.append(self._L1penalty(beta_npm_hat))\n",
    "                w = (t / (t+ 3))\n",
    "                beta_npm_hat_momentumguess = beta_npm_hat + w*(beta_npm_hat_1 - beta_npm_hat_2)\n",
    "                \n",
    "                beta_npm_hat , lamb = self._btalgorithm(beta_npm_hat_momentumguess,lamb,.5,1000, rl)\n",
    "                print(beta_npm_hat_momentumguess.max(), beta_npm_hat.max(),self._L2loss(beta_npm_hat))\n",
    "                beta_npm_hat_2 = beta_npm_hat_1.copy()\n",
    "                beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "                \n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "\n",
    "            fit_params[-1]['beta'] = beta_npm_hat\n",
    "            self.lossresults[rl] = L\n",
    "            self.l2loss[rl] = L2\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "\n",
    "        self.fit_ = fit_params\n",
    "        self.ynull_ = np.mean(y)\n",
    "\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GGL = GradientGroupLasso(dg_M, df_M, [.001], 0.,1000,.1, 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "2.0160248756398174e-05 0.0007299558707007444 0.1411977904264434\n",
      "1\n",
      "0\n",
      "0.0009118394934151822 0.0015512900938709892 0.12814702754484653\n",
      "2\n",
      "0\n",
      "0.001879823783139087 0.002478551920794346 0.1145096879053913\n",
      "3\n",
      "0\n",
      "0.0029456203289438794 0.0034815095693966533 0.10092945353050617\n",
      "4\n",
      "0\n",
      "0.004054628225740829 0.004523533660804303 0.08791818856228346\n",
      "5\n",
      "0\n",
      "0.005174798717934085 0.005576033302280018 0.07588538479912144\n",
      "6\n",
      "0\n",
      "0.006277699729930495 0.006612811997899298 0.06504478067557128\n",
      "7\n",
      "0\n",
      "0.0073385570848327935 0.007610110865613454 0.05556171498743165\n",
      "8\n",
      "0\n",
      "0.008335419133041932 0.008547523290517176 0.04748047373761715\n",
      "9\n",
      "0\n",
      "0.009250582609194967 0.009408239925066621 0.040751966122141985\n",
      "10\n",
      "0\n",
      "0.01007032964395081 0.010179429264827098 0.035256788912620074\n",
      "11\n",
      "0\n",
      "0.010785363746067473 0.010852437834608873 0.03084770374318249\n",
      "12\n",
      "0\n",
      "0.011390844690434292 0.011422572786960983 0.027342357480468985\n",
      "13\n",
      "0\n",
      "0.011885807435747072 0.01188895702425467 0.02456180418598154\n",
      "14\n",
      "0\n",
      "0.012273038160849471 0.012254228407308916 0.02235388646876407\n",
      "15\n",
      "0\n",
      "0.012558621226520789 0.012524071174428622 0.020630255663042274\n",
      "16\n",
      "0\n",
      "0.012751307188845215 0.012707324380324502 0.01922664260465488\n",
      "17\n",
      "0\n",
      "0.012863089605336 0.012815275870566441 0.01803578827371467\n",
      "18\n",
      "0\n",
      "0.012907805719345246 0.012860822687721906 0.016981762871210778\n",
      "19\n",
      "0\n",
      "0.012900158575265263 0.01285755867014004 0.016051231793915273\n",
      "20\n",
      "0\n",
      "0.012854720393981896 0.012818968698295434 0.01513028916163745\n",
      "21\n",
      "0\n",
      "0.013185867292657975 0.013297529449371622 0.014139449279400834\n",
      "22\n",
      "0\n",
      "0.014043924745139044 0.014149528141675246 0.013142877435275988\n",
      "23\n",
      "0\n",
      "0.01490321929255922 0.015002764607125596 0.01212604571018115\n",
      "24\n",
      "0\n",
      "0.015761197020859243 0.01585470480905239 0.011136700015449856\n",
      "25\n",
      "0\n",
      "0.01661536570362988 0.016702873596803825 0.01021305802823813\n",
      "26\n",
      "0\n",
      "0.017463300785822356 0.01754486519784582 0.009377272624077762\n",
      "27\n",
      "0\n",
      "0.018302657638783613 0.018378351812727397 0.008617047175643532\n",
      "28\n",
      "0\n",
      "0.019131178432620435 0.019201092148859226 0.00792024486190091\n",
      "29\n",
      "0\n",
      "0.019946700578478697 0.02001093959211247 0.00726576148775686\n",
      "30\n",
      "0\n",
      "0.020747164540524506 0.020805849516829567 0.006629517589977986\n",
      "31\n",
      "0\n",
      "0.021530620330542215 0.021583884696725363 0.0060162106255039595\n",
      "32\n",
      "0\n",
      "0.022295231146915805 0.022343221850407373 0.005436490022113209\n",
      "33\n",
      "0\n",
      "0.023039280907949215 0.023082157124067783 0.004893339871439367\n",
      "34\n",
      "0\n",
      "0.02376117872689086 0.023799111206090594 0.0043978521333403854\n",
      "35\n",
      "0\n",
      "0.024459463650058973 0.024492633621615415 0.003940777546215612\n",
      "36\n",
      "0\n",
      "0.025132808159022944 0.025161406233304602 0.0035338574282038687\n",
      "37\n",
      "0\n",
      "0.0257800208991171 0.025804246100424758 0.003171873495991981\n",
      "38\n",
      "0\n",
      "0.026400048904097098 0.026420107726306187 0.0028545901091613798\n",
      "39\n",
      "0\n",
      "0.02699197923605323 0.027008084583632137 0.00256934402153785\n",
      "40\n",
      "0\n",
      "0.0275550397997493 0.02756740975838668 0.002320555980630589\n",
      "41\n",
      "0\n",
      "0.028088599125771597 0.02809745642246391 0.0020922467280316674\n",
      "42\n",
      "0\n",
      "0.028592166642269327 0.028597736511149868 0.001877082263978403\n",
      "43\n",
      "0\n",
      "0.02906538963753022 0.02906791585824156 0.001682307711653681\n",
      "44\n",
      "0\n",
      "0.02950808375764655 0.029507780322728504 0.0014989716458076179\n",
      "45\n",
      "0\n",
      "0.029920153258185013 0.029917244215437882 0.001326166703115392\n",
      "46\n",
      "0\n",
      "0.030301638890226277 0.030296349356571436 0.0011656716127054156\n",
      "47\n",
      "0\n",
      "0.030652708189236978 0.030645262861239208 0.0010219253848367303\n",
      "48\n",
      "0\n",
      "0.030973652042102992 0.030964278597782453 0.0008943468925216969\n",
      "49\n",
      "0\n",
      "0.03126488958029436 0.03125384478194709 0.0007879844882203226\n",
      "50\n",
      "0\n",
      "0.031527020427385435 0.0315144987474778 0.0007022765365660431\n",
      "51\n",
      "0\n",
      "0.031760671937145685 0.03174688245062543 0.0006604557659940436\n",
      "52\n",
      "0\n",
      "0.03196659067905592 0.03195173727951736 0.0006746677233110815\n",
      "53\n",
      "0\n",
      "0.032145617742575794 0.03212989903130194 0.0007286218966474997\n",
      "54\n",
      "0\n",
      "0.03229868384878206 0.03228228227262338 0.0008143108789310928\n",
      "55\n",
      "0\n",
      "0.03242678362215233 0.03240987461244321 0.0009301211484708425\n",
      "56\n",
      "0\n",
      "0.03253097920617051 0.032513727908184256 0.0010536039818414315\n",
      "57\n",
      "0\n",
      "0.03261238853913825 0.03259495159485452 0.001172181055432411\n",
      "58\n",
      "0\n",
      "0.032672180673983614 0.03265470567404213 0.0012923448384743028\n",
      "59\n",
      "0\n",
      "0.03271156842681743 0.032694193024285445 0.0013921999945121937\n",
      "60\n",
      "0\n",
      "0.03273180002451718 0.03271465375735424 0.0014859771624444117\n",
      "61\n",
      "0\n",
      "0.03273415539356043 0.0327173588087578 0.0016015309634600638\n",
      "62\n",
      "0\n",
      "0.032719939011635044 0.03270359796649584 0.0017579945962753682\n",
      "63\n",
      "0\n",
      "0.03269046261706397 0.0326747139901792 0.001932896951260987\n",
      "64\n",
      "0\n",
      "0.032647123326234935 0.032632073724352756 0.0020722755246646283\n",
      "65\n",
      "0\n",
      "0.03259131464672454 0.03257705575324547 0.002193913928136829\n",
      "66\n",
      "0\n",
      "0.032524429867838496 0.03251103971858292 0.0022786513440339355\n",
      "67\n",
      "0\n",
      "0.03244785294254877 0.03243539934632503 0.002304288053417202\n",
      "68\n",
      "0\n",
      "0.03236295504613437 0.0323515116922062 0.002307096458135984\n",
      "69\n",
      "0\n",
      "0.03227111935700899 0.03226072209150645 0.002265561739464053\n",
      "70\n",
      "0\n",
      "0.0321736635702875 0.032164339527168205 0.0021958248134378074\n",
      "71\n",
      "0\n",
      "0.03207186436408692 0.0320636322752301 0.0021120402748783353\n",
      "72\n",
      "0\n",
      "0.03196695331336952 0.031959822273423893 0.0020259765496294675\n",
      "73\n",
      "0\n",
      "0.032519314951872276 0.032351729419890715 0.0019098828414930414\n",
      "74\n",
      "0\n",
      "0.03344984546976698 0.03215166555521158 0.0018223889471830812\n",
      "75\n",
      "0\n",
      "0.033690169892267606 0.03379742211315406 0.0017729506749310437\n",
      "76\n",
      "0\n",
      "0.03538068158661772 0.03548210587694225 0.0017214386724003267\n",
      "77\n",
      "0\n",
      "0.03710361399958838 0.03719858850702879 0.0016407933694948275\n",
      "78\n",
      "0\n",
      "0.03885149770637139 0.03893932269287569 0.0015201362275149078\n",
      "79\n",
      "0\n",
      "0.04061637148167942 0.040696720680563986 0.0013601622540735069\n",
      "80\n",
      "0\n",
      "0.04239059825905873 0.042463122718308176 0.0011810425225874399\n",
      "81\n",
      "0\n",
      "0.04416643896899007 0.04423088769298955 0.0010041181060085694\n",
      "82\n",
      "0\n",
      "0.04593626096268217 0.04599242395308944 0.0008486070342905407\n",
      "83\n",
      "0\n",
      "0.04769251127388351 0.04774026344015549 0.0007269229377170512\n",
      "84\n",
      "0\n",
      "0.04942783260008134 0.049467090712151315 0.0006434493768669498\n",
      "85\n",
      "0\n",
      "0.051135048872601827 0.05116588027892949 0.0005891837081010049\n",
      "86\n",
      "0\n",
      "0.05280740727604098 0.05282976934017495 0.0005498830914520132\n",
      "87\n",
      "0\n",
      "0.05443819543271223 0.05445200531597621 0.0005198892307919094\n",
      "88\n",
      "0\n",
      "0.05602076098488292 0.05602619859098505 0.0005210166428540377\n",
      "89\n",
      "0\n",
      "0.05754905947659143 0.05754626683417632 0.0005673411577744691\n",
      "90\n",
      "0\n",
      "0.0590173006179098 0.05900656501240846 0.0006257676770057211\n",
      "91\n",
      "0\n",
      "0.060420257929633195 0.06040187750560507 0.0006825736846596028\n",
      "92\n",
      "0\n",
      "0.06175312749901653 0.061727439383103905 0.0006967195861105723\n",
      "93\n",
      "0\n",
      "0.0630115774519309 0.06297894671390362 0.0006776944085083138\n",
      "94\n",
      "0\n",
      "0.06419174763241058 0.06415249778493336 0.0006608139945609429\n",
      "95\n",
      "0\n",
      "0.06529012382317646 0.06524471245638896 0.000650546234762473\n",
      "96\n",
      "0\n",
      "0.06630382971355804 0.06625270962203973 0.0006426884707279719\n",
      "97\n",
      "0\n",
      "0.06723046687272098 0.06717410660395856 0.0006532052021879921\n",
      "98\n",
      "0\n",
      "0.06806813535869168 0.06800703821886114 0.000659321386847441\n",
      "99\n",
      "0\n",
      "0.06881547184509013 0.06875013940636945 0.0006584710541961565\n",
      "100\n",
      "0\n",
      "0.06947159686996976 0.06940257004495032 0.0006518627394971773\n",
      "101\n",
      "0\n",
      "0.07003618056895673 0.06996396834922125 0.0006612754556634626\n",
      "102\n",
      "0\n",
      "0.07050932670194159 0.07043447522244 0.0006703187972555591\n",
      "103\n",
      "0\n",
      "0.07089166586339783 0.07081473326144484 0.0006960856235135219\n",
      "104\n",
      "0\n",
      "0.07118432986010376 0.07110582602052662 0.0007333869243943536\n",
      "105\n",
      "0\n",
      "0.0713888328696339 0.07130929306369559 0.000809886988855926\n",
      "106\n",
      "0\n",
      "0.0715071600965021 0.0714271137121824 0.0009361077993582517\n",
      "107\n",
      "0\n",
      "0.07154172107025593 0.07146175382486458 0.0011892951006764105\n",
      "108\n",
      "0\n",
      "0.07149545771828508 0.07141622894010877 0.001394139537173003\n",
      "109\n",
      "0\n",
      "0.0713719234719089 0.07129385205671547 0.0015125120176954908\n",
      "110\n",
      "0\n",
      "0.07117472411712908 0.07109811813895024 0.0015307461801136652\n",
      "111\n",
      "0\n",
      "0.07090753511375779 0.07083291572362023 0.0014504392048448377\n",
      "112\n",
      "0\n",
      "0.07057463163216839 0.07050243639166057 0.001288620488717099\n",
      "113\n",
      "0\n",
      "0.07018050393897572 0.07011112902057937 0.0010814085284710617\n",
      "114\n",
      "0\n",
      "0.07047849036191879 0.07053511856655371 0.0008799477534209216\n",
      "115\n",
      "0\n",
      "0.0726009192041428 0.07269759233390147 0.0007168766896123738\n",
      "116\n",
      "0\n",
      "0.0748055499558539 0.07494488345300183 0.0005935827242173767\n",
      "117\n",
      "0\n",
      "0.0771359922941247 0.07728606507629371 0.0005213875738676239\n",
      "118\n",
      "0\n",
      "0.07956920087421471 0.07973513237713055 0.000484089286562687\n",
      "119\n",
      "0\n",
      "0.08212397671155337 0.08228301918214596 0.0004950709148904304\n",
      "120\n",
      "0\n",
      "0.08476876240655125 0.08495690251843795 0.000523403255302225\n",
      "121\n",
      "0\n",
      "0.0875660951288519 0.08776597126041555 0.0005685416547771459\n",
      "122\n",
      "0\n",
      "0.09050762235258569 0.09074348038731136 0.0005827791563804995\n",
      "123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.09365009643975727 0.09387454134432865 0.0005601598278104816\n",
      "124\n",
      "0\n",
      "0.09693164023149514 0.09712897524479322 0.0005983186922836566\n",
      "125\n",
      "0\n",
      "0.10030713335071564 0.1005979969880886 0.0006056252042747472\n",
      "126\n",
      "0\n",
      "0.10398634380712131 0.1042741358386434 0.0005871765498275443\n",
      "127\n",
      "0\n",
      "0.10786544071572385 0.10812899958475013 0.000552117600715534\n",
      "128\n",
      "0\n",
      "0.11189558400842693 0.11211675811757771 0.0006656457402873054\n",
      "129\n",
      "0\n",
      "0.11601388577465921 0.11637215639696952 0.0007574833625903082\n",
      "130\n",
      "0\n",
      "0.12053156824900663 0.12089286334765559 0.000806676262598733\n",
      "131\n",
      "0\n",
      "0.12531236044123675 0.12564924821332185 0.0008115314190128212\n",
      "132\n",
      "0\n",
      "0.13029993563752887 0.13058949759846825 0.0008157279720401649\n",
      "133\n",
      "0\n",
      "0.1354207708942364 0.13568899468090279 0.0007768455518823922\n",
      "134\n",
      "0\n",
      "0.14067682394401393 0.14088353825645528 0.0007404058118246773\n",
      "135\n",
      "0\n",
      "0.14596515697166967 0.14613991623811087 0.0007898212634335787\n",
      "136\n",
      "0\n",
      "0.15128284721296814 0.151554991092621 0.0008013140215366283\n",
      "137\n",
      "0\n",
      "0.1568540286288202 0.15711488572033397 0.0008518653853544137\n",
      "138\n",
      "0\n",
      "0.16255648471767006 0.16287608098112819 0.0008660964228937297\n",
      "139\n",
      "0\n",
      "0.16851556084908872 0.16883174625376665 0.0008417101273142334\n",
      "140\n",
      "0\n",
      "0.1746624674997064 0.17495056096775297 0.0007870808422679673\n",
      "141\n",
      "0\n",
      "0.1809419003751979 0.18118749792184263 0.0007277017403463277\n",
      "142\n",
      "0\n",
      "0.1872953948010201 0.1874919391093941 0.000654912226785839\n",
      "143\n",
      "0\n",
      "0.19366683698487258 0.19380079755286211 0.000580917902786881\n",
      "144\n",
      "0\n",
      "0.19998090378319813 0.20004483476569418 0.0005191013389047711\n",
      "145\n",
      "0\n",
      "0.2061623036566445 0.2061552487018346 0.00047640321297401454\n",
      "146\n",
      "0\n",
      "0.21214263416946214 0.212069232936548 0.00046682342044966976\n",
      "147\n",
      "0\n",
      "0.21786493748656713 0.21773311136959975 0.0004926742669556989\n",
      "148\n",
      "0\n",
      "0.22328446241656438 0.22310739177013253 0.0005421481823691292\n",
      "149\n",
      "0\n",
      "0.2283756008469706 0.22816899915099337 0.000596308061074115\n",
      "150\n",
      "0\n",
      "0.23313135932830792 0.23291151515530653 0.0007131778610059395\n",
      "151\n",
      "0\n",
      "0.237561644484211 0.23734399102406956 0.0007963702254915341\n",
      "152\n",
      "0\n",
      "0.24169067703730812 0.24148922199572764 0.0008136581544756686\n",
      "153\n",
      "0\n",
      "0.24555473698716154 0.2453805158628208 0.0008289310737222962\n",
      "154\n",
      "0\n",
      "0.24919745379614786 0.24906390484711788 0.0007883981611655698\n",
      "155\n",
      "0\n",
      "0.2526773560658903 0.25258445130016094 0.0007116201955562451\n",
      "156\n",
      "0\n",
      "0.2560385723484296 0.25598597365932885 0.0006186277655553556\n",
      "157\n",
      "0\n",
      "0.25932371747426236 0.259311075890763 0.0005356307270339085\n",
      "158\n",
      "0\n",
      "0.2625742196955245 0.2625960020176275 0.0005052256336149672\n",
      "159\n",
      "0\n",
      "0.2658200961791797 0.26586429290078767 0.0005762912706428788\n",
      "160\n",
      "0\n",
      "0.2690724311910062 0.2691338604237251 0.0006784854328394047\n",
      "161\n",
      "0\n",
      "0.2723436187846576 0.2724128141405249 0.0008353726421528117\n",
      "162\n",
      "0\n",
      "0.27563215051701917 0.27570568026071446 0.0010978454688391702\n",
      "163\n",
      "0\n",
      "0.2789390367522259 0.279002601919524 0.0012848967146446935\n",
      "164\n",
      "0\n",
      "0.2822402974407501 0.2822863704998176 0.0013679789748961809\n",
      "165\n",
      "0\n",
      "0.2855115003554631 0.28553400406582424 0.0016229025566793455\n",
      "166\n",
      "0\n",
      "0.2887239873318426 0.2887205687125124 0.0017120744016440451\n",
      "167\n",
      "0\n",
      "0.29185089986543544 0.29182071344764626 0.0016153413008638017\n",
      "168\n",
      "0\n",
      "0.294866469678655 0.2948106278341342 0.0014006832077971452\n",
      "169\n",
      "0\n",
      "0.29774839255109037 0.29767026106002475 0.0011494631327947681\n",
      "170\n",
      "0\n",
      "0.30048030527043745 0.3003856256459997 0.0009790844313486346\n",
      "171\n",
      "0\n",
      "0.303054173601182 0.30295659997802327 0.0008956854732587892\n",
      "172\n",
      "0\n",
      "0.305483500464355 0.3053980040218512 0.0007824403816027907\n",
      "173\n",
      "0\n",
      "0.30779779322402295 0.3077207474751803 0.0009368711840582807\n",
      "174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-493-0e7d18cbf81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGGL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-491-2bc58025f255>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, beta0_npm)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mbeta_npm_hat_momentumguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_npm_hat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_hat_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_npm_hat_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mbeta_npm_hat\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_btalgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_hat_momentumguess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_hat_momentumguess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_npm_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_L2loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mbeta_npm_hat_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_npm_hat_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-491-2bc58025f255>\u001b[0m in \u001b[0;36m_btalgorithm\u001b[0;34m(self, beta_npm, lam, b, maxiter_bt, rl)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxiter_bt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mbeta_npm_postgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_npm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mbeta_npm_postgrad_postprox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_postgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mfz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_L2loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_postgrad_postprox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m#fhatz = self.fhatlambda(lam,beta_npm_postgrad_postprox, beta_npm_postgrad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-491-2bc58025f255>\u001b[0m in \u001b[0;36m_prox\u001b[0;34m(self, beta_npm, thresh)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mpotentialoutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_npm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta_npm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mposind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mnegind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mpo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_npm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mpo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpotentialoutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "GGL.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00, -1.00621342e-06,  0.00000000e+00],\n",
       "       [-9.28591924e-07, -2.04845188e-06, -2.82835157e-07],\n",
       "       [-1.52423714e-06, -2.66756867e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -7.38422282e-08, -6.00577401e-06],\n",
       "       [-6.33932024e-07, -1.31324565e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -4.07798001e-06, -6.34408536e-06],\n",
       "       [-2.23849909e-06, -9.17454061e-06,  0.00000000e+00],\n",
       "       [-4.51499821e-07, -2.28780462e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.97154984e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.86784905e-07,  0.00000000e+00],\n",
       "       [-2.53274651e-05, -1.43644087e-05,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -2.37521370e-07,  0.00000000e+00],\n",
       "       [-9.57642412e-07, -3.82079920e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -2.97299600e-06, -4.06941974e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -7.93339655e-07],\n",
       "       [ 0.00000000e+00, -7.93674737e-07, -1.00329242e-06],\n",
       "       [ 0.00000000e+00, -4.05307903e-08, -6.74999894e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -9.25858404e-06, -1.85467034e-06],\n",
       "       [ 0.00000000e+00, -1.40589706e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -8.35449334e-06, -1.78761070e-05],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.92244855e-06, -5.25654956e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-8.89442760e-07,  0.00000000e+00, -2.39422210e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -1.20878148e-06],\n",
       "       [ 0.00000000e+00, -3.82335947e-07, -1.38257982e-07],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -1.15720502e-05],\n",
       "       [-2.61288803e-07, -9.27345230e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -2.87972684e-07, -2.30737030e-07],\n",
       "       [ 0.00000000e+00, -3.78566563e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -8.00073360e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.15027641e-06, -2.06161341e-07],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -2.20115309e-07],\n",
       "       [ 0.00000000e+00, -1.60848762e-07, -1.20266162e-07],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-6.85957273e-07, -1.35805654e-06,  0.00000000e+00],\n",
       "       [-3.25909089e-07, -2.71515781e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.56472823e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.88588183e-06,  0.00000000e+00],\n",
       "       [-6.53611235e-08, -1.00419885e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.90239162e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -2.63252675e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.14591870e-05,  0.00000000e+00],\n",
       "       [-1.14904217e-07, -1.52943655e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -3.13500554e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.76604931e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -3.57968937e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.59666888e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -2.41644154e-06, -1.93012388e-07],\n",
       "       [ 0.00000000e+00, -1.22854432e-06,  0.00000000e+00],\n",
       "       [-5.30948134e-07, -3.93341077e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -2.23977765e-07, -1.23440560e-07],\n",
       "       [ 0.00000000e+00, -2.95215903e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.37812019e-06, -5.54848668e-07],\n",
       "       [-1.49654484e-08, -1.31166766e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-1.60566876e-07, -4.04388317e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -7.18492802e-07, -1.61876781e-06],\n",
       "       [-4.34474155e-07, -1.59918545e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.28870050e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.44486310e-06, -2.30135108e-07],\n",
       "       [ 0.00000000e+00, -2.86750913e-06, -6.50603306e-06],\n",
       "       [ 0.00000000e+00, -4.45752378e-06,  0.00000000e+00],\n",
       "       [-1.60632135e-08,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-2.42520845e-08, -7.87214265e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -3.11865567e-06, -5.12427404e-08],\n",
       "       [ 0.00000000e+00, -6.31700694e-07,  0.00000000e+00],\n",
       "       [-4.73858846e-06, -1.41701628e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -7.38374126e-06],\n",
       "       [ 0.00000000e+00, -4.04899117e-07, -2.77357174e-07],\n",
       "       [ 0.00000000e+00, -4.96131925e-09, -2.54398027e-08],\n",
       "       [-3.85992635e-07, -1.60239556e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -9.12287556e-07, -2.30186805e-07],\n",
       "       [-4.79241120e-08, -2.40740441e-07,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -4.42986975e-06, -5.00409390e-06],\n",
       "       [ 0.00000000e+00, -1.28901774e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -3.19728569e-09,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -1.49280792e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -4.42734595e-07, -4.43880206e-06],\n",
       "       [ 0.00000000e+00, -5.07649215e-06, -5.37207159e-06],\n",
       "       [ 0.00000000e+00, -9.63317582e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -2.10359175e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -5.10540349e-06, -5.42918988e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00, -6.00577615e-07, -1.77099119e-06],\n",
       "       [ 0.00000000e+00, -6.70920047e-07, -1.26266610e-06],\n",
       "       [ 0.00000000e+00, -1.10205800e-06, -1.24015763e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -3.14840246e-06],\n",
       "       [-5.07396471e-11,  0.00000000e+00, -1.94689420e-06],\n",
       "       [ 0.00000000e+00, -1.36385144e-06, -1.07849769e-10]])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL.fit_[l]['beta'][:,4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GradientGroupLasso' object has no attribute 'fit_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-477-2720156454dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGGL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GradientGroupLasso' object has no attribute 'fit_params'"
     ]
    }
   ],
   "source": [
    "GGL.fit_params[-1]['beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self = GGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta0_npm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_l1s = self.reg_l1s\n",
    "n = self.n\n",
    "m = self.m\n",
    "p = self.p\n",
    "\n",
    "dg_M = self.dg_M\n",
    "df_M = self.df_M\n",
    "\n",
    "tol = self.tol\n",
    "np.random.RandomState(0)\n",
    "\n",
    "if beta0_npm != None:\n",
    "    beta_npm_hat = beta0_npm #1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "else: \n",
    "    beta_npm_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n, p,m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(enumerate(reg_l1s))[0][0]\n",
    "rl= list(enumerate(reg_l1s))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "if l == 0:\n",
    "    fit_params[-1]['beta'] = beta_npm_hat\n",
    "else:\n",
    "    fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "\n",
    "alpha = 1.\n",
    "beta_mnp_hat = fit_params[-1]['beta']\n",
    "#g = np.zeros([n_features, n_classes])\n",
    "L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "lamb = self.learning_rate\n",
    "beta_npm_hat_1 = beta_npm_hat.copy()\n",
    "beta_npm_hat_2 = beta_npm_hat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= 0 \n",
    "L.append(self._loss(beta_npm_hat, rl))\n",
    "L2.append(self._L2loss(beta_npm_hat))\n",
    "PEN.append(self._L1penalty(beta_npm_hat))\n",
    "w = (t / (t+ 3))\n",
    "beta_npm_hat_momentumguess = beta_npm_hat + w*(beta_npm_hat_1 - beta_npm_hat_2)\n",
    "#print(t, beta_npm_hat.shape , beta_npm_hat_momentumguess.shape)\n",
    "#print(beta_npm_hat_momentumguess.shape, beta_mnp_hat.shape, beta_npm_hat_1.shape, beta_npm_hat_2.shape)\n",
    "beta_npm_hat , lamb = self._btalgorithm(beta_npm_hat_momentumguess,lamb,.5,1000, rl)\n",
    "beta_npm_hat_2 = beta_npm_hat_1.copy()\n",
    "beta_npm_hat_1 = beta_npm_hat.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.19447006e-06, -6.81366302e-06,  3.67677955e-08],\n",
       "       [-6.94370927e-06, -2.19745847e-06,  5.76495855e-06],\n",
       "       [ 3.57773729e-06,  4.96425146e-06, -8.49925806e-07],\n",
       "       ...,\n",
       "       [-3.15428400e-06, -7.52295857e-06, -4.50819809e-06],\n",
       "       [-5.01927473e-06,  8.07201096e-07, -1.00967604e-06],\n",
       "       [ 5.53594168e-06,  1.84822231e-06, -2.93032208e-06]])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_npm_hat_momentumguess[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.19273883e-06, -6.81395681e-06,  0.00000000e+00],\n",
       "       [-6.92256425e-06, -2.20222290e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -8.46389040e-07],\n",
       "       ...,\n",
       "       [-3.15965745e-06, -7.52161822e-06, -4.51209610e-06],\n",
       "       [-5.04125459e-06,  0.00000000e+00, -1.01233596e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -2.92599270e-06]])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_npm_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "    #def _btalgorithm(self,beta_npm ,lam,b,maxiter_bt,rl):\n",
    "    beta_npm = beta_npm_hat_momentumguess\n",
    "    lam = lamb\n",
    "    b=  .5\n",
    "    maxiter_bt = 1000\n",
    "    \n",
    "    grad_beta = self._grad_L2loss(beta_npm = beta_npm)\n",
    "    for i in range(maxiter_bt):\n",
    "        print(i)\n",
    "        beta_npm_postgrad = beta_npm - lam * grad_beta\n",
    "        beta_npm_postgrad_postprox = self._prox(beta_npm_postgrad, lam * rl)\n",
    "        fz = self._L2loss(beta_npm_postgrad_postprox)\n",
    "        fhatz = self.fhatlambda(lam,beta_npm_postgrad_postprox, beta_npm)\n",
    "\n",
    "        if fz <= fhatz:\n",
    "            break\n",
    "        lam = b*lam\n",
    "    #return(beta_npm_postgrad_postprox,lam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.19273883e-06, -6.81395681e-06,  0.00000000e+00],\n",
       "       [-6.92256425e-06, -2.20222290e-06,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -8.46389040e-07],\n",
       "       ...,\n",
       "       [-3.15965745e-06, -7.52161822e-06, -4.51209610e-06],\n",
       "       [-5.04125459e-06,  0.00000000e+00, -1.01233596e-06],\n",
       "       [ 0.00000000e+00,  0.00000000e+00, -2.92599270e-06]])"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_npm_postgrad_postprox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.19447006e-06, -6.81366302e-06,  3.67677955e-08],\n",
       "       [-6.94370927e-06, -2.19745847e-06,  5.76495855e-06],\n",
       "       [ 3.57773729e-06,  4.96425146e-06, -8.49925806e-07],\n",
       "       ...,\n",
       "       [-3.15428400e-06, -7.52295857e-06, -4.50819809e-06],\n",
       "       [-5.01927473e-06,  8.07201096e-07, -1.00967604e-06],\n",
       "       [ 5.53594168e-06,  1.84822231e-06, -2.93032208e-06]])"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_npm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15311881347624157, 0.1540217911013544)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fz,fhatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def _btalgorithm(self,bet,lam,b,maxx,rl):\n",
    "        \n",
    "        #print('lam',lam)\n",
    "        X = self.xs\n",
    "        y = self.ys\n",
    "        #print('beginbt', np.linalg.norm(y))\n",
    "        #print('beginbt',self._L2loss(bet,X,y))\n",
    "        #print(np.linalg.norm(bet))\n",
    "        grad_beta = self._grad_L2loss(beta = bet, X = X, y = y)\n",
    "        for i in range(maxx):\n",
    "            betn = bet - lam * grad_beta\n",
    "            z = self._prox(betn, lam * rl)\n",
    "            fz = self._L2loss(z,X,y)\n",
    "            #print(fz,'fz')\n",
    "            fhatz = self.fhatlambda(lam,z, bet)\n",
    "            if fz <= fhatz:\n",
    "            #print(fhatz - fz)\n",
    "            #if 0 <= 1:\n",
    "                break\n",
    "            lam = b*lam\n",
    "        return(z,lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    def _prox(self,beta_mnp, thresh):\n",
    "        beta_\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "        \n",
    "        p = self.p\n",
    "        #group_ids = np.unique(self.group)\n",
    "        result = np.zeros(beta_npm.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        for j in range(p):\n",
    "            if np.linalg.norm(beta_npm[:,j,:]) > 0.:\n",
    "                potentialoutput = beta_npm[:,j,:] - (thresh / np.linalg.norm(beta_npm[:,j,:])) * beta_npm[:,j,:]\n",
    "                posind = np.asarray(np.where(beta_npm[:,j,:] > 0.))\n",
    "                negind = np.asarray(np.where(beta_npm[:,j,:] < 0.))\n",
    "                po = beta_npm[:,j,:].copy()\n",
    "                po[posind] = np.asarray(np.clip(potentialoutput[posind],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind] = np.asarray(np.clip(potentialoutput[negind],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[:,j,:] = po\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [-3.67588675e-04,  0.00000000e+00, -1.53095617e-05],\n",
       "        [-6.01342391e-05,  0.00000000e+00, -3.53743708e-04],\n",
       "        ...,\n",
       "        [-3.89208028e-04,  0.00000000e+00, -1.15674912e-03],\n",
       "        [ 0.00000000e+00,  0.00000000e+00, -1.22458684e-03],\n",
       "        [-3.49562013e-05,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-3.35535311e-04,  0.00000000e+00, -3.86380053e-04],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -1.18442729e-03, -5.39650208e-04],\n",
       "        [-7.28181059e-04, -6.33142548e-04, -4.82990504e-04],\n",
       "        [-4.38909714e-04,  0.00000000e+00, -1.13169395e-04]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00, -3.96251040e-04],\n",
       "        [-7.15160177e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "        [-5.09334341e-04, -1.82762164e-04,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-6.05975369e-04,  0.00000000e+00, -2.59465681e-04],\n",
       "        [ 0.00000000e+00, -1.76347756e-04, -8.96467893e-05],\n",
       "        [ 0.00000000e+00, -3.11401248e-04, -4.46675396e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [-1.69657362e-03,  0.00000000e+00, -5.46019223e-04],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -9.61545832e-04,  0.00000000e+00],\n",
       "        [ 0.00000000e+00, -1.92049065e-05, -4.26916321e-05],\n",
       "        [ 0.00000000e+00, -1.97619894e-04,  0.00000000e+00]],\n",
       "\n",
       "       [[ 0.00000000e+00, -9.95047182e-05,  0.00000000e+00],\n",
       "        [-2.08955414e-04,  0.00000000e+00, -1.41852463e-03],\n",
       "        [-6.20422238e-05, -3.96428610e-05,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -1.26947270e-03, -3.58891681e-04],\n",
       "        [-3.34456701e-04,  0.00000000e+00, -2.70646306e-04],\n",
       "        [ 0.00000000e+00, -2.71420981e-04, -6.42181041e-04]],\n",
       "\n",
       "       [[-1.15211343e-03,  0.00000000e+00, -1.69990493e-05],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00, -7.53742318e-06],\n",
       "        [-4.44982838e-04,  0.00000000e+00, -6.07862291e-04]]])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_npm_postgrad_postprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-5.52275036e-06, -5.35146289e-06, -7.27303281e-06],\n",
       "        [ 2.82972315e-06, -8.64112983e-06,  1.17854069e-07],\n",
       "        [ 4.60318353e-07, -3.72091238e-06,  2.70785369e-06],\n",
       "        ...,\n",
       "        [ 2.81314075e-06, -1.24112248e-06,  8.36081954e-06],\n",
       "        [-1.44322832e-06, -2.72560066e-06,  9.92011831e-06],\n",
       "        [ 2.70813391e-07, -8.65900799e-06, -1.84515933e-06]],\n",
       "\n",
       "       [[ 2.61827521e-06, -2.39691725e-06,  3.01503086e-06],\n",
       "        [-7.91696848e-06, -8.33242309e-06, -1.32455805e-06],\n",
       "        [-6.36539686e-06, -1.09145535e-06, -1.30043586e-06],\n",
       "        ...,\n",
       "        [-3.06837782e-06,  8.56087347e-06,  3.90051562e-06],\n",
       "        [ 5.89884035e-06,  5.12895353e-06,  3.91260366e-06],\n",
       "        [ 3.40033024e-06, -2.45386032e-06,  8.76748230e-07]],\n",
       "\n",
       "       [[-7.89017997e-06, -4.39042113e-06,  3.09205691e-06],\n",
       "        [ 5.50535271e-06, -1.54161038e-06, -2.50491756e-06],\n",
       "        [ 3.89887605e-06,  1.39901626e-06, -4.72141134e-06],\n",
       "        ...,\n",
       "        [ 4.37990453e-06, -6.65143424e-07,  1.87538136e-06],\n",
       "        [-1.36028690e-05,  1.42855578e-06,  7.26209632e-07],\n",
       "        [-1.37683172e-06,  2.41249406e-06,  3.46049269e-06]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.19187003e-06,  5.70660158e-06,  1.13472702e-06],\n",
       "        [ 3.46382209e-06,  3.36430470e-06,  7.26080095e-06],\n",
       "        [ 1.29870102e-05, -1.49706269e-06,  4.17969319e-06],\n",
       "        ...,\n",
       "        [-3.88632737e-06,  6.94991770e-06, -2.50485125e-06],\n",
       "        [-5.89699819e-06,  1.55574875e-07,  3.45835860e-07],\n",
       "        [-1.71923142e-06,  1.53100485e-06, -1.83018130e-06]],\n",
       "\n",
       "       [[-2.54749642e-06,  7.76462951e-07, -3.49623427e-06],\n",
       "        [ 1.60855329e-06, -2.38952672e-06,  1.09199011e-05],\n",
       "        [ 4.74923682e-07,  3.03460005e-07, -3.40095057e-06],\n",
       "        ...,\n",
       "        [-3.75747569e-06,  9.17556969e-06,  2.59401847e-06],\n",
       "        [ 2.70936282e-06, -7.13576993e-06,  2.19244833e-06],\n",
       "        [-1.92360313e-06,  2.10275813e-06,  4.97511798e-06]],\n",
       "\n",
       "       [[ 8.99026106e-06, -7.95409951e-07,  1.32648303e-07],\n",
       "        [-5.53021998e-06, -7.13415648e-06, -4.47618296e-07],\n",
       "        [ 1.58716127e-06,  6.38243404e-06,  7.38503687e-06],\n",
       "        ...,\n",
       "        [-2.59411725e-06, -7.34871334e-06, -4.15322195e-07],\n",
       "        [-1.02057657e-06, -1.25213379e-08,  6.10590670e-08],\n",
       "        [ 3.44738006e-06, -3.19578981e-06,  4.70924306e-06]]])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_npm_hat_momentumguess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-9.77891373e-04, -3.59666253e-04, -3.41320765e-04],\n",
       "        [-2.22240525e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "        [-5.95867586e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-9.45407132e-04,  0.00000000e+00, -2.48431182e-06],\n",
       "        [ 0.00000000e+00,  0.00000000e+00, -1.31380931e-03],\n",
       "        [-4.97441756e-04,  0.00000000e+00, -1.57244403e-03]],\n",
       "\n",
       "       [[-3.71726531e-04, -9.77189276e-05,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00, -8.66166328e-04],\n",
       "        [-2.42104716e-05, -7.91208152e-04, -9.80121524e-05],\n",
       "        ...,\n",
       "        [ 0.00000000e+00, -3.13756828e-04,  0.00000000e+00],\n",
       "        [ 0.00000000e+00, -1.11766540e-03,  0.00000000e+00],\n",
       "        [-7.61369650e-04, -9.49755318e-04, -8.23741466e-04]],\n",
       "\n",
       "       [[ 0.00000000e+00, -1.91158581e-04,  0.00000000e+00],\n",
       "        [ 0.00000000e+00, -7.06678345e-04, -7.69026287e-04],\n",
       "        [-4.97924228e-04, -1.00199795e-03,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-3.63985858e-04,  0.00000000e+00, -8.45268465e-04],\n",
       "        [ 0.00000000e+00, -7.23589635e-04,  0.00000000e+00],\n",
       "        [ 0.00000000e+00, -3.96668857e-04, -3.46979524e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-9.20586769e-04,  0.00000000e+00, -1.52249760e-05],\n",
       "        [-1.17828141e-03,  0.00000000e+00,  0.00000000e+00],\n",
       "        [-2.82199725e-04,  0.00000000e+00, -1.14660811e-04],\n",
       "        ...,\n",
       "        [-1.49472461e-03,  0.00000000e+00, -3.02364694e-04],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00, -5.96417964e-04,  0.00000000e+00]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00, -7.33557952e-04],\n",
       "        [ 0.00000000e+00, -1.19257457e-03,  0.00000000e+00],\n",
       "        [-8.80206384e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-3.59160221e-04,  0.00000000e+00, -2.17766535e-04],\n",
       "        [-1.75220001e-04,  0.00000000e+00,  0.00000000e+00],\n",
       "        [-1.52509124e-03,  0.00000000e+00, -6.01378017e-04]],\n",
       "\n",
       "       [[ 0.00000000e+00, -3.08500116e-04,  0.00000000e+00],\n",
       "        [-2.54400847e-04, -1.68900726e-04,  0.00000000e+00],\n",
       "        [-2.88094329e-04, -9.75092612e-05,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00, -5.93019538e-04],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_npm_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.6 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.linalg.norm(GGL._grad_L2loss(beta_npm) * (beta_npm-beta_npm))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 756, 3)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GGL._grad_L2loss(beta_npm).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 756, 3)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(beta_npm-beta_npm).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,756,100) and (100,756,3) not aligned: 100 (dim 2) != 756 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-0e7d18cbf81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mGGL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-321-646bc79217f9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, beta0_npm)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;31m#print(t, beta_npm_hat.shape , beta_npm_hat_momentumguess.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;31m#print(beta_npm_hat_momentumguess.shape, beta_mnp_hat.shape, beta_npm_hat_1.shape, beta_npm_hat_2.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mbeta_npm_hat\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_btalgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_hat_momentumguess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0mbeta_npm_hat_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_npm_hat_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mbeta_npm_hat_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_npm_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-321-646bc79217f9>\u001b[0m in \u001b[0;36m_btalgorithm\u001b[0;34m(self, beta_npm, lam, b, maxiter_bt, rl)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mbeta_npm_postgrad_postprox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_postgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mfz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_L2loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_postgrad_postprox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mfhatz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfhatlambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_npm_postgrad_postprox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_npm_postgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfz\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mfhatz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-321-646bc79217f9>\u001b[0m in \u001b[0;36mfhatlambda\u001b[0;34m(self, lamb, beta_npm_new, beta_npm_old)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfhatlambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_npm_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_npm_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_L2loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_old\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad_L2loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_new\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta_npm_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_npm_new\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta_npm_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manifold_env_april/lib/python3.5/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,756,100) and (100,756,3) not aligned: 100 (dim 2) != 756 (dim 1)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 756, 3)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_npm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2, 756)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.67760213e-06,  3.81233077e-07, -7.79825316e-08],\n",
       "        [-2.10649199e-05,  4.79197776e-06, -2.59872847e-06],\n",
       "        [-4.11438725e-06,  9.45289441e-07, -3.52750644e-06],\n",
       "        ...,\n",
       "        [ 5.41617444e-06, -1.24226880e-06,  3.96044755e-06],\n",
       "        [ 2.20507161e-05, -5.01610737e-06,  2.67973413e-06],\n",
       "        [-2.88558913e-05,  6.56659733e-06, -4.29885136e-06]],\n",
       "\n",
       "       [[-8.34859258e-06, -1.01497111e-05, -1.04150158e-06],\n",
       "        [ 1.95783439e-05,  2.29372310e-05,  2.88353372e-06],\n",
       "        [ 2.69094867e-05,  3.31504802e-05,  3.13487508e-06],\n",
       "        ...,\n",
       "        [-2.79842255e-05, -3.45117250e-05, -3.24108362e-06],\n",
       "        [-1.23443555e-05, -1.41931594e-05, -1.95529021e-06],\n",
       "        [ 2.94837769e-06,  3.54366400e-06,  3.88619777e-07]],\n",
       "\n",
       "       [[-1.46267987e-06, -1.43049082e-06,  2.11252663e-06],\n",
       "        [ 1.44502116e-05,  1.16739871e-05, -9.97688348e-06],\n",
       "        [-1.19165042e-07,  2.37227724e-06, -1.08568316e-05],\n",
       "        ...,\n",
       "        [ 7.37145094e-07, -2.12997009e-06,  1.15687806e-05],\n",
       "        [-1.20732100e-05, -9.81278746e-06,  8.59772909e-06],\n",
       "        [-5.86461570e-06, -1.58763383e-06, -9.91088830e-06]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.86025175e-05, -2.46999220e-05, -3.37313846e-05],\n",
       "        [-7.34927643e-05, -1.76985456e-04,  4.39436631e-05],\n",
       "        [-5.42206755e-05,  7.04201811e-05,  9.78051491e-05],\n",
       "        ...,\n",
       "        [ 5.63386289e-05, -6.71819316e-05, -9.96773307e-05],\n",
       "        [ 6.17777385e-05,  1.74158677e-04, -2.86808423e-05],\n",
       "        [-7.02866698e-05, -1.01086761e-04,  6.42053257e-05]],\n",
       "\n",
       "       [[-2.70136353e-06, -1.16395642e-06,  4.94711463e-06],\n",
       "        [-2.03096884e-06, -6.68216532e-07,  2.32298892e-06],\n",
       "        [-1.83651921e-08,  2.48345991e-06, -1.67825549e-05],\n",
       "        ...,\n",
       "        [ 1.22628741e-06, -2.34820830e-06,  1.71705486e-05],\n",
       "        [ 4.85085824e-06,  1.54076860e-06, -5.17554178e-06],\n",
       "        [-2.40827247e-05, -4.53574137e-06,  4.67853511e-06]],\n",
       "\n",
       "       [[ 3.09202664e-07, -5.45528874e-07,  1.08160726e-06],\n",
       "        [-1.45296071e-05,  1.85299076e-05,  1.28786943e-05],\n",
       "        [-7.69063977e-07,  8.23417278e-07,  2.09285437e-06],\n",
       "        ...,\n",
       "        [ 8.16308970e-07, -8.00634250e-07, -2.87925569e-06],\n",
       "        [ 1.35526618e-05, -1.70800148e-05, -1.38416559e-05],\n",
       "        [-6.58614687e-06,  6.75642744e-06,  2.05696842e-05]]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%timeit\n",
    "GGL._grad_L2loss(beta_npm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#df_M_hat = np.einsum('npd,mpd->mdn',dg_M, beta_mnp)\n",
    "df_M_hat = np.einsum('ndp,npm->ndm',dg_M, beta_npm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "error = df_M_hat - df_M\n",
    "#grad_beta = np.einsum('ndm,ndp->npm',error,dg_M) #+ reg_l2 * np.ones()\n",
    "##if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "grad_beta = np.einsum('ndm,ndp->npm',error,dg_M) #+ reg_l2 * np.ones()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta_npm = rearrange(beta_mnp, 'm n p -> n p m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15311784236143913"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%timeit\n",
    "GGL._L2loss(beta_npm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 µs ± 2.79 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "GGL._L1penalty(beta_npm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 µs ± 5.43 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "GGL._prox(beta_npm, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2,100) (100,2,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-50063962fe64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrad_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_M_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf_M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdg_M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2,100) (100,2,3) "
     ]
    }
   ],
   "source": [
    "        \n",
    "        beta_mnp = resh_mnp\n",
    "        dg_M = replicate.dg_M\n",
    "        df_M = replicate.df_M\n",
    "        df_M_hat = np.einsum('ijk,lij->lki',dg_M, beta_mnp)\n",
    "        #z = np.dot(X, beta)\n",
    "        #grad_beta = 1. / n_samples * np.transpose(np.dot(np.transpose(z - y), X))\n",
    "        \n",
    "        error = df_M_hat - df_M\n",
    "        grad_beta = np.einsum('ndm,npd->mnp',error,dg_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 756)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum('ndm,npd->mnp',df_M,dg_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2, 3)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 756, 2)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 756)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_mnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The true champion\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import jacobian\n",
    "from autograd import elementwise_grad\n",
    "from autograd import grad\n",
    "\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "\n",
    "#import numpy as np\n",
    "from scipy.special import expit\n",
    "from pyglmnet import utils\n",
    "\n",
    "\n",
    "class GLM:\n",
    "    \n",
    "    def __init__(self, xs, ys, reg_lambda, group,max_iter, learning_rate, tol,parameter):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.group = np.asarray(group)\n",
    "        #print(self.group.shape)\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tol = tol\n",
    "        self.Tau = None\n",
    "        self.alpha = 1.\n",
    "        self.lossresults = {}\n",
    "        self.dls = {}\n",
    "        self.parameter = parameter\n",
    "        self.l2loss = {}\n",
    "        self.penalty = {}\n",
    "        \n",
    "    def _prox(self,beta, thresh):\n",
    "        \"\"\"Proximal operator.\"\"\"\n",
    "        \n",
    "        #print(thresh, beta)\n",
    "        #print('beginprox', beta[0:2],thresh)\n",
    "        group_ids = np.unique(self.group)\n",
    "        result = np.zeros(beta.shape)\n",
    "        result = np.asarray(result, dtype = float)\n",
    "        #print('gids',group_ids)\n",
    "        for i in range(len(group_ids)):\n",
    "            gid = i \n",
    "            #print(self.group)\n",
    "            idxs_to_update = np.where(self.group == gid)[0]\n",
    "            #print('idx',idxs_to_update)\n",
    "            #print('norm', np.linalg.norm(beta[idxs_to_update]))\n",
    "            if np.linalg.norm(beta[idxs_to_update]) > 0.:\n",
    "                #print('in here', len(idxs_to_update))\n",
    "                potentialoutput = beta[idxs_to_update] - (thresh / np.linalg.norm(beta[idxs_to_update])) * beta[idxs_to_update]\n",
    "                posind = np.where(beta[idxs_to_update] > 0.)[0]\n",
    "                negind = np.where(beta[idxs_to_update] < 0.)[0]\n",
    "                po = beta[idxs_to_update].copy()\n",
    "                #print('potention', potentialoutput[0:2])\n",
    "                po[posind] = np.asarray(np.clip(potentialoutput[posind],a_min = 0., a_max = 1e15), dtype = float)\n",
    "                po[negind] = np.asarray(np.clip(potentialoutput[negind],a_min = -1e15, a_max = 0.), dtype = float)\n",
    "                result[idxs_to_update] = po\n",
    "        #print('end', result[0:2])\n",
    "        return result\n",
    "\n",
    "    def _grad_L2loss(self, beta, X, y):\n",
    "        #print(beta.shape,X.shape,y.shape)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "        n_samples = np.float(X.shape[0])\n",
    "        z = np.dot(X, beta)\n",
    "        #grad_beta = 1. / n_samples * np.transpose(np.dot(np.transpose(z - y), X))\n",
    "        grad_beta = np.transpose(np.dot(np.transpose(z - y), X))\n",
    "        #print('gb',grad_beta.shape)\n",
    "        return grad_beta\n",
    "    \n",
    "    def _loss(self,beta, reg_lambda, X, y):\n",
    "        \"\"\"Define the objective function for elastic net.\"\"\"\n",
    "        L = self._logL(beta, X, y)\n",
    "        P = self._L1penalty(beta)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "    \n",
    "    def _logL(self,beta, X, y):\n",
    "        \"\"\"The log likelihood.\"\"\"\n",
    "        #print('beginlogL', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y),y.shape,beta.shape,X.shape,)\n",
    "        l = np.dot(X, beta)\n",
    "        logL = -0.5 * np.sum((y - l)**2)\n",
    "        #print('endlogL',logL)\n",
    "        return logL\n",
    "    \n",
    "    def _L2loss(self,beta,X,y):\n",
    "        #print('beginl2', np.linalg.norm(beta), np.linalg.norm(X), np.linalg.norm(y), y.shape)\n",
    "        output = -self._logL(beta, X, y)\n",
    "        #print('outl2',output)\n",
    "        return(output)\n",
    "    \n",
    "    def _L1penalty(self, beta):\n",
    "        \"\"\"The L1 penalty\"\"\"\n",
    "        # Compute the L1 penalty\n",
    "        group = self.group\n",
    "        group_ids = np.unique(self.group)\n",
    "        L1penalty = 0.0\n",
    "        for group_id in group_ids:\n",
    "            L1penalty += np.linalg.norm(beta[self.group == group_id], 2)\n",
    "        return L1penalty\n",
    "    \n",
    "    #def fhatlambda(self,lamb,x,y):\n",
    "    def fhatlambda(self,lamb,betanew,betaold):\n",
    "        xs = self.xs\n",
    "        ys = self.ys\n",
    "        #print(ys.shape,'fhatlam')\n",
    "        #print(self._L2loss(betaold,xs,ys),self._L2loss(betanew,xs,ys),'old','new') \n",
    "        output = self._L2loss(betaold,xs,ys) + np.dot(self._grad_L2loss(betaold,xs,ys).transpose(),(betanew-betaold)) + (1/(2*lamb)) * np.linalg.norm(betanew-betaold)**2\n",
    "        return(output)\n",
    "    \n",
    "    #_btalgorithm(yk,lamb,.5,1000, rl)\n",
    "    def _btalgorithm(self,bet,lam,b,maxx,rl):\n",
    "        \n",
    "        #print('lam',lam)\n",
    "        X = self.xs\n",
    "        y = self.ys\n",
    "        #print('beginbt', np.linalg.norm(y))\n",
    "        #print('beginbt',self._L2loss(bet,X,y))\n",
    "        #print(np.linalg.norm(bet))\n",
    "        grad_beta = self._grad_L2loss(beta = bet, X = X, y = y)\n",
    "        for i in range(maxx):\n",
    "            betn = bet - lam * grad_beta\n",
    "            z = self._prox(betn, lam * rl)\n",
    "            fz = self._L2loss(z,X,y)\n",
    "            #print(fz,'fz')\n",
    "            fhatz = self.fhatlambda(lam,z, bet)\n",
    "            if fz <= fhatz:\n",
    "            #print(fhatz - fz)\n",
    "            #if 0 <= 1:\n",
    "                break\n",
    "            lam = b*lam\n",
    "        return(z,lam)\n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "\n",
    "        group  = self.group\n",
    "        print(group.shape)\n",
    "        lambdas = self.reg_lambda\n",
    "        parameter = self.parameter\n",
    "        X = self.xs\n",
    "        #print(X.shape)\n",
    "        y = self.ys\n",
    "        \n",
    "        np.random.RandomState(0)\n",
    "        group = np.asarray(group, dtype = np.int64)\n",
    "\n",
    "        #print(group.shape[0])\n",
    "        group.dtype = np.int64\n",
    "        #print(group.shape[0])\n",
    "        #print(X.shape[1])\n",
    "        if group.shape[0] != X.shape[1]:\n",
    "            raise ValueError('group should be (n_features,)')\n",
    "\n",
    "        # type check for data matrix\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError('Input data should be of type ndarray (got %s).'\n",
    "                             % type(X))\n",
    "\n",
    "        n_features = np.float(X.shape[1])\n",
    "        n_features = np.int64(n_features)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, np.newaxis]\n",
    "            self.ys = y\n",
    "        #print(y.shape)\n",
    "        n_classes = 1\n",
    "        n_classes = np.int64(n_classes)\n",
    "\n",
    "        beta_hat = 1 / (n_features) * np.random.normal(0.0, 1.0, [n_features, n_classes])\n",
    "        fit_params = list()\n",
    "        \n",
    "        for l, rl in enumerate(lambdas):\n",
    "            fit_params.append({'beta': beta_hat})\n",
    "            if l == 0:\n",
    "                fit_params[-1]['beta'] = beta_hat\n",
    "            else:\n",
    "                fit_params[-1]['beta'] = fit_params[-2]['beta']\n",
    "            tol = self.tol\n",
    "            alpha = 1.\n",
    "            beta = np.zeros([n_features, n_classes])\n",
    "            beta = fit_params[-1]['beta']\n",
    "            #print('losser',self._L2loss(beta,X,y))\n",
    "            g = np.zeros([n_features, n_classes])\n",
    "            L, DL ,L2,PEN = list(), list() , list(), list()\n",
    "            lamb = self.learning_rate\n",
    "            bm1 = beta.copy()\n",
    "            bm2 = beta.copy()\n",
    "            for t in range(0, self.max_iter):\n",
    "                L.append(self._loss(beta, rl, X, y))\n",
    "                L2.append(self._L2loss(beta,X,y))\n",
    "                PEN.append(self._L1penalty(beta))\n",
    "                w = (t / (t+ 3))\n",
    "                yk = beta + w*(bm1 - bm2)\n",
    "                #print('losser',self._L2loss(yk,X,y))\n",
    "                #print('beforebt',np.linalg.norm(yk),np.linalg.norm(X),np.linalg.norm(y))\n",
    "                beta , lamb = self._btalgorithm(yk,lamb,.5,1000, rl)\n",
    "                #X = self.xs\n",
    "                #y = self.ys\n",
    "                #print('losser2',self._L2loss(beta,X,y))\n",
    "                bm2 = bm1.copy()\n",
    "                bm1 = beta.copy()\n",
    "                if t > 1:\n",
    "                    DL.append(L[-1] - L[-2])\n",
    "                    if np.abs(DL[-1] / L[-1]) < tol:\n",
    "                        print('converged', rl)\n",
    "                        msg = ('\\tConverged. Loss function:'\n",
    "                               ' {0:.2f}').format(L[-1])\n",
    "                        msg = ('\\tdL/L: {0:.6f}\\n'.format(DL[-1] / L[-1]))\n",
    "                        break\n",
    "                    \n",
    "            #print(beta)\n",
    "            fit_params[-1]['beta'] = beta\n",
    "            self.lossresults[rl] = L\n",
    "            self.l2loss[rl] = L2\n",
    "            self.penalty[rl] = PEN\n",
    "            self.dls[rl] = DL\n",
    "            #print(L)\n",
    "        # Update the estimated variables\n",
    "        \n",
    "        self.fit_ = fit_params\n",
    "        self.ynull_ = np.mean(y)\n",
    "\n",
    "        # Return\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manifold_env_april2",
   "language": "python",
   "name": "manifold_env_april2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
