%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for mac at 2019-10-07 17:46:55 -0700


@Article{ChenBuja:localMDS09,
  author =       {Lisha Chen and Andreas Buja},
  title =        {Local {Multidimensional} {Scaling} for nonlinear dimension reduction, graph drawing and proximity analysis},
  journal =      {Journal of the American Statistical Association},
  year =         {2009},
  volume =       {104},
  number =       {485},
  pages =        {209--219},
  month =        {March},
}


@ARTICLE{Fefferman2016-bc,
  title    = "Testing the manifold hypothesis",
  author   = "Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan",
  abstract = "The hypothesis that high dimensional data tend to lie in the
              vicinity of a low dimensional manifold is the basis of manifold
              learning. The goal of this paper is to develop an algorithm (with
              accompanying complexity guarantees) for testing the existence of
              a manifold that fits a probability distribution supported in a
              separable Hilbert space, only using i.i.d. samples from that
              distribution. More precisely, our setting is the following.
              Suppose that data are drawn independently at random from a
              probability distribution supported on the unit ball of a
              separable Hilbert space . Let be the set of submanifolds of the
              unit ball of whose volume is at most and reach (which is the
              supremum of all such that any point at a distance less than has a
              unique nearest point on the manifold) is at least . Let denote
              the mean-squared distance of a random point from the probability
              distribution to . We obtain an algorithm that tests the manifold
              hypothesis in the following sense.",
  journal  = "J. Amer. Math. Soc.",
  volume   =  29,
  number   =  4,
  pages    = "983--1049",
  month    =  feb,
  year     =  2016
}


@book{Wasserman:2006:NS:1202956,
 author = {Wasserman, Larry},
 title = {All of Nonparametric Statistics (Springer Texts in Statistics)},
 year = {2006},
 isbn = {0387251456},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}

%% Saved with string encoding Unicode (UTF-8)
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{yalcin2001,
author = "Yalcin, Ilker and Amemiya, Yasuo",
doi = "10.1214/ss/1009213729",
fjournal = "Statistical Science",
journal = "Statist. Sci.",
month = "08",
number = "3",
pages = "275--294",
publisher = "The Institute of Mathematical Statistics",
title = "Nonlinear Factor Analysis as a Statistical Method",
url = "https://doi.org/10.1214/ss/1009213729",
volume = "16",
year = "2001"
}

@article{jumarMohriTalwakar:12,
	Author = {{Kumar}, Sanjiv and {Mohri}, Mehryar and {Talwalkar}, Ameet},
	Date-Added = {2019-10-08 00:46:33 +0000},
	Date-Modified = {2019-10-08 00:46:53 +0000},
	Journal = {Journal of Machine Learning Research},
	Keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	Month = {Apr},
	Pages = {981-1006},
	Primaryclass = {stat.ML},
	Title = {{Sampling Methods for the Nyström Method}},
	Year = {2012}}



@article{arxivVersion,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181111891M},
	Archiveprefix = {arXiv},
	Author = {{Meila}, Marina and {Koelle}, Samson and {Zhang}, Hanyu},
	Date-Added = {2019-10-08 00:46:33 +0000},
	Date-Modified = {2019-10-08 00:46:53 +0000},
	Eid = {arXiv:1811.11891},
	Eprint = {1811.11891},
	Journal = {arXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	Month = {Nov},
	Pages = {arXiv:1811.11891},
	Primaryclass = {stat.ML},
	Title = {{A regression approach for explaining manifold embedding coordinates}},
	Year = {2018}}

	@article{vanderplasConnolly:09,
 author={Jake Vanderplas and Andrew Connolly},
 title={Reducing the Dimensionality of Data: Locally Linear Embedding of
Sloan Galaxy Spectra},
 journal={The Astronomical Journal},
 volume={138},
 number={5},
 pages={1365},
 url={http://stacks.iop.org/1538-3881/138/i=5/a=1365},
 year={2009},
 abstract={We introduce locally linear embedding (LLE) to the
astronomical community as a new classification technique, using Sloan
Digital Sky Survey spectra as an example data set. LLE is a nonlinear
dimensionality reduction technique that has been studied in the context of
computer perception. We compare the performance of LLE to well-known
spectral classification techniques, e.g., principal component analysis and
line-ratio diagnostics. We find that LLE combines the strengths of both
methods in a single, coherent technique, and leads to improved
classification of emission-line spectra at a relatively small
computational cost. We also present a data subsampling technique that
preserves local information content, and proves effective for creating
small, efficient training samples from large, high-dimensional data sets.
Software used in this LLE-based classification is made available.}
}

@article{tsne,
	Author = {Laurens van der {Maaten} and Geoffrey {Hinton}},
	Date-Added = {2019-10-08 00:43:06 +0000},
	Date-Modified = {2019-10-08 00:44:28 +0000},
	Journal = {Journal of Machine Learning Research},
	Month = {Nov},
	Pages = {2579-2605},
	Title = {Visualizing Data using t-SNE},
	Volume = {9},
	Year = {2008}}

@book{AbrahamMarsden,
	Address = {Providence, RI},
	Author = {Ralph Abraham and Jerrod E. Marsden},
	Edition = {Second},
	Publisher = {American Mathematical Society},
	Title = {Foundations of Mechanics},
	Year = {1978}}

@book{morse,
	Abstract = {<p>One of the most cited books in mathematics, John Milnor's exposition of Morse theory has been the most important book on the subject for more than forty years. Morse theory was developed in the 1920s by mathematician Marston Morse. (Morse was on the faculty of the Institute for Advanced Study, and Princeton published his<em>Topological Methods in the Theory of Functions of a Complex Variable</em>in the Annals of Mathematics Studies series in 1947.) One classical application of Morse theory includes the attempt to understand, with only limited information, the large-scale structure of an object. This kind of problem occurs in mathematical physics, dynamic systems, and mechanical engineering. Morse theory has received much attention in the last two decades as a result of a famous paper in which theoretical physicist Edward Witten relates Morse theory to quantum field theory.</p><p>Milnor was awarded the Fields Medal (the mathematical equivalent of a Nobel Prize) in 1962 for his work in differential topology. He has since received the National Medal of Science (1967) and the Steele Prize from the American Mathematical Society twice (1982 and 2004) in recognition of his explanations of mathematical concepts across a wide range of scienti.c disciplines. The citation reads, "The phrase sublime elegance is rarely associated with mathematical exposition, but it applies to all of Milnor's writings. Reading his books, one is struck with the ease with which the subject is unfolding and it only becomes apparent after re.ection that this ease is the mark of a master.?</p><p>Milnor has published five books with Princeton University Press.</p>},
	Author = {J. Milnor and M. SPIVAK and R. WELLS},
	Date-Added = {2019-06-24 18:59:43 +0000},
	Date-Modified = {2019-06-24 18:59:54 +0000},
	Isbn = {9780691080086},
	Publisher = {Princeton University Press},
	Title = {Morse Theory. (AM-51), Volume 51},
	Url = {http://www.jstor.org/stable/j.ctv3f8rb6},
	Year = {1969},
	Bdsk-Url-1 = {http://www.jstor.org/stable/j.ctv3f8rb6}}

@inproceedings{AgarwalMRSWY15,
	Author = {Pankaj K. Agarwal and Thomas M{\o}lhave and Morten Revsb{\ae}k and Issam Safa and Yusu Wang and Jungwoo Yang},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/conf/compgeom/AgarwalMRSWY15},
	Booktitle = {31st International Symposium on Computational Geometry, SoCG 2015},
	Doi = {10.4230/LIPIcs.SOCG.2015.796},
	Pages = {796--811},
	Timestamp = {Wed, 17 May 2017 14:24:34 +0200},
	Title = {Maintaining Contour Trees of Dynamic Terrains},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.4230/LIPIcs.SOCG.2015.796}}

@inproceedings{Backurs2018-cj,
	Abstract = {Given a kernel function k(.,.) and a dataset P$\subset$ R^d, the
               kernel density function of P at a point x$\in$ Rdis equal to
               KDFP(x):= 1/|P| $\Sigma$y$\in$P k(x, y). Kernel density
               evaluation has numerous applications, in scientific computing,
               statistics, computer vision, machine learning and other fields.
               In all of them it is necessary to evaluate KDFP(x)quickly, often
               for many inputs x and large point-sets P. In this paper we
               present a collection of algorithms for efficient KDF evaluation
               under the assumptions that the kernel k is ``smooth'', i.e. the
               value changes at most polynomially with the distance. This
               assumption is satisfied by several well-studied kernels,
               including the (generalized) t-student kernel and rational
               quadratic kernel. For smooth kernels, we give a data structure
               that, after O(dn log ($\Phi$ n)/$\epsilon$^2) preprocessing,
               estimates KDFP(x)up to a factor of 1 $\pm$ $\epsilon$ in O(dlog
               ($\Phi$ n)/$\epsilon$2) time, where Phi; is the aspect ratio.
               The log($\Phi$n) term can be further replaced by log n under an
               additional decay condition on k, which is satisfied by the
               aforementioned examples. We further extend the results in two
               ways. First, we use low-distortion embeddings to extend the
               results to kernels defined for spaces other than ?\_2. The key
               feature of this reduction is that the distortion of the
               embedding affects only the running time of the algorithm, not
               the accuracy of the estimation. As a result, we obtain
               (1+$\epsilon$)-approximate estimation algorithms for kernels
               over other ?pnorms, Earth-Mover Distance, and other metric
               spaces. Second, for smooth kernels that are decreasing with
               distance, we present a general reduction from density estimation
               to approximate near neighbor in the underlying space. This
               allows us to construct algorithms for general doubling metrics,
               as well as alternative algorithms for lpnorms and other spaces.},
	Author = {Backurs, A and Charikar, M and Indyk, P and Siminelakis, P},
	Booktitle = {2018 {IEEE} 59th Annual Symposium on Foundations of Computer Science ({FOCS})},
	Keywords = {data handling;data structures;polynomials;smooth kernels;kernel density function;kernel density evaluation;well-studied kernels;t-student kernel;rational quadratic kernel;density estimation;kernel function;KDF evaluation;approximate estimation algorithms;Kernel;Data structures;Approximation algorithms;Measurement;Estimation;Complexity theory;Distortion;Heavy-tailed kernels, Dimensionality Reduction, Quad-Trees, Hashing, Approximate Nearest Neighbor Search},
	Month = oct,
	Pages = {615--626},
	Title = {Efficient Density Evaluation for Smooth Kernels},
	Year = 2018}

@article{bates:16,
	Author = {Bates, Jonathan},
	Journal = {Applied and Computational Harmonic Analysis},
	Number = {3},
	Pages = {516--530},
	Publisher = {Elsevier},
	Title = {The embedding dimension of Laplacian eigenfunction maps},
	Volume = {37},
	Year = {2014}}

@inproceedings{belkin:01,
	Address = {Cambridge, MA},
	Author = {M. Belkin and P. Niyogi},
	Booktitle = {Advances in Neural Information Processing Systems 14},
	Original = {orig/AA42.ps},
	Publisher = {MIT Press},
	Title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
	Year = {2002}}

@incollection{belkin:07,
	Author = {Belkin, Mikhail and Partha Niyogi},
	Booktitle = {Advances in Neural Information Processing Systems 19},
	Editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
	Pages = {129--136},
	Publisher = {MIT Press},
	Title = {Convergence of Laplacian Eigenmaps},
	Url = {http://papers.nips.cc/paper/2989-convergence-of-laplacian-eigenmaps.pdf},
	Year = {2007},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/2989-convergence-of-laplacian-eigenmaps.pdf}}

@article{berardBessonGallot:94,
	Author = {P. B\'{e}rard and G. Besson, and S. Gallot},
	Journal = {Geometric Functional Analysis},
	Number = {4},
	Pages = {373--398},
	Title = {Embedding Riemannian manifolds by their heat kernel},
	Volume = {4},
	Year = {1994}}

@misc{bernsteinDeSilvaLangfordTenn:00,
	Author = {Mira Bernstein and Vin {de Silva} and John C. Langford and Josh Tennenbaum},
	Howpublished = {{\tt http://web.mit.edu/cocosci/isomap/BdSLT.pdf}},
	Month = {December},
	Title = {Graph approximations to geodesics on embedded manifolds},
	Year = {2000}}

@article{berry:16,
	Author = {T. Berry and J. Harlim},
	Journal = {Applied and Computational Harmonic Analysis},
	Number = {1},
	Pages = {68-96},
	Title = {Variable bandwidth diffusion kernels},
	Volume = {40},
	Year = {2016}}

@inproceedings{blau2017non,
  title={Non-redundant Spectral Dimensionality Reduction},
  author={Blau, Yochai and Michaeli, Tomer},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={256--271},
  year={2017},
  organization={Springer}
}

@article{bruntonBruntonProctorKutz:16,
	Author = {Steven L. Brunton and Bingni W. Brunton and Joshua L. Proctor and J. Nathan Kutz},
	Journal = {PLoS ONE},
	Number = {2},
	Pages = {e0150171},
	Title = {Koopman Invariant Subspaces and Finite Linear Representations of Nonlinear Dynamical Systems for Control},
	Volume = {11},
	Year = {2016}}

@article{Constantine2014-qi,
	Abstract = {Many multivariate functions in engineering models vary primarily
               along a few directions in the space of input parameters. When
               these directions correspond to coordinate directions, one may
               apply global sensitivity measures to determine the most
               influential parameters. However, these methods perform poorly
               when the directions of variability are not aligned with the
               natural coordinates of the input space. We present a method to
               first detect the directions of the strongest variability using
               evaluations of the gradient and subsequently exploit these
               directions to construct a response surface on a low-dimensional
               subspace---i.e., the active subspace---of the inputs. We develop
               a theoretical framework with error bounds, and we link the
               theoretical quantities to the parameters of a kriging response
               surface on the active subspace. We apply the method to an
               elliptic PDE model with coefficients parameterized by 100
               Gaussian random variables and compare it with a local
               sensitivity analysis method for dimension reduction.},
	Author = {Constantine, P and Dow, E and Wang, Q},
	Journal = {SIAM J. Sci. Comput.},
	Month = jan,
	Number = 4,
	Pages = {A1500--A1524},
	Publisher = {Society for Industrial and Applied Mathematics},
	Title = {Active Subspace Methods in Theory and Practice: Applications to Kriging Surfaces},
	Volume = 36,
	Year = 2014}

@inproceedings{ChazalGM16,
	Author = {Fr{\'{e}}d{\'{e}}ric Chazal and Ilaria Giulini and Bertrand Michel},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/conf/nips/ChazalGM16},
	Booktitle = {Advances in Neural Information Processing Systems 29},
	Pages = {3963--3971},
	Timestamp = {Fri, 03 Mar 2017 14:59:41 +0100},
	Title = {Data driven estimation of Laplace-Beltrami operator},
	Url = {http://papers.nips.cc/paper/6210-data-driven-estimation-of-laplace-beltrami-operator},
	Year = {2016},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6210-data-driven-estimation-of-laplace-beltrami-operator}}

@article{ChazalGLM15,
	Author = {Fr{\'{e}}d{\'{e}}ric Chazal and Marc Glisse and Catherine Labru{\`{e}}re and Bertrand Michel},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/journals/jmlr/ChazalGLM15},
	Journal = {Journal of Machine Learning Research},
	Pages = {3603--3635},
	Timestamp = {Mon, 01 May 2017 11:48:32 +0200},
	Title = {Convergence rates for persistence diagram estimation in topological data analysis},
	Url = {http://dl.acm.org/citation.cfm?id=2912112},
	Volume = {16},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2912112}}

@inbook{Chen2013,
	Abstract = {Large data sets arise in a wide variety of applications and are often modeled as samples from a probability distribution in high-dimensional space. It is sometimes assumed that the support of such probability distribution is well approximated by a set of low intrinsic dimension, perhaps even a low-dimensional smooth manifold. Samples are often corrupted by high-dimensional noise. We are interested in developing tools for studying the geometry of such high-dimensional data sets. In particular, we present here a multiscale transform that maps high-dimensional data as above to a set of multiscale coefficients that are compressible/sparse under suitable assumptions on the data. We think of this as a geometric counterpart to multi-resolution analysis in wavelet theory: whereas wavelets map a signal (typically low dimensional, such as a one-dimensional time series or a two-dimensional image) to a set of multiscale coefficients, the geometric wavelets discussed here map points in a high-dimensional point cloud to a multiscale set of coefficients. The geometric multi-resolution analysis (GMRA) we construct depends on the support of the probability distribution, and in this sense it fits with the paradigm of dictionary learning or data-adaptive representations, albeit the type of representation we construct is in fact mildly nonlinear, as opposed to standard linear representations. Finally, we apply the transform to a set of synthetic and real-world data sets.},
	Address = {Boston},
	Author = {Chen, Guangliang and Little, Anna V. and Maggioni, Mauro},
	Booktitle = {Excursions in Harmonic Analysis, Volume 1: The February Fourier Talks at the Norbert Wiener Center},
	Doi = {10.1007/978-0-8176-8376-4-13},
	Editor = {Andrews, Travis D. and Balan, Radu and Benedetto, John J. and Czaja, Wojciech and Okoudjou, Kasso A.},
	Isbn = {978-0-8176-8376-4},
	Pages = {259--285},
	Publisher = {Birkh{\"a}user Boston},
	Title = {Multi-Resolution Geometric Analysis for Data in High Dimensions},
	Url = {https://doi.org/10.1007/978-0-8176-8376-4-13},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-0-8176-8376-4-13},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-0-8176-8376-4-13}}

@article{Chen2016-eq,
	Abstract = {This paper describes InfoGAN, an information-theoretic
                   extension to the Generative Adversarial Network that is able
                   to learn disentangled representations in a completely
                   unsupervised manner. InfoGAN is a generative adversarial
                   network that also maximizes the mutual information between a
                   small subset of the latent variables and the observation. We
                   derive a lower bound to the mutual information objective
                   that can be optimized efficiently, and show that our
                   training procedure can be interpreted as a variation of the
                   Wake-Sleep algorithm. Specifically, InfoGAN successfully
                   disentangles writing styles from digit shapes on the MNIST
                   dataset, pose from lighting of 3D rendered images, and
                   background digits from the central digit on the SVHN
                   dataset. It also discovers visual concepts that include hair
                   styles, presence/absence of eyeglasses, and emotions on the
                   CelebA face dataset. Experiments show that InfoGAN learns
                   interpretable representations that are competitive with
                   representations learned by existing fully supervised
                   methods.},
	Archiveprefix = {arXiv},
	Author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	Eprint = {1606.03657},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{InfoGAN}: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
	Year = 2016}

@article{Dsilva13-Nonlinear,
	Author = {Dsilva,Carmeline J. and Talmon,Ronen and Rabin,Neta and Coifman,Ronald R. and Kevrekidis,Ioannis G.},
	Doi = {10.1063/1.4828457},
	Eprint = {https://doi.org/10.1063/1.4828457},
	Journal = {The Journal of Chemical Physics},
	Number = {18},
	Pages = {184109},
	Title = {Nonlinear intrinsic variables and state reconstruction in multiscale simulations},
	Url = {https://doi.org/10.1063/1.4828457},
	Volume = {139},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1063/1.4828457},
	Bdsk-Url-2 = {http://dx.doi.org/10.1063/1.4828457}}

@article{dsilva2018parsimonious,
    title={Parsimonious representation of nonlinear dynamical systems through manifold learning: A chemotaxis case study},
    author={Dsilva, Carmeline J and Talmon, Ronen and Coifman, Ronald R and Kevrekidis, Ioannis G},
    journal={Applied and Computational Harmonic Analysis},
    volume={44},
    number={3},
    pages={759--773},
    year={2018},
    publisher={Elsevier}
}

@inproceedings{ChenGHW15,
	Author = {Yen{-}Chi Chen and Christopher R. Genovese and Shirley Ho and Larry A. Wasserman},
	Booktitle = {Advances in Neural Information Processing Systems 28},
	Pages = {316--324},
	Title = {Optimal Ridge Detection using Coverage Risk},
	Url = {http://papers.nips.cc/paper/5996-optimal-ridge-detection-using-coverage-risk},
	Year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5996-optimal-ridge-detection-using-coverage-risk}}

@article{ChenGW14,
	Author = {Yen{-}Chi Chen and Christopher R. Genovese and Larry A. Wasserman},
	Journal = {arxiv},
	Title = {Generalized Mode and Ridge Estimation},
	Url = {http://arxiv.org/abs/1406.1803},
	Volume = {1406.1803},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1406.1803}}

@article{ChenGW13,
	Author = {Yen{-}Chi Chen and Christopher R. Genovese and Larry A. Wasserman},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChenGW13},
	Journal = {arxiv},
	Timestamp = {Wed, 07 Jun 2017 14:41:29 +0200},
	Title = {Uncertainty Measures and Limiting Distributions for Filament Estimation},
	Url = {http://arxiv.org/abs/1312.2098},
	Volume = {1312.2098},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1312.2098}}

@inproceedings{Cohen:2015:DRK:2746539.2746569,
	Acmid = {2746569},
	Address = {New York, NY, USA},
	Author = {Cohen, Michael B. and Elder, Sam and Musco, Cameron and Musco, Christopher and Persu, Madalina},
	Booktitle = {Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing},
	Doi = {10.1145/2746539.2746569},
	Isbn = {978-1-4503-3536-2},
	Keywords = {Johnson-Lindenstrauss, clustering, dimensionality reduction, distributed, feature extraction, feature selection, k-means, low rank approximation, principal component analysis, projection cost preserving, singular value decomposition, sketching, streaming},
	Location = {Portland, Oregon, USA},
	Numpages = {10},
	Pages = {163--172},
	Publisher = {ACM},
	Series = {STOC '15},
	Title = {Dimensionality Reduction for k-Means Clustering and Low Rank Approximation},
	Url = {http://doi.acm.org.offcampus.lib.washington.edu/10.1145/2746539.2746569},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org.offcampus.lib.washington.edu/10.1145/2746539.2746569},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/2746539.2746569}}

@article{coifman:06,
	Author = {R. R. Coifman and S. Lafon},
	Journal = {Applied and Computational Harmonic Analysis},
	Number = {1},
	Pages = {5--30},
	Title = {Diffusion Maps},
	Volume = {30},
	Year = {2006}}

@article{Singer2011VectorDM,
  title={Vector Diffusion Maps and the Connection Laplacian.},
  author={Amit Singer and H-T Wu},
  journal={Communications on pure and applied mathematics},
  year={2011},
  volume={65 8}
}

@inproceedings{CoiLafLeeMag05,
	Author = {Coifman, R. R. and Lafon, S. and Lee, A. and Maggioni and Warner and Zucker},
	Booktitle = {Proceedings of the National Academy of Sciences},
	Pages = {7426--7431},
	Title = {Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps},
	Year = {2005}}

@inproceedings{connor.16b,
	Address = {Barcelona, Spain},
	Author = {Connor, M. and Rozell, C.},
	Booktitle = {Neural Information Processing Systems (NIPS) Workshop, Brains and Bits: Neuroscience Meets Machine Learning},
	Month = dec,
	Title = {Unsupervised learning of manifold models for neural coding of physical transformations in the ventral visual pathway},
	Year = 2016}

@article{cunninghamYu:14,
	Author = {John P. Cunningham and Byron M. Yu},
	Journal = {Nature Neuroscience},
	Pages = {1500---1509},
	Title = {Dimensionality reduction for large-scale neural recordings},
	Volume = {16},
	Year = {2014}}

@article{elhamifarVidal:13,
	Author = {Ehsan Elhamifar and Ren{\'{e}} Vidal},
	Journal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	Number = {11},
	Pages = {2765--2781},
	Title = {Sparse Subspace Clustering: Algorithm, Theory, and Applications},
	Url = {https://doi.org/10.1109/TPAMI.2013.57},
	Volume = {35},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/TPAMI.2013.57}}

@inproceedings{DeySW16,
	Author = {Tamal K. Dey and Dayu Shi and Yusu Wang},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/conf/esa/DeySW16},
	Booktitle = {24th Annual European Symposium on Algorithms, {ESA} 2016},
	Doi = {10.4230/LIPIcs.ESA.2016.35},
	Pages = {35:1--35:16},
	Timestamp = {Fri, 26 May 2017 00:49:36 +0200},
	Title = {SimBa: An Efficient Tool for Approximating Rips-Filtration Persistence via Simplicial Batch-Collapse},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.4230/LIPIcs.ESA.2016.35}}

@article{Dong_undated-ux,
	Author = {Dong, Yihe and Indyk, Piotr and Razenshteyn, Ilya},
	Title = {Learning {Sublinear-Time} Indexing for Nearest Neighbor Search}}

@article{feffermanIKLNarayanan:whitney15,
	Author = {Charles Fefferman and Sergei Ivanov and Yaroslav Kurylev and Matti Lassas and Hariharan Narayanan},
	Journal = {arxiv},
	Title = {Reconstruction and interpolation of manifolds I: The geometric Whitney problem},
	Volume = {1508.00674},
	Year = {2015}}

@article{Gear2012-rj,
	Abstract = {In this report we consider the parameterization of
                   low-dimensional manifolds that are specified (approximately)
                   by a set of points very close to the manifold in the
                   original high-dimensional space. Our objective is to obtain
                   a parameterization that is (1-1) and non singular (in the
                   sense that the Jacobian of the map between the manifold and
                   the parameter space is bounded and non singular).},
	Archiveprefix = {arXiv},
	Author = {Gear, C W},
	Eprint = {1208.5246},
	Month = aug,
	Primaryclass = {physics.comp-ph},
	Title = {Parameterization of non-linear manifolds},
	Year = 2012}

@article{GenovesePVW12,
	Author = {Christopher R. Genovese and Marco Perone{-}Pacifico and Isabella Verdinelli and Larry A. Wasserman},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/journals/jmlr/GenovesePVW12},
	Journal = {Journal of Machine Learning Research},
	Pages = {1263--1291},
	Timestamp = {Wed, 30 Mar 2016 23:40:00 +0200},
	Title = {Minimax Manifold Estimation},
	Url = {http://dl.acm.org/citation.cfm?id=2343687},
	Volume = {13},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2343687}}

@inproceedings{gerber2007robust,
  title={Robust non-linear dimensionality reduction using successive 1-dimensional Laplacian eigenmaps},
  author={Gerber, Samuel and Tasdizen, Tolga and Whitaker, Ross},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={281--288},
  year={2007},
  organization={ACM}
}

@article{goldberg08,
  title={Manifold learning: The price of normalization},
  author={Goldberg, Yair and Zakai, Alon and Kushnir, Dan and Ritov, Ya’acov},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={Aug},
  pages={1909--1939},
  year={2008}
}

@article{Herring2018-cq,
	Abstract = {Modern single-cell technologies allow multiplexed sampling of
              cellular states within a tissue. However, computational tools
              that can infer developmental cell-state transitions reproducibly
              from such single-cell data are lacking. Here, we introduce
              p-Creode, an unsupervised algorithm that produces multi-branching
              graphs from single-cell data, compares graphs with differing
              topologies, and infers a statistically robust hierarchy of
              cell-state transitions that define developmental trajectories. We
              have applied p-Creode to mass cytometry, multiplex
              immunofluorescence, and single-cell RNA-seq data. As a test case,
              we validate cell-state-transition trajectories predicted by
              p-Creode for intestinal tuft cells, a rare, chemosensory cell
              type. We clarify that tuft cells are specified outside of the
              Atoh1-dependent secretory lineage in the small intestine.
              However, p-Creode also predicts, and we confirm, that tuft cells
              arise from an alternative, Atoh1-driven developmental program in
              the colon. These studies introduce p-Creode as a reliable method
              for analyzing large datasets that depict branching transition
              trajectories.},
	Author = {Herring, Charles A and Banerjee, Amrita and McKinley, Eliot T and Simmons, Alan J and Ping, Jie and Roland, Joseph T and Franklin, Jeffrey L and Liu, Qi and Gerdes, Michael J and Coffey, Robert J and Lau, Ken S},
	Journal = {Cell Syst},
	Keywords = {cell-state transitions; differentiation hierachies; graph theory; intestine and colon; mass cytometry; pseudo-time analysis; single-cell RNA-seq; single-cell biology; trajectories; tuft cells},
	Language = {en},
	Month = jan,
	Number = 1,
	Pages = {37--51.e9},
	Title = {Unsupervised Trajectory Analysis of {Single-Cell} {RNA-Seq} and Imaging Data Reveals Alternative Tuft Cell Origins in the Gut},
	Volume = 6,
	Year = 2018}

@article{HeinAL:07,
	Author = {Matthias Hein and Jean{-}Yves Audibert and Ulrike von Luxburg},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp1.uni-trier.de/rec/bib/journals/jmlr/HeinAL07},
	Journal = {Journal of Machine Learning Research},
	Pages = {1325--1368},
	Timestamp = {Tue, 18 Jan 4429289 17:08:32 +search},
	Title = {Graph Laplacians and their Convergence on Random Neighborhood Graphs},
	Url = {http://dl.acm.org/citation.cfm?id=1314544},
	Volume = {8},
	Year = {2007},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1314544}}

@inproceedings{HeinAL:05,
	Author = {Matthias Hein and Jean{-}Yves Audibert and Ulrike von Luxburg},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp1.uni-trier.de/rec/bib/conf/colt/HeinAL05},
	Booktitle = {Learning Theory, 18th Annual Conference on Learning Theory, {COLT} 2005, Bertinoro, Italy, June 27-30, 2005, Proceedings},
	Doi = {10.1007/11503415_32},
	Pages = {470--485},
	Timestamp = {Wed, 15 Oct 2014 17:04:32 +0200},
	Title = {From Graphs to Manifolds - Weak and Strong Pointwise Consistency of Graph Laplacians},
	Url = {http://dx.doi.org/10.1007/11503415_32},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/11503415_32}}

@inproceedings{Kleindessner2015DimensionalityEW,
	Author = {Matth{\"a}us Kleindessner and Ulrike von Luxburg},
	Booktitle = {AISTATS},
	Title = {Dimensionality estimation without distances},
	Year = {2015}}

@book{Kozlov:08,
	Altauthor = {Dmitry Kozlov},
	Optaddress = {Heidelberg},
	Optseries = {Algorithms and Computation in Mathematics},
	Optvolume = {21},
	Publisher = {Springer},
	Title = {Combinatorial algebraic topology},
	Year = {2008}}

@book{Lee:03,
	Author = {John M. Lee},
	Optaddress = {Seattle, WA USA},
	Publisher = {Springer},
	Title = {Introduction to Smooth Manifolds},
	Year = {2003}}


@inproceedings{LevinaB04,
	Author = {Elizaveta Levina and Peter J. Bickel},
	Booktitle = {Advances in Neural Information Processing Systems 17 {NIPS} 2004, December 13-18, 2004, Vancouver, British Columbia, Canada]},
	Pages = {777--784},
	Title = {Maximum Likelihood Estimation of Intrinsic Dimension},
	Url = {http://papers.nips.cc/paper/2577-maximum-likelihood-estimation-of-intrinsic-dimension},
	Year = {2004},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/2577-maximum-likelihood-estimation-of-intrinsic-dimension}}

@article{CNS:CNS12415,
	Author = {Li, Xin-Wei and Li, Qiong-Ling and Li, Shu-Yu and Li, De-Yu and the Alzheimer's Disease Neuroimaging Initiative},
	Doi = {10.1111/cns.12415},
	Issn = {1755-5949},
	Journal = {CNS Neuroscience & Therapeutics},
	Keywords = {Hippocampal segmentation, Local label fusion, Manifold learning, Multiatlas segmentation},
	Number = {10},
	Pages = {826--836},
	Title = {Local manifold learning for multiatlas segmentation: application to hippocampal segmentation in healthy population and Alzheimer's disease},
	Url = {http://dx.doi.org/10.1111/cns.12415},
	Volume = {21},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1111/cns.12415}}

@article{linHJXMF:16,
	Author = {Chuang Lin and Bing-Hui Wang and Ning Jiang and Ren Xu and Natalie Mrachacz-Kersting and Dario Farina},
	Journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	Month = {September},
	Number = {9},
	Pages = {921--927},
	Title = {Discriminative Manifold Learning Based Detection of Movement-Related Cortical Potentials},
	Volume = {24},
	Year = {2016}}

@inproceedings{LittleJM09,
	Author = {Anna V. Little and Yoon{-}Mo Jung and Mauro Maggioni},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/conf/aaaifs/LittleJM09},
	Booktitle = {Manifold Learning and Its Applications, Papers from the 2009 {AAAI}},
	Timestamp = {Thu, 26 Sep 2013 16:17:55 +0200},
	Title = {Multiscale Estimation of Intrinsic Dimensionality of Data Sets},
	Year = {2009}}

@article{Luo2009-mp,
	Abstract = {The gradient of a function defined on a manifold is perhaps one
               of the most important differential objects in data analysis.
               Most often in practice, the input function is available only at
               discrete points sampled from the underlying manifold, and the
               manifold is approximated by either a mesh or simply a point
               cloud. While many methods exist for computing gradients of a
               function defined over a mesh, computing and simplifying
               gradients and related quantities such as critical points, of a
               function from a point cloud is non-trivial. In this paper, we
               initiate the investigation of computing gradients under a
               different metric on the manifold from the original natural
               metric induced from the ambient space. Specifically, we map the
               input manifold to the eigenspace spanned by its Laplacian
               eigenfunctions, and consider the so-called diffusion distance
               metric associated with it. We show the relation of gradient
               under this metric with that under the original metric. It turns
               out that once the Laplace operator is constructed, it is easier
               to approximate gradients in the eigenspace for discrete inputs
               (especially point clouds) and it is robust to noises in the
               input function and in the underlying manifold. More importantly,
               we can easily smooth the gradient field at different scales
               within this eigenspace framework. We demonstrate the use of our
               new eigen-gradients with two applications: approximating /
               simplifying the critical points of a function, and the Jacobi
               sets of two input functions (which describe the correlation
               between these two functions), from point clouds data.},
	Author = {Luo, Chuanjiang and Safa, Issam and Wang, Yusu},
	Journal = {Comput. Graph. Forum},
	Keywords = {I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling; Geometric algorithms, languages, and systems},
	Month = jul,
	Number = 5,
	Pages = {1497--1508},
	Publisher = {Blackwell Publishing Ltd},
	Title = {Approximating Gradients for Meshes and Point Clouds via Diffusion Metric},
	Volume = 28,
	Year = 2009}

@article{vonluxburg:07,
	Author = {Ulrike von Luxburg},
	Journal = {Statistics and Computing},
	Number = {4},
	Pages = {395--416},
	Title = {A tutorial on spectral clustering},
	Volume = {17},
	Year = {2007}}

@techreport{vandermaaten:09comparative,
	Author = {L.J.P. van der Maaten and E.O. Postma and H.J. van den Herik},
	Institution = {Tilburg University},
	Number = {TiCC-TR 2009-005},
	Title = {Dimensionality Reduction: A Comparative Review},
	Year = {2009}}

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}

@article{mohammedNarayanan:localpcs17,
	Author = {Kitty Mohammed and Hariharan Narayanan},
	Journal = {arxiv},
	Title = {Manifold Learning Using Kernel Density Estimation and Local Principal Components Analysis},
	Volume = {1709.03615},
	Year = {2017}}

@article{flann:2014,
	Author = {Marius Muja and David G. Lowe},
	Journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	Publisher = {IEEE},
	Title = {Scalable Nearest Neighbor Algorithms for High Dimensional Data},
	Volume = {36},
	Year = {2014}}

@inproceedings{nadler:06,
	Address = {Cambridge, MA},
	Author = {Boaz Nadler and Stephane Lafon and Ronald Coifman and Ioannis Kevrekidis},
	Booktitle = {Advances in Neural Information Processing Systems 18},
	Editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	Pages = {955--962},
	Publisher = {MIT Press},
	Title = {Diffusion Maps, Spectral Clustering and Eigenfunctions of {Fokker-Planck} Operators},
	Year = {2006}}

@phdthesis{Nelson2011-bn,
	Author = {Nelson, Jelani (jelani Osei)},
	Keywords = {Electrical Engineering and Computer Science.; Thesis},
	Language = {en},
	Publisher = {Massachusetts Institute of Technology},
	School = {Massachusetts Institute of Technology},
	Title = {Sketching and streaming high-dimensional vectors},
	Year = 2011}

@article{Nguyen2013-eo,
	Abstract = {We present a new locality sensitive hashing (LSH) algorithm
                   for $c$-approximate nearest neighbor search in $\ell_p$ with
                   $1<p<2$. For a database of $n$ points in $\ell_p$, we
                   achieve $O(dn^\{\rho\})$ query time and $O(dn+n^\{1+\rho\})$
                   space, where $\rho \le O((\ln c)^2/c^p)$. This improves upon
                   the previous best upper bound $\rho\le 1/c$ by Datar et al.
                   (SOCG 2004), and is close to the lower bound $\rho \ge
                   1/c^p$ by O'Donnell, Wu and Zhou (ITCS 2011). The proof is a
                   simple generalization of the LSH scheme for $\ell_2$ by
                   Andoni and Indyk (FOCS 2006).},
	Archiveprefix = {arXiv},
	Author = {Nguyen, Huy L},
	Eprint = {1306.3601},
	Month = jun,
	Primaryclass = {cs.DS},
	Title = {Approximate Nearest Neighbor Search in $\ell_p$},
	Year = 2013}

@book{Nocedal:06,
	Author = {Nocedal, Jorge and Wright, S.},
	Publisher = {Springer},
	Title = {Numerical Optimization},
	Year = {2006}}

@article{OE:11,
	Author = {Umut Ozertem and Deniz Erdogmus},
	Journal = {Journal of Machine Learning Research},
	Pages = {1249-1286},
	Title = {Locally Defined Principal Curves and Surfaces},
	Url = {http://www.jmlr.org/papers/volume12/ozertem11a/ozertem11a.pdf},
	Volume = {12},
	Year = {2011},
	Bdsk-Url-1 = {http://www.jmlr.org/papers/volume12/ozertem11a/ozertem11a.pdf}}

@article{pedregosa2011,
	Author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
	Journal = {The Journal of Machine Learning Research},
	Pages = {2825--2830},
	Publisher = {JMLR. org},
	Title = {Scikit-learn: Machine learning in {P}ython},
	Volume = {12},
	Year = {2011}}

@article{Portegies:16,
	Author = {Portegies, Jacobus W},
	Journal = {Communications on Pure and Applied Mathematics},
	Number = {3},
	Pages = {478--518},
	Publisher = {Wiley Online Library},
	Title = {Embeddings of Riemannian manifolds with heat kernels and eigenfunctions},
	Volume = {69},
	Year = {2016}}

@inproceedings{pmlr-v75-indyk18a,
	Abstract = {We consider the $(1+\epsilon)$-approximate nearest neighbor search problem: given a set $X$ of $n$ points in a $d$-dimensional space, build a data structure that, given any query point  $y$,  finds a point $x \in X$ whose distance to $y$ is at most $(1+\epsilon) \min_{x \in X} \|x-y\|$ for an accuracy parameter $\epsilon \in (0,1)$.  Our main result is a data structure that occupies only $O(\epsilon^{-2} n \log(n) \log(1/\epsilon))$ bits of space, assuming all point coordinates are integers in the range  $\{-n^{O(1)} \ldots n^{O(1)}\}$, i.e., the coordinates have $O(\log n)$ bits of precision. This improves over the best previously known space bound of         $O(\epsilon^{-2} n \log(n)^2)$, obtained via the randomized dimensionality reduction method of Johnson and Lindenstrauss (1984).  We also consider the more general problem of estimating all distances from a collection of query points to all data points $X$, and provide almost tight upper and lower bounds for the space complexity of this problem. },
	Author = {Indyk, Piotr and Wagner, Tal},
	Booktitle = {Proceedings of the 31st Conference On Learning Theory},
	Editor = {Bubeck, S\'ebastien and Perchet, Vianney and Rigollet, Philippe},
	Month = {06--09 Jul},
	Pages = {2012--2036},
	Pdf = {http://proceedings.mlr.press/v75/indyk18a/indyk18a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Approximate Nearest Neighbors in Limited Space},
	Url = {http://proceedings.mlr.press/v75/indyk18a.html},
	Volume = {75},
	Year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v75/indyk18a.html}}

@article{roweis:00,
	Author = {Sam Roweis and Lawrence Saul},
	Journal = {Science},
	Month = {December},
	Number = {5500},
	Pages = {2323--2326},
	Title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	Volume = {290},
	Year = {2000}}

@article{soltanolkotabiElhamifarCandes:14,
	Author = {M. Soltanolkotabi and E. Elhamifar and E.J. Candes},
	Journal = {The Annals of Statistics 42},
	Optannote = {consistency proof for elhamifar and vidal},
	Optnumber = {2},
	Optpages = {669--699},
	Optvolume = {42},
	Title = {Robust Subspace Clustering},
	Year = {2104}}

@article{Sidiropoulos2019-qd,
	Abstract = {We present several approximation algorithms for the problem of
               embedding metric spaces into a line, and into the 2-dimensional
               plane. Among other results, we give an
               $O(\sqrt\{n\})$-approximation algorithm for the problem of
               finding a line embedding of a metric induced by a given
               unweighted graph, that minimizes the (standard) multiplicative
               distortion. We give an improved $\tilde\{O\}(n^\{1/3\})$
               approximation for the case of metrics induced by unweighted
               trees.},
	Author = {Sidiropoulos, A and Badoiu, M and Dhamdhere, K and Gupta, A and Indyk, P and Rabinovich, Y and Racke, H and Ravi, R},
	Journal = {SIAM J. Discrete Math.},
	Month = jan,
	Number = 1,
	Pages = {454--473},
	Publisher = {Society for Industrial and Applied Mathematics},
	Title = {Approximation Algorithms for {Low-Distortion} Embeddings into {Low-Dimensional} Spaces},
	Volume = 33,
	Year = 2019}

@article{suKurtekKlassenSrivastava:trajectories-aoas14,
	Author = {Jingyong Su and Sebastian Kurtek and Eric Klassen and Anuj Srivastava},
	Journal = {The Annals of Applied Statistics},
	Note = {not ML; they know the manifold, but develop a method of analyzing the trajectory},
	Number = {1},
	Pages = {530---552},
	Title = {STATISTICAL ANALYSIS OF TRAJECTORIES ON RIEMANNIAN MANIFOLDS: BIRD MIGRATION, HURRICANE TRACKING AND VIDEO SURVEILLANCE},
	Volume = {8},
	Year = {2014}}

@inproceedings{TingHJ:10,
	Author = {Daniel Ting and Ling Huang and Michael I. Jordan},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp1.uni-trier.de/rec/bib/conf/icml/TingHJ10},
	Booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
	Pages = {1079--1086},
	Timestamp = {Fri, 16 Dec 2011 12:43:25 +0100},
	Title = {An Analysis of the Convergence of Graph Laplacians},
	Url = {http://www.icml2010.org/papers/554.pdf},
	Year = {2010},
	Bdsk-Url-1 = {http://www.icml2010.org/papers/554.pdf}}

@article{vanderplasConnolly:09,
	Abstract = {We introduce locally linear embedding (LLE) to the astronomical community as a new classification technique, using Sloan Digital Sky Survey spectra as an example data set. LLE is a nonlinear dimensionality reduction technique that has been studied in the context of computer perception. We compare the performance of LLE to well-known spectral classification techniques, e.g., principal component analysis and line-ratio diagnostics. We find that LLE combines the strengths of both methods in a single, coherent technique, and leads to improved classification of emission-line spectra at a relatively small computational cost. We also present a data subsampling technique that preserves local information content, and proves effective for creating small, efficient training samples from large, high-dimensional data sets. Software used in this LLE-based classification is made available.},
	Author = {Jake Vanderplas and Andrew Connolly},
	Journal = {The Astronomical Journal},
	Number = {5},
	Pages = {1365},
	Title = {Reducing the Dimensionality of Data: Locally Linear Embedding of Sloan Galaxy Spectra},
	Url = {http://stacks.iop.org/1538-3881/138/i=5/a=1365},
	Volume = {138},
	Year = {2009},
	Bdsk-Url-1 = {http://stacks.iop.org/1538-3881/138/i=5/a=1365}}

@misc{wittman:10,
	Author = {T. Wittman},
	Howpublished = {http://www.math.umn.edu/\textasciitilde{}wittman/mani/, retrieved 07/2010},
	Title = {MANIfold learning matlab demo},
	Year = {2010}}

@article{ZhangZ:04,
	Author = {Zhenyue Zhang and Hongyuan Zha},
	Bibsource = {DBLP, http://dblp.uni-trier.de},
	Ee = {http://dx.doi.org/10.1137/S1064827502419154},
	Journal = {SIAM J. Scientific Computing},
	Number = {1},
	Pages = {313-338},
	Title = {Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment},
	Volume = {26},
	Year = {2004}}

@book{Zomorodian:05,
	Author = {Afra Zomorodian},
	Publisher = {Cambridge University Press},
	Series = {Cambridge Monographs on applied and computational mathematics},
	Title = {Topology for computing},
	Year = {2005}}
