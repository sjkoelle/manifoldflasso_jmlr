%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for mac at 2018-11-28 10:06:27 -0800 


%% Saved with string encoding Unicode (UTF-8) 



@ARTICLE{Aswani2011-li,
  title     = "{REGRESSION} {ON} {MANIFOLDS}: {ESTIMATION} {OF} {THE}
               {EXTERIOR} {DERIVATIVE}",
  author    = "Aswani, Anil and Bickel, Peter and Tomlin, Claire",
  abstract  = "[Collinearity and near-collinearity of predictors cause
               difficulties when doing regression. In these cases, variable
               selection becomes untenable because of mathematical issues
               concerning the existence and numerical stability of the
               regression coefficients, and interpretation of the coefficients
               is ambiguous because gradients are not defined. Using a
               differential geometric interpretation, in which the regression
               coefficients are interpreted as estimates of the exterior
               derivative of a function, we develop a new method to do
               regression in the presence of collinearities. Our regularization
               scheme can improve estimation error, and it can be easily
               modified to include lasso-type regularization. These estimators
               also have simple extensions to the ``large p, small n''
               context.]",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  39,
  number    =  1,
  pages     = "48--81",
  year      =  2011
}


@article{GO2011,
	Abstract = {In multivariate regression, a K -dimensional response vector is regressed upon a common set of p covariates, with a matrix B* ‚àà ‚Ñù p√óK of regression coefficients. We study the behavior of the multivariate group Lasso, in which block regularization based on the ùìÅ‚ÇÅ/ùìÅ‚ÇÇ norm is used for support union recovery, or recovery of the set of s rows for which B* is nonzero. Under high-dimensional scaling, we show that the multivariate group Lasso exhibits a threshold for the recovery of the exact row pattern with high probability over the random design and noise that is specified by the sample complexity parameter Œ∏(n, p, s):=n/[2œà(B*) log(p ‚àí s)]. Here n is the sample size, and œà(B*) is a sparsity-overlap function measuring a combination of the sparsities and overlaps of the K -regression coefficient vectors that constitute the model. We prove that the multivariate group Lasso succeeds for problem sequences (n, p, s) such that Œ∏(n, p, s) exceeds a critical level Œ∏ u , and fails for sequences such that Œ∏(n, p, s) lies below a critical level Œ∏ ùìÅ . For the special case of the standard Gaussian ensemble, we show that Œ∏ ùìÅ = Œ∏ u so that the characterization is sharp. The sparsity-overlap function œà(B*) reveals that, if the design is uncorrelated on the active rows, ùìÅ‚ÇÅ/ùìÅ‚ÇÇ regularization for multivariate regression never harms performance relative to an ordinary Lasso approach and can yield substantial improvements in sample complexity (up to a factor of K) when the coefficient vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the multivariate group Lasso. We complement our analysis with simulations that demonstrate the sharpness of our theoretical results, even for relatively small problems.},
	Author = {Guillaume Obozinski and Martin J. Wainwright and Michael I. Jordan},
	Date-Added = {2018-11-28 18:06:01 +0000},
	Date-Modified = {2018-11-28 18:06:15 +0000},
	Issn = {00905364},
	Journal = {The Annals of Statistics},
	Number = {1},
	Pages = {1--47},
	Publisher = {Institute of Mathematical Statistics},
	Title = {SUPPORT UNION RECOVERY IN HIGH-DIMENSIONAL MULTIVARIATE REGRESSION},
	Url = {http://www.jstor.org/stable/29783630},
	Volume = {39},
	Year = {2011},
	Bdsk-Url-1 = {http://www.jstor.org/stable/29783630}}

@inproceedings{Kleindessner2015DimensionalityEW,
	Author = {Matth{\"a}us Kleindessner and Ulrike von Luxburg},
	Booktitle = {AISTATS},
	Date-Added = {2018-11-28 17:58:05 +0000},
	Date-Modified = {2018-11-28 17:58:05 +0000},
	Title = {Dimensionality estimation without distances},
	Year = {2015}}

@article{Noe2017-up,
	Abstract = {Collective variables are an important concept to study
              high-dimensional dynamical systems, such as molecular dynamics of
              macromolecules, liquids, or polymers, in particular to define
              relevant metastable states and state-transition or
              phase-transition. Over the past decade, a rigorous mathematical
              theory has been formulated to define optimal collective variables
              to characterize slow dynamical processes. Here we review recent
              developments, including a variational principle to find optimal
              approximations to slow collective variables from simulation data,
              and algorithms such as the time-lagged independent component
              analysis. Using these concepts, a distance metric can be defined
              that quantifies how slowly molecular conformations interconvert.
              Extensions and open questions are discussed.},
	Author = {No{\'e}, Frank and Clementi, Cecilia},
	Date-Added = {2018-11-28 17:50:39 +0000},
	Date-Modified = {2018-11-28 17:50:39 +0000},
	Journal = {Curr. Opin. Struct. Biol.},
	Language = {en},
	Month = apr,
	Pages = {141--147},
	Title = {Collective variables for the study of long-time kinetics from molecular trajectories: theory and methods},
	Volume = 43,
	Year = 2017}

@article{Gear2016-rj,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1208.5246G},
	Archiveprefix = {arXiv},
	Author = {{Gear}, C.~W.},
	Date-Added = {2018-11-28 17:46:42 +0000},
	Date-Modified = {2018-11-28 17:46:53 +0000},
	Eprint = {1208.5246},
	Journal = {ArXiv e-prints},
	Keywords = {Physics - Computational Physics, Mathematics - Numerical Analysis, 65P99, 37M99},
	Month = aug,
	Primaryclass = {physics.comp-ph},
	Title = {{Parameterization of non-linear manifolds}},
	Year = 2012}

@article{Brunton-2016dt,
	Abstract = {Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts.Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	Author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	Date-Added = {2018-11-28 17:44:52 +0000},
	Date-Modified = {2018-11-28 17:45:04 +0000},
	Doi = {10.1073/pnas.1517384113},
	Eprint = {http://www.pnas.org/content/113/15/3932.full.pdf},
	Issn = {0027-8424},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {15},
	Pages = {3932--3937},
	Publisher = {National Academy of Sciences},
	Title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	Url = {http://www.pnas.org/content/113/15/3932},
	Volume = {113},
	Year = {2016},
	Bdsk-Url-1 = {http://www.pnas.org/content/113/15/3932},
	Bdsk-Url-2 = {http://dx.doi.org/10.1073/pnas.1517384113}}

@article{Wainwright:2009sharp,
	Author = {Martin J. Wainwright},
	Date-Added = {2018-11-28 17:42:44 +0000},
	Date-Modified = {2018-11-28 17:56:06 +0000},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {2183-2202},
	Title = {Sharp Thresholds for High-Dimensional and Noisy Sparsity Recovery Using $\ell _{1}$ -Constrained Quadratic Programming (Lasso)},
	Volume = {55},
	Year = {2009}}

@article{Elyaderani2017-ce,
	Author = {M.K. Elyaderani and S.Jain and J.M.Druce and S.Gonella and J.D.Haupt},
	Date-Added = {2018-11-28 17:35:55 +0000},
	Date-Modified = {2018-11-28 17:40:49 +0000},
	Journal = {CoRR},
	Title = {Improved Support Recovery Guarantees for the Group Lasso With Applications to Structural Health Monitoring},
	Volume = {abs/1708.08826},
	Year = {2017}}

@inproceedings{tehRoweis:nips,
	Author = {Yee Whye Teh and Sam T. Roweis},
	Booktitle = {NIPS},
	Date-Added = {2018-11-28 00:13:45 +0000},
	Date-Modified = {2018-11-28 00:14:02 +0000},
	Title = {Automatic Alignment of Local Representations},
	Year = {2002}}

@article{saulRoweis:03jmlr,
	Acmid = {945372},
	Author = {Lawrence K. Saul and Sam T. Roweis},
	Date-Added = {2018-11-28 00:12:19 +0000},
	Date-Modified = {2018-11-28 00:12:41 +0000},
	Doi = {10.1162/153244304322972667},
	Issn = {1532-4435},
	Issue_Date = {12/1/2003},
	Journal = {J. Mach. Learn. Res.},
	Month = dec,
	Numpages = {37},
	Pages = {119--155},
	Publisher = {JMLR.org},
	Title = {Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds},
	Url = {https://doi.org/10.1162/153244304322972667},
	Volume = {4},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1162/153244304322972667},
	Bdsk-Url-2 = {http://dx.doi.org/10.1162/153244304322972667}}

@article{Constantine2014,
	Author = {P. Constantine and E. Dow and Q. Wang},
	Date-Added = {2018-11-28 00:06:56 +0000},
	Date-Modified = {2018-11-28 00:07:29 +0000},
	Doi = {10.1137/130916138},
	Eprint = {https://doi.org/10.1137/130916138},
	Journal = {SIAM Journal on Scientific Computing},
	Number = {4},
	Pages = {A1500-A1524},
	Title = {Active Subspace Methods in Theory and Practice: Applications to Kriging Surfaces},
	Url = {https://doi.org/10.1137/130916138},
	Volume = {36},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1137/130916138},
	Bdsk-Url-2 = {http://dx.doi.org/10.1137/130916138}}

@book{DoCarmo,
	Author = {Manfredo do Carmo},
	Date-Added = {2018-11-27 23:55:21 +0000},
	Date-Modified = {2018-11-27 23:56:15 +0000},
	Publisher = {Springer},
	Title = {Riemannian Geometry},
	Year = {1992}}

@conference{Oliva2013-ti,
	Abstract = {We present the FuSSO, a functional analogue to the LASSO, that efficiently finds a sparse set of functional input covariates to regress a real-valued response against. The FuSSO does so in a semi-parametric fashion, making no parametric assumptions about the nature of input functional covariates and assuming a linear form to the mapping of functional covariates to the response. We provide a statistical backing for use of the FuSSO via proof of asymptotic sparsistency under various conditions. Furthermore, we observe good results on both synthetic and real-world data.},
	Author = {Junier Oliva and Barnabas Poczos and Timothy Verstynen and Aarti Singh and Jeff Schneider and Fang-Cheng Yeh and Wen-Yih Tseng},
	Booktitle = {International Conference on AI and Statistics(AISTATS)},
	Date-Added = {2018-10-31 18:48:58 +0000},
	Date-Modified = {2018-10-31 19:08:34 +0000},
	Pdf = {http://proceedings.mlr.press/v33/oliva14b.pdf},
	Title = {{FuSSO: Functional Shrinkage and Selection Operator}},
	Url = {http://proceedings.mlr.press/v33/oliva14b.html},
	Year = {2014},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v33/oliva14b.html}}

@article{Ye2012,
	Abstract = {Variable selection and dimension reduction are two commonly adopted approaches for high-dimensional data analysis, but have traditionally been treated separately. Here we propose an integrated approach, called sparse gradient learning (SGL), for variable selection and dimension reduction via learning the gradients of the prediction function directly from samples. By imposing a sparsity constraint on the gradients, variable selection is achieved by selecting variables corresponding to non-zero partial derivatives, and effective dimensions are extracted based on the eigenvectors of the derived sparse empirical gradient covariance matrix. An error analysis is given for the convergence of the estimated gradients to the true ones in both the Euclidean and the manifold setting. We also develop an efficient forward-backward splitting algorithm to solve the SGL problem, making the framework practically scalable for medium or large datasets. The utility of SGL for variable selection and feature extraction is explicitly given and illustrated on artificial data as well as real-world examples. The main advantages of our method include variable selection for both linear and nonlinear predictions, effective dimension reduction with sparse loadings, and an efficient algorithm for large p, small n problems.},
	Author = {Ye, Gui-Bo and Xie, Xiaohui},
	Date-Added = {2018-10-31 18:42:18 +0000},
	Date-Modified = {2018-10-31 18:42:18 +0000},
	Day = {01},
	Doi = {10.1007/s10994-012-5284-9},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Month = {Jun},
	Number = {3},
	Pages = {303--355},
	Title = {Learning sparse gradients for variable selection and dimension reduction},
	Url = {https://doi.org/10.1007/s10994-012-5284-9},
	Volume = {87},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10994-012-5284-9},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s10994-012-5284-9}}

@book{smoothmfd,
	Author = {John M. Lee},
	Date-Added = {2018-09-14 16:09:44 +0000},
	Date-Modified = {2018-10-31 18:58:13 +0000},
	Publisher = {Springer-Verlag New York},
	Title = {Introduction to Smooth Manifolds},
	Year = {2003}}

@book{Zorich,
	Author = {Vladimir A. Zorich},
	Date-Added = {2018-09-14 16:13:01 +0000},
	Date-Modified = {2018-09-14 16:15:08 +0000},
	Publisher = {Springer-Verlag Berlin Heidelberg},
	Title = {Mathematical Analysis I},
	Year = {2004}}

@article{Saul2003-ou,
	Author = {Saul, Lawrence K and Roweis, Sam T},
	Journal = {J. Mach. Learn. Res.},
	Number = {Jun},
	Pages = {119--155},
	Title = {Think Globally, Fit Locally: Unsupervised Learning of Low Dimensional Manifolds},
	Volume = 4,
	Year = 2003}

@incollection{Teh2003-oh,
	Author = {Teh, Yee W and Roweis, Sam T},
	Booktitle = {Advances in Neural Information Processing Systems 15},
	Editor = {Becker, S and Thrun, S and Obermayer, K},
	Pages = {865--872},
	Publisher = {MIT Press},
	Title = {Automatic Alignment of Local Representations},
	Year = 2003}

@article{Luo2009-mp,
	Abstract = {The gradient of a function defined on a manifold is perhaps one
               of the most important differential objects in data analysis.
               Most often in practice, the input function is available only at
               discrete points sampled from the underlying manifold, and the
               manifold is approximated by either a mesh or simply a point
               cloud. While many methods exist for computing gradients of a
               function defined over a mesh, computing and simplifying
               gradients and related quantities such as critical points, of a
               function from a point cloud is non-trivial. In this paper, we
               initiate the investigation of computing gradients under a
               different metric on the manifold from the original natural
               metric induced from the ambient space. Specifically, we map the
               input manifold to the eigenspace spanned by its Laplacian
               eigenfunctions, and consider the so-called diffusion distance
               metric associated with it. We show the relation of gradient
               under this metric with that under the original metric. It turns
               out that once the Laplace operator is constructed, it is easier
               to approximate gradients in the eigenspace for discrete inputs
               (especially point clouds) and it is robust to noises in the
               input function and in the underlying manifold. More importantly,
               we can easily smooth the gradient field at different scales
               within this eigenspace framework. We demonstrate the use of our
               new eigen-gradients with two applications: approximating /
               simplifying the critical points of a function, and the Jacobi
               sets of two input functions (which describe the correlation
               between these two functions), from point clouds data.},
	Author = {Luo, Chuanjiang and Safa, Issam and Wang, Yusu},
	Journal = {Comput. Graph. Forum},
	Keywords = {I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling; Geometric algorithms, languages, and systems},
	Month = jul,
	Number = 5,
	Pages = {1497--1508},
	Publisher = {Blackwell Publishing Ltd},
	Title = {Approximating Gradients for Meshes and Point Clouds via Diffusion Metric},
	Volume = 28,
	Year = 2009}

@article{PerraultM:arxiv1406.0118,
	Adsurl = {http://arxiv.org/abs/1405.0118},
	Archiveprefix = {arXiv},
	Author = {Dominique Perrault-Joncas and Marina Meila},
	Eprint = {1406.0118},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning},
	Month = june,
	Primaryclass = {stat.ML},
	Title = {Improved graph Laplacian via geometric self-consistency},
	Year = 2014}

@article{Yuan2006-af,
	Abstract = {We consider the problem of selecting grouped variables (factors)
               for accurate prediction in regression. Such a problem arises
               naturally in many practical situations with the multifactor
               analysis?of?variance problem as the most important and
               well?known example. Instead of ?},
	Author = {Yuan, M and Lin, Y},
	Journal = {J. R. Stat. Soc. Series B Stat. Methodol.},
	Publisher = {Wiley Online Library},
	Title = {Model selection and estimation in regression with grouped variables},
	Year = 2006}
