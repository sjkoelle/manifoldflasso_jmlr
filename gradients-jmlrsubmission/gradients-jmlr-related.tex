\section{Related work}
\label{sec:related}
\comment{To our knowledge, ours is the first solution to estimating a function $f$ as a {\em non-linear} sparse combination of functions in a dictionary. Below we cite some of the closest related work.}

With respect to parametrizing manifolds, the early work of
\citet{saulRoweis:03jmlr} and \citet{tehRoweis:nips} (and references therein)
proposes parametrizing the manifold by finite mixtures of local linear
models, aligned so as to provides global coordinates, in a way
reminiscent of LTSA \citep{ZhangZ:04}. \mmp{also parametrization from paper by
  chris fu}

{\em Gradient Learning} \citep{Ye2012} recovers non-zero partial
derivatives via Group Lasso type regression as methods of variable
selection and dimension reduction. Their work does not rely on a functional dictionary like ours as input,
is concerned with estimating the gradient of a single regression function,
and is in a supervised setting.

The {\em Active Subspace} method of \citet{Constantine2014} uses the information in the gradient to discover a subspace of maximum variation of a function $f$. This subspace is given by the principal subspace of the matrix $C=E_\rho[\nabla f\nabla f^T]$, where $\rho $ is a weighting function averaging over a finite or infinite set of points in the domain of $f$. While this method uses the gradient information, it can only find a global subspace, which would not be adequate for function composition, or for functions defined on non-linear manifolds.

The work of \citet{Brunton-2016dt} is similar to ours in that it uses a
dictionary and the idea of differential composition. The goal is to identify the functional equations of
non-linear dynamical systems by regressing the time derivatives of the
state variables on a subset of functions in the dictionary, with a
sparsity inducing penalty. The recovered functional equation is {\em
  linear} in the dictionary functions, hence any non-linearity in the state variables must be explicitly included in the dictionary. On the other hand, when the functional equation can be expressed as a sum of dictionary functions, then the system is completely identified.

A point of view different from ours is that a set of $d$ eigenvectors of
the Laplace-Beltrami operator $\Delta_{\M}$ are a parametrization of
$\M$. Hence, the Diffusion Maps coordinates could be considered such a
parametrization \citep{coifman:06,CoiLafLeeMag05,Gear2012-rj}. \mmp{it is not known how many needed; but we also don't know about our method.} In
\citet{mohammedNarayanan:localpcs17}, it was shown that principal curves
and surfaces can provide an approximate manifold parametrization. Our
work differs from these in two ways: (1) first, the
explanations we obtain are endowed with the physical meaning of the
domain specific dictionaries, (2) less obviously, descriptors like
principal curves or Laplacian eigenfunctions are generally still
non-parametric (i.e exist in infinite dimensional function spaces),
while the parameterizations by dictionaries we obtain (e.g the
torsions) are in finite dimensional spaces. \citet{Dsilva2018-dz} tackles the
related problem of choosing among the infinitely many Laplacian
eigenfunctions $d$ which provide a $d$-dimensional parametrization of
the manifold; the approach is to solve a set of Local Linear Embedding
\citep{roweis:00} problems, each aiming to represent an eigenfunction
as a combination of the preceding ones. This method reduces the number of "covarying" eigenfunctions but fails to provide physical meaning to the selected functions. 

In \citet{Dsilva13-Nonlinear} the authors consider finding nonlinear intrinsic variables of an observed stochastic process. Suppose that $Y(t)=f(x(t))$ where $t$ is a time parameter, $Y(t)$ is the observed process, and $x(t)$ is a unknown $d-$dimensional stochastic processes driven by stochastic differential equations. The authors further assume that $f$ is bi-Lipschitz so that they can approximate the Euclidean distance in $x$ space by Mahalanobis distance in $Y$ space.  By plugging in this distance into the Gaussian RBF kernel they use the diffusion map to find the function $x$. In this setting, the dictionary functions are unknown. But the parametrization is defined if the manifold and the dictionary functions only differs from a bi-Lipschitz mapping.

Pulling back the gradients from an embedding of the data to $\T_{\xi_i}\M$ by solving a least squares problem was used in \citet{Luo2009-mp}. There, the pull-back was from an approximately isometric mapping of the original data. Hence, the Riemannian metric was not considered. 

\mmp{Autoencoders learn parametric embeddings of fixed $m$; \mmp{shall we compare}. However, there is no a-priori guarantee that the learned function is a manifold outside the data points. \mmp{This would be a problem for us too; except that we know the functions in $\G$ so we can presumably check. }}

