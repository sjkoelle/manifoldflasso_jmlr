\section{Manifold learning and intrinsic geometry}
\label{sec:background}
Suppose we observe data points $\xi_i \in \rrr^D$ that are sampled
from a smooth $d$-dimensional submanifold $\M \subset \rrr^D$. The
task of manifold learning is to provide a diffeomorphism $\phi: \M \to
\phi(\M) \subset \rrr^m$ where $m \ll D$. The Whitney Embedding
Theorem \citep{smoothmfd} guarantees the existence of a map satisfying this
property with $m \leq 2d$. That is, a good manifold learner will
identify a smooth map $\phi: \M \to \rrr^m$ with $d \le m \le 2d \ll
D$.

\paragraph{The neighborhood graph and kernel matrix} The {\em neighborhood graph} is a data structure that 
associates to each data point $\xi_i\in\dataset$ its set of {\em neighbors} 
$\neigh_i=\{i'\in [n], \text{with}\,||\xi_{i'}-\xi_{i}||\leq r_N\}$, 
where $r_N$ is a {\em neighborhood radius} parameter. 
The neighborhood relation is symmetric, and determines an undirected graph with nodes
 represented by the data points $\xi_{1:n}$;  $|\neigh_i|$ is  denoted by $k_i$.
 
Closely related to the neighborhood graph are the local position matrices $\Xi_i = \{\xi_{i'} : i' \in \neigh_i\} \in \rrr^{k_i \times d}$, local embedding coordinate matrices $\Phi_i = \{\phi(\xi_{i'}) : i' \in \neigh_i\} \in \rrr^{k_i \times d}$, and the {\em kernel matrix}
$K \in \rrr^{n \times n}$ whose elements are
\beq
K_{ii'}= \begin{cases} \exp\left( -\tfrac{||\xi_i-\xi_{i'}||}{\epsilon_N^2}\right)  \; &\text{ if} \;i'\in\neigh_i  \\
0 \; \text{otherwise}.
\end{cases}
\label{eq:kernelmatrix}
\eeq Typically, the radius $r_N$ and the {\em bandwidth} parameter
$\epsilon_N$ are related by $r_N=c \epsilon_N$ with $c$ a small
constant greater than 1. This ensures that $K$ is close to its limit
when $r_N\rightarrow \infty$ while remaining sparse, with sparsity
structure induced by the neighborhood graph. Rows of this matrix will be
denoted $K_{i, \neigh_i}$ to emphasize that when a particular row is
passed to an algorithm, only $k_i$ values need to be
passed. Next, we show how the neighborhood graph, local position matrices, and kernel
matrix are used in manifold estimation algorithms.


\paragraph{Estimating tangent spaces in the ambient space $\rrr^D$}
The tangent subspace at point $\xi_i$ in the data, denoted $\T_{\xi_i}\M$, can be estimated by
{\em Weighted Local Principal Component Analysis} \citep{Chen2013}, as described in the
\tppalg~algorithm. The output of this algorithm is an orthogonal matrix
$T_i\in\rrr^{D\times d}$, representing a basis for $\T_{\xi_i}\M$. For
this algorithm and others we define the SVD algorithm
$\svd(X,d)$ of a symmetrical matrix $X$ as outputting
$V,\Lambda$, where $\Lambda$ and $V$ are the largest $d$ eigenvalues
and their eigenvectors, respectively. We denote a column vector of
ones of length $k$ by $ \bm{1}_k$. 
%
\begin{algorithm}[H]
\floatname{algorithm}{\tppalg} \renewcommand{\thealgorithm}{}
\caption{(local data $\Xi_i$,  kernel row $K_{i,\neigh_i}$, intrinsic dimension $d$)}
\begin{algorithmic}[1]
\STATE Compute weighted mean $\bar \xi_i = (K_{i,\neigh_i} \bm{1}_{k_i})^{-1} K_{i,\neigh_i} \Xi_i $
  \STATE Compute weighted local difference matrix $Z_{i} = (K_{i, \neigh_i}  \bm{1}_{k_i}) ^{-1} K_{i , \neigh_i} (\Xi_i - \bm{1}_{k_i} \bar \xi_i)$
  \STATE Compute $T_i, \Lambda \leftarrow \text{SVD} (Z_i^T Z_i, d)$
  \STATE {\bf Output} $T_i$ 
\end{algorithmic}
\end{algorithm}


\paragraph{The renormalized graph Laplacian}
The {\em renormalized graph Laplacian}, also known as the {\em sample
  Laplacian}, or {\em Diffusion Maps Laplacian} $L$, constructed by the 
\lapalg~ algorithm, converges to the manifold Laplace operator
$\Delta_\M$; \citet{coifman:06} shows that this estimator is unbiased
w.r.t. the sampling density on $\M$ (see also \citet{HeinAL:05}, \citet{HeinAL:07,TingHJ:10}). The
Laplacian $L$ is a sparse matrix; row $i$ of $L$ contains non-zeros
only for $i'\in\neigh_i$. Thus, as for $K$, rows of this matrix will
be denoted $L_{i, \neigh_i}$. The sparsity pattern of $L$, too, is given by the
neighborhood graph; construction of the neighborhood graph is
therefore the computationally expensive component of this
algorithm. 

% used in section Section \ref{sec:explain-dphi} to 

%\begin{algorithm}[H]
%\floatname{algorithm}{\lapalg}
%\renewcommand{\thealgorithm}{}
%\caption{(Dataset $\dataset$, neighborhoods $\neigh_{i:n}$, bandwidth $\epsilon_N$)}
%\begin{algorithmic}[1]
%  \STATE {\bf Input} 
%  \STATE Compute kernel matrix $K$ using \ref{}
%  \STATE Compute normalization weights $w_{i}\,\gets\,  K_{i, \neigh_i} \bm{1}_{k_i} $, $w\,\gets\,\diag(w_{i} \; {i=1:n})$
%  \STATE Normalize $\tilde{L}\,\gets\, w^{-1}K $
%  \STATE Compute renormalization weights $\tilde w_i\,\gets\,  \tilde{L}_{i, \neigh_i} \bm{1}_{k_i}, \; \tilde w=\diag( \tilde w_i \;{i=1:n})$
%  \STATE Renormalize $L\,\gets\,4({\tilde w }^{-1} \tilde L -\diag(\tilde w_i)_{i=1:n})$ \skcomment{Check}
%  \STATE {\bf Output} Laplacian matrix $L$, [optionally $K$]
%\end{algorithmic}
%\end{algorithm}
%
\begin{algorithm}[H]
\floatname{algorithm}{\lapalg}
\renewcommand{\thealgorithm}{}
\caption{(neighborhoods $\neigh_{i:n}$, local data $\Xi_{1:n}$, bandwidth $\epsilon_N$)}
\begin{algorithmic}[1]
%  \STATE {\bf Input} 
  \STATE Compute kernel matrix $K$ using (\ref{eq:kernelmatrix})
  \STATE Compute normalization weights $w_{i}\,\gets\,  K_{i, \neigh_i} \bm{1}_{k_i} $, $i=1,\ldots n$,  $W\,\gets\,\diag(w_{i} \; {i=1:n})$
  \STATE Normalize $\tilde{L}\,\gets\, W^{-1}K W^{-1} $
  \STATE Compute renormalization weights $\tilde w_i\,\gets\,  \tilde{L}_{i, \neigh_i} \bm{1}_{k_i}$, $i=1,\ldots n$, $\tilde W=\diag( \tilde w_i \;{i=1:n})$
  \STATE Renormalize $L\,\gets\,\frac{4}{ \epsilon_N^2}({\tilde W }^{-1} \tilde L - I_n)$
  \STATE {\bf Output} Kernel matrix $K$, Laplacian $L$, [optionally $\tilde{w}_{1:n}$]
\end{algorithmic}
\end{algorithm}
%
The $m$ principal eigenvectors of the Laplacian $L$ (or alternatively
of the matrix $\tilde{L}$ of Algorithm \lapalg), corresponding to its
smallest non-zero eigenvalues, are sometimes used as embedding coordinates
$\Phi$ of the data; the embedding obtained is known as the {\em
  Diffusion Map} \citep{coifman:06} or the {\em Laplacian Eigenmap} \citep{belkin:01} of
$\dataset$.  We use this embedding approach for convenience, but in
general, any algorithm which asymptotically generates a smooth
embedding is acceptable.

\paragraph{The pushforward Riemannian metric} Geometric quantities such as angles and lengths of vectors in the tangent bundle $\T\M$ and distances along curves in $\M$ are captured by Riemannian geometry. We assume that $(\M,\id)$ is a {\em Riemannian manifold}, with the metric $\id$ induced from $\rrr^D$. Furthermore, we associate with $\phi(\M)$ a Riemannian metric $\g$ which preserves the geometry of $(\M,\id)$. This metric is called the {\em pushforward Riemannian metric} and is defined by
\beq \label{eq:rmetric0}
\langle u,v\rangle_{\g}\;=\;\langle D\phi^{-1}(\xi)u,D\phi^{-1}(\xi)v\rangle
\quad\text{ for all }u,v\in\T_{\phi(\xi)}\phi(\M).
\eeq
In the above, $D$ denotes the differential operator, and $D\phi^{-1}(\xi)$ is the {\em pull-back} operator, that maps vectors from $\T_{\phi(\xi)}\phi(\M)$ to $\T_\xi\M$, and $\langle , \rangle$ is the Euclidean scalar product. 

For each $\phi(\xi_i)$, the associated push-forward Riemannian metric
expressed in the coordinates of $\rrr^m$, is a symmetric,
semi-positive definite $m\times m$ matrix $G_i$ of rank $d$. The scalar product $\langle u,v \rangle_{\g}$ takes the form $u^TG_iv$.

The matrices $G_i$ can be estimated by the algorithm \rmalg~of
\citet{2013arXiv1305.7255P}. The algorithm uses only local information, and thus can be run
efficiently. For notational simplicity, we refer to the $m$ embedding
coordinates of the points in a neighborhood of a point $i$ as
$\Phi_{i}$.
%
\begin{algorithm}[H]
\floatname{algorithm}{\rmalg}
\renewcommand{\thealgorithm}{}
\caption{(Laplacian row $L_{i, \neigh_i}$, local embedding coordinates $\Phi_{i}$, intrinsic dimension $d$)}
\begin{algorithmic}[1]
\STATE Compute centered local embedding coordinates $\tilde \Phi_i \gets \Phi_{i} - \phi(\xi_i)\bm{1}_{k_i}^T  $
\STATE %$H_{ikk'}=L_{i,\neigh_i} (\tilde \Phi_{i,k} \odot \tilde \Phi_{i,k'})$ for $k,k'=1:m$.
Form matrix $H_i$ by $H_i \gets [H_{ikk'}]_{k,k' \in 1:m}$ with 
 $H_{i,kk'}=\sum_{i'\in \neigh_i}L_{i,i'}\tilde \Phi_{i,i'k} \tilde \Phi_{i,i'k'}$ for $k,k'=1:m$. 
  \STATE  Compute $V_i, \Lambda_i \gets \text{SVD} (H_i, d)$
    \STATE $G_i\,\gets\,V_i \Lambda_i^{-1} V_i^T$.
  \STATE {\bf Output} $G_i$
\end{algorithmic}
\end{algorithm}
%
%Together, these algorithms form the machinery of {\em metric learning}.

